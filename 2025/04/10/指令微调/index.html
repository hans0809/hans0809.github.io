<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>LLM指令微调：训练一个人工智能助手-大模型炼丹术(八) | 南极Python</title><meta name="keywords" content="LLM"><meta name="author" content="雨落诗山山亦奇"><meta name="copyright" content="雨落诗山山亦奇"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="在上一篇文章中，我们通过对预训练的 GPT-2 进行微调，得到了一个垃圾邮件分类器。事实上，这种方式是使用 GPT-2 的网络作为 backbone，在其输出后接一个分类头，来完成二分类任务。 在本文中，我们将介绍另一种微调方式：指令微调（Instruction Tuning）。 通过指令微调，我们可以打造一个对话机器人，就像你一直在使用的各种大语言模型应用那样 —— 它能够接收用户的自然语言指令">
<meta property="og:type" content="article">
<meta property="og:title" content="LLM指令微调：训练一个人工智能助手-大模型炼丹术(八)">
<meta property="og:url" content="http://yoursite.com/2025/04/10/%E6%8C%87%E4%BB%A4%E5%BE%AE%E8%B0%83/index.html">
<meta property="og:site_name" content="南极Python">
<meta property="og:description" content="在上一篇文章中，我们通过对预训练的 GPT-2 进行微调，得到了一个垃圾邮件分类器。事实上，这种方式是使用 GPT-2 的网络作为 backbone，在其输出后接一个分类头，来完成二分类任务。 在本文中，我们将介绍另一种微调方式：指令微调（Instruction Tuning）。 通过指令微调，我们可以打造一个对话机器人，就像你一直在使用的各种大语言模型应用那样 —— 它能够接收用户的自然语言指令">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg">
<meta property="article:published_time" content="2025-04-10T14:32:21.000Z">
<meta property="article:modified_time" content="2025-04-14T13:28:20.106Z">
<meta property="article:author" content="雨落诗山山亦奇">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg"><link rel="shortcut icon" href="https://www.cdnjson.com/images/2021/11/27/_20210211193948.png"><link rel="canonical" href="http://yoursite.com/2025/04/10/%E6%8C%87%E4%BB%A4%E5%BE%AE%E8%B0%83/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'LLM指令微调：训练一个人工智能助手-大模型炼丹术(八)',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-04-14 21:28:20'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/StaticFile_HEXO@latest/butterfly/css/macblack.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://www.cdnjson.com/images/2021/11/27/_20210211193948.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">173</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div></div><hr/></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">南极Python</a></span><div id="menus"><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">LLM指令微调：训练一个人工智能助手-大模型炼丹术(八)</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-04-10T14:32:21.000Z" title="发表于 2025-04-10 22:32:21">2025-04-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-04-14T13:28:20.106Z" title="更新于 2025-04-14 21:28:20">2025-04-14</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF/">大模型炼丹术</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">3.4k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>14分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="LLM指令微调：训练一个人工智能助手-大模型炼丹术(八)"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>在上一篇文章中，我们通过对预训练的 GPT-2 进行微调，得到了一个垃圾邮件分类器。事实上，这种方式是使用 GPT-2 的网络作为 backbone，在其输出后接一个分类头，来完成二分类任务。</p>
<p>在本文中，我们将介绍另一种微调方式：指令微调（Instruction Tuning）。</p>
<p>通过指令微调，我们可以打造一个对话机器人，就像你一直在使用的各种大语言模型应用那样 —— 它能够接收用户的自然语言指令，并输出相应的回复。</p>
<h1 id="一、什么是指令微调？"><a href="#一、什么是指令微调？" class="headerlink" title="一、什么是指令微调？"></a>一、什么是指令微调？</h1><p>指令微调（Instruction Tuning） 是一种让预训练语言模型学会“听懂人话”的方法。它的核心思想是：通过监督微调（Supervised Fine-Tuning, SFT），让模型学习从「指令（Instruction）」到「输出（Response）」的映射。</p>
<p>这种方式与传统的分类、回归等任务不同，指令微调的数据格式通常是自然语言对话格式：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">用户：请告诉我Python中如何定义一个函数？</span><br><span class="line">助手：你可以使用`def`关键词，例如：</span><br><span class="line">def my_function():</span><br><span class="line">    print(&quot;Hello World&quot;)</span><br></pre></td></tr></table></figure>

<p>在训练阶段，我们通常提供大量这样的指令-响应对，模型在学习之后就能够泛化到未见过的指令上进行合理回答。</p>
<h1 id="二、指令微调的数据格式-附数据集下载链接"><a href="#二、指令微调的数据格式-附数据集下载链接" class="headerlink" title="二、指令微调的数据格式(附数据集下载链接)"></a>二、指令微调的数据格式(附数据集下载链接)</h1><p>指令微调的数据格式通常是一个Instruction-Response（指令-回复）对。</p>
<p>典型的数据格式如下（以JSON为例）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;instruction&quot;: &quot;请简要介绍一下Python语言。&quot;,</span><br><span class="line">  &quot;input&quot;: &quot;&quot;,</span><br><span class="line">  &quot;output&quot;: &quot;Python是一种高级编程语言，具有简洁的语法和丰富的库，广泛应用于数据科学、Web 开发、自动化等领域。&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>或者，有时也可以带上input字段作为补充输入信息，比如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;instruction&quot;: &quot;请根据以下文本总结重点内容。&quot;,</span><br><span class="line">  &quot;input&quot;: &quot;Python 是一种广泛使用的编程语言，具有丰富的生态系统。&quot;,</span><br><span class="line">  &quot;output&quot;: &quot;Python 拥有广泛的应用和丰富的生态系统，是一门流行的编程语言。&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>本文使用的数据集链接为：<a target="_blank" rel="noopener" href="https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/instruction-data.json">https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/instruction-data.json</a></p>
<p>里面包含了1100个符合指令微调要求的数据样本对。示例如下：<br><img src="1.png"></p>
<p>将这些数据存储到data变量，并划分到训练集、验证集和测试集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">train_portion = <span class="built_in">int</span>(<span class="built_in">len</span>(data) * <span class="number">0.85</span>)  <span class="comment"># 85% for training</span></span><br><span class="line">test_portion = <span class="built_in">int</span>(<span class="built_in">len</span>(data) * <span class="number">0.1</span>)    <span class="comment"># 10% for testing</span></span><br><span class="line">val_portion = <span class="built_in">len</span>(data) - train_portion - test_portion  <span class="comment"># Remaining 5% for validation</span></span><br><span class="line"></span><br><span class="line">train_data = data[:train_portion]</span><br><span class="line">test_data = data[train_portion:train_portion + test_portion]</span><br><span class="line">val_data = data[train_portion + test_portion:]</span><br></pre></td></tr></table></figure>
<p>查看：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Example entry:\n&quot;</span>, train_data[<span class="number">5</span>])</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Example entry:</span><br><span class="line"> &#123;&#x27;instruction&#x27;: &#x27;Identify the correct spelling of the following word.&#x27;, &#x27;input&#x27;: &#x27;Ocassion&#x27;, &#x27;output&#x27;: &quot;The correct spelling is &#x27;Occasion.&#x27;&quot;&#125;</span><br></pre></td></tr></table></figure>

<h1 id="三、如何转成模型输入格式？"><a href="#三、如何转成模型输入格式？" class="headerlink" title="三、如何转成模型输入格式？"></a>三、如何转成模型输入格式？</h1><p>为了适配 GPT 类模型的自回归训练方式，我们需要将输入和输出拼接在一起作为一个序列，并让模型学习在给定指令后生成合适的响应。</p>
<p>这里，在构造模型的input-target pair时，仍然遵循我们之前做预训练时所介绍的向右偏移一位的方法。</p>
<p>也就是说，<strong>指令微调并不改变 GPT 模型的结构本质，而是通过构造“指令+输入+输出”的文本串，让模型在语言建模中学会完成任务式的生成。标签就是这个串整体向右平移一位。</strong></p>
<p>比如，以上面介绍的第二个样本数据为例，将其转换为模型输入的格式，如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">### instruction：</span><br><span class="line">请根据以下文本总结重点内容。</span><br><span class="line"></span><br><span class="line">### input:</span><br><span class="line">Python 是一种广泛使用的编程语言，具有丰富的生态系统。</span><br><span class="line"></span><br><span class="line">### output:</span><br><span class="line">Python 拥有广泛的应用和丰富的生态系统，是一门流行的编程语言。</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这一整个是一个文本序列，也是模型的输入数据格式。</p>
<h1 id="四、构建指令微调的数据加载器"><a href="#四、构建指令微调的数据加载器" class="headerlink" title="四、构建指令微调的数据加载器"></a>四、构建指令微调的数据加载器</h1><p>定义一个数据加载器，将上面1100个样本对转换到模型的输入格式，即：将每一条数据中的 instruction、input 和 output 拼接成一个完整的文本序列。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Below is an instruction that describes a task.  </span><br><span class="line">Write a response that appropriately completes the request.</span><br><span class="line"></span><br><span class="line">### Instruction:</span><br><span class="line">&#123;instruction&#125;</span><br><span class="line"></span><br><span class="line">### Input:</span><br><span class="line">&#123;input&#125;</span><br><span class="line"></span><br><span class="line">### Response:</span><br><span class="line">&#123;output&#125;</span><br></pre></td></tr></table></figure>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">import</span> tiktoken</span><br><span class="line">tokenizer = tiktoken.get_encoding(<span class="string">&quot;gpt2&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义一个 Dataset 类，便于与 DataLoader 配合使用</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">InstructionDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, data, tokenizer</span>):</span></span><br><span class="line">        self.data = data  <span class="comment"># 原始的 instruction 数据（ list of dict）</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 对文本进行预编码，减少训练时重复工作</span></span><br><span class="line">        self.encoded_texts = []</span><br><span class="line">        <span class="keyword">for</span> entry <span class="keyword">in</span> data:</span><br><span class="line">            <span class="comment"># 构造 Prompt 部分（instruction + input）</span></span><br><span class="line">            instruction_plus_input = format_input(entry)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 构造 Response 部分（即模型需要学习生成的内容）</span></span><br><span class="line">            response_text = <span class="string">f&quot;\n\n### Response:\n<span class="subst">&#123;entry[<span class="string">&#x27;output&#x27;</span>]&#125;</span>&quot;</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 拼接完整输入：Prompt + Response</span></span><br><span class="line">            full_text = instruction_plus_input + response_text</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 使用 tokenizer 将文本编码为 token id 序列</span></span><br><span class="line">            self.encoded_texts.append(</span><br><span class="line">                tokenizer.encode(full_text)</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">        <span class="comment"># 返回编码后的 token 序列（暂时未进行 padding）</span></span><br><span class="line">        <span class="keyword">return</span> self.encoded_texts[index]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 数据集中样本数量</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 一个辅助函数，用于构建 Prompt 部分（instruction + optional input）</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">format_input</span>(<span class="params">entry</span>):</span></span><br><span class="line">    <span class="comment"># Instruction 文本（每条样本必须包含）</span></span><br><span class="line">    instruction_text = (</span><br><span class="line">        <span class="string">f&quot;Below is an instruction that describes a task. &quot;</span></span><br><span class="line">        <span class="string">f&quot;Write a response that appropriately completes the request.&quot;</span></span><br><span class="line">        <span class="string">f&quot;\n\n### Instruction:\n<span class="subst">&#123;entry[<span class="string">&#x27;instruction&#x27;</span>]&#125;</span>&quot;</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Input 文本（某些任务可能为空）</span></span><br><span class="line">    input_text = <span class="string">f&quot;\n\n### Input:\n<span class="subst">&#123;entry[<span class="string">&#x27;input&#x27;</span>]&#125;</span>&quot;</span> <span class="keyword">if</span> entry[<span class="string">&quot;input&quot;</span>] <span class="keyword">else</span> <span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回完整的 Prompt 部分（Instruction + Input）</span></span><br><span class="line">    <span class="keyword">return</span> instruction_text + input_text</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>PyTorch的DataLoader会从Dataset（这里是InstructionDataset）中拿到多个样本，然后会调用collate_fn(batch)把这些样本打包成一个batch。默认行为是用default_collate（就是stack）。但对于像GPT这种 输入长度不同、还需要做input-target 对构造的任务，必须自己写一个collate_fn。</p>
<p>custom_collate_fn是在使用DataLoader加载训练数据时，控制“如何把多个样本拼成一个batch”的关键函数。它直接决定了送入模型的输入和目标长什么样。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">custom_collate_fn</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    batch,</span></span></span><br><span class="line"><span class="params"><span class="function">    pad_token_id=<span class="number">50256</span>,         <span class="comment"># 用于 padding 的 token，一般是 &lt;|endoftext|&gt;（GPT 的 pad）</span></span></span></span><br><span class="line"><span class="params"><span class="function">    ignore_index=-<span class="number">100</span>,          <span class="comment"># 用于目标（target）中 mask 掉 padding 区域，避免影响 loss</span></span></span></span><br><span class="line"><span class="params"><span class="function">    allowed_max_length=<span class="literal">None</span>,    <span class="comment"># 可选，限制序列的最大长度，节约资源</span></span></span></span><br><span class="line"><span class="params"><span class="function">    device=<span class="string">&quot;cpu&quot;</span>                </span></span></span><br><span class="line"><span class="params"><span class="function"></span>):</span></span><br><span class="line">    <span class="comment"># 获取当前 batch 中的最大长度（每个样本都添加了一个 endoftext token）</span></span><br><span class="line">    batch_max_length = <span class="built_in">max</span>(<span class="built_in">len</span>(item)+<span class="number">1</span> <span class="keyword">for</span> item <span class="keyword">in</span> batch)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化用于保存最终输入与目标的列表</span></span><br><span class="line">    inputs_lst, targets_lst = [], []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> batch:</span><br><span class="line">        new_item = item.copy()</span><br><span class="line">        <span class="comment"># 在序列末尾添加一个 endoftext token（GPT 的格式）</span></span><br><span class="line">        new_item += [pad_token_id]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将序列 pad 到 batch 中最大长度</span></span><br><span class="line">        padded = (</span><br><span class="line">            new_item + [pad_token_id] *</span><br><span class="line">            (batch_max_length - <span class="built_in">len</span>(new_item))</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 构造输入：去掉最后一个 token（GPT 是用 input 预测下一个 token）</span></span><br><span class="line">        inputs = torch.tensor(padded[:-<span class="number">1</span>])</span><br><span class="line">        <span class="comment"># 构造目标：去掉第一个 token，相当于 input 的右移一位</span></span><br><span class="line">        targets = torch.tensor(padded[<span class="number">1</span>:])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 替换 target 中除第一个 padding 外的所有 padding 为 ignore_index（不参与 loss 计算）</span></span><br><span class="line">        mask = targets == pad_token_id                <span class="comment"># 找到 pad 位置</span></span><br><span class="line">        indices = torch.nonzero(mask).squeeze()       <span class="comment"># 获取 pad 的索引</span></span><br><span class="line">        <span class="keyword">if</span> indices.numel() &gt; <span class="number">1</span>:                       <span class="comment"># 如果有多个 padding</span></span><br><span class="line">            targets[indices[<span class="number">1</span>:]] = ignore_index       <span class="comment"># 忽略除第一个之外的所有 padding 区域</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果设置了最大序列长度，进一步裁剪 input 和 target</span></span><br><span class="line">        <span class="keyword">if</span> allowed_max_length <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            inputs = inputs[:allowed_max_length]</span><br><span class="line">            targets = targets[:allowed_max_length]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 添加到 batch 列表</span></span><br><span class="line">        inputs_lst.append(inputs)</span><br><span class="line">        targets_lst.append(targets)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将所有样本拼接成一个 batch，并移动到指定设备</span></span><br><span class="line">    inputs_tensor = torch.stack(inputs_lst).to(device)</span><br><span class="line">    targets_tensor = torch.stack(targets_lst).to(device)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> inputs_tensor, targets_tensor</span><br></pre></td></tr></table></figure>

<p>举个例子，假设 InstructionDataset 里有两条样本：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">batch = [</span><br><span class="line">    [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],         <span class="comment"># 第一条数据，token_ids</span></span><br><span class="line">    [<span class="number">4</span>, <span class="number">5</span>]             <span class="comment"># 第二条数据，短一点</span></span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<p>设置：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pad_token_id = <span class="number">50256</span></span><br><span class="line">ignore_index = -<span class="number">100</span></span><br></pre></td></tr></table></figure>

<p>以下是将batch使用custom_collate_fn进行转换的中间变化过程：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">1. 加 &lt;|endoftext|&gt; 后：</span><br><span class="line"></span><br><span class="line">[1, 2, 3, 50256]</span><br><span class="line">[4, 5, 50256]</span><br><span class="line"></span><br><span class="line">2. 补齐（最长是 4）：</span><br><span class="line"></span><br><span class="line">[1, 2, 3, 50256]        # 已满，无需 padding</span><br><span class="line">[4, 5, 50256, 50256]    # 多加一个 pad</span><br><span class="line"></span><br><span class="line">3. 构造 input 和 target（分别左移 &amp; 右移）：</span><br><span class="line"></span><br><span class="line">inputs:  [1, 2, 3, 50256]       target: [2, 3, 50256, 50256]</span><br><span class="line">inputs:  [4, 5, 50256, 50256]   target: [5, 50256, 50256, 50256]</span><br><span class="line"></span><br><span class="line">4. 把 target 中除了第一个 pad 的地方都替换为 -100（ignore_index）：</span><br><span class="line"></span><br><span class="line">target: [2, 3, 50256, -100]</span><br><span class="line">target: [5, 50256, -100, -100]</span><br><span class="line"></span><br><span class="line">5. 最后组成两个张量传给模型训练：</span><br><span class="line">inputs_tensor.shape  =&gt; [batch_size=2, seq_len=4]</span><br><span class="line">targets_tensor.shape =&gt; [batch_size=2, seq_len=4]</span><br></pre></td></tr></table></figure>

<p>ok，现在数据加载器所需的组件已经准备好了，把他们组合在一起，构建数据加载器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> partial</span><br><span class="line">customized_collate_fn = partial(custom_collate_fn, device=device, allowed_max_length=<span class="number">1024</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">num_workers = <span class="number">0</span></span><br><span class="line">batch_size = <span class="number">8</span></span><br><span class="line"></span><br><span class="line">train_dataset = InstructionDataset(train_data, tokenizer)</span><br><span class="line">train_loader = DataLoader(</span><br><span class="line">    train_dataset,</span><br><span class="line">    batch_size=batch_size,</span><br><span class="line">    collate_fn=customized_collate_fn,</span><br><span class="line">    shuffle=<span class="literal">True</span>,</span><br><span class="line">    drop_last=<span class="literal">True</span>,</span><br><span class="line">    num_workers=num_workers</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">val_dataset = InstructionDataset(val_data, tokenizer)</span><br><span class="line">val_loader = DataLoader(</span><br><span class="line">    val_dataset,</span><br><span class="line">    batch_size=batch_size,</span><br><span class="line">    collate_fn=customized_collate_fn,</span><br><span class="line">    shuffle=<span class="literal">False</span>,</span><br><span class="line">    drop_last=<span class="literal">False</span>,</span><br><span class="line">    num_workers=num_workers</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">test_dataset = InstructionDataset(test_data, tokenizer)</span><br><span class="line">test_loader = DataLoader(</span><br><span class="line">    test_dataset,</span><br><span class="line">    batch_size=batch_size,</span><br><span class="line">    collate_fn=customized_collate_fn,</span><br><span class="line">    shuffle=<span class="literal">False</span>,</span><br><span class="line">    drop_last=<span class="literal">False</span>,</span><br><span class="line">    num_workers=num_workers</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>实例化查看数据加载器的输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Train loader:&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> inputs, targets <span class="keyword">in</span> train_loader:</span><br><span class="line">    <span class="built_in">print</span>(inputs.shape, targets.shape)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Train loader:</span><br><span class="line">torch.Size([8, 61]) torch.Size([8, 61])</span><br><span class="line">torch.Size([8, 76]) torch.Size([8, 76])</span><br><span class="line">torch.Size([8, 73]) torch.Size([8, 73])</span><br><span class="line">torch.Size([8, 68]) torch.Size([8, 68])</span><br><span class="line">torch.Size([8, 65]) torch.Size([8, 65])</span><br><span class="line">torch.Size([8, 72]) torch.Size([8, 72])</span><br><span class="line">torch.Size([8, 80]) torch.Size([8, 80])</span><br><span class="line">torch.Size([8, 67]) torch.Size([8, 67])</span><br><span class="line">torch.Size([8, 62]) torch.Size([8, 62])</span><br><span class="line">torch.Size([8, 75]) torch.Size([8, 75])</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>可以看到，每一个batch的最大序列长度会有所不同，这个最大长度就是当前batch中最长序列的长度，其它较短的序列会被padding以匹配这个最大长度。</p>
<p>这种方式能够确保每个batch的处理是动态的，避免了固定max_length可能带来的浪费（如果固定长度过长，很多短序列会浪费计算资源；如果固定长度过短，则会导致信息丢失）。同时，通过动态计算batch内最长序列长度，而不是统一使用一个较大的固定长度，可以更高效地利用内存。</p>
<h1 id="五、LLM的指令微调"><a href="#五、LLM的指令微调" class="headerlink" title="五、LLM的指令微调"></a>五、LLM的指令微调</h1><p>这一部分和之前的预训练代码几乎一致，这里直接搬过来：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_loss_batch</span>(<span class="params">input_batch, target_batch, model, device</span>):</span></span><br><span class="line">    input_batch, target_batch = input_batch.to(device), target_batch.to(device)</span><br><span class="line">    logits = model(input_batch)</span><br><span class="line">    loss = torch.nn.functional.cross_entropy(logits.flatten(<span class="number">0</span>, <span class="number">1</span>), target_batch.flatten())</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_loss_loader</span>(<span class="params">data_loader, model, device, num_batches=<span class="literal">None</span></span>):</span></span><br><span class="line">    total_loss = <span class="number">0.</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(data_loader) == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">float</span>(<span class="string">&quot;nan&quot;</span>)</span><br><span class="line">    <span class="keyword">elif</span> num_batches <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        num_batches = <span class="built_in">len</span>(data_loader)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># Reduce the number of batches to match the total number of batches in the data loader</span></span><br><span class="line">        <span class="comment"># if num_batches exceeds the number of batches in the data loader</span></span><br><span class="line">        num_batches = <span class="built_in">min</span>(num_batches, <span class="built_in">len</span>(data_loader))</span><br><span class="line">    <span class="keyword">for</span> i, (input_batch, target_batch) <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_loader):</span><br><span class="line">        <span class="keyword">if</span> i &lt; num_batches:</span><br><span class="line">            loss = calc_loss_batch(input_batch, target_batch, model, device)</span><br><span class="line">            total_loss += loss.item()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> total_loss / num_batches</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model_simple</span>(<span class="params">model, train_loader, val_loader, optimizer, device, num_epochs,</span></span></span><br><span class="line"><span class="params"><span class="function">                       eval_freq, eval_iter, start_context, tokenizer</span>):</span></span><br><span class="line">    <span class="comment"># Initialize lists to track losses and tokens seen</span></span><br><span class="line">    train_losses, val_losses, track_tokens_seen = [], [], []</span><br><span class="line">    tokens_seen, global_step = <span class="number">0</span>, -<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Main training loop</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        model.train()  <span class="comment"># Set model to training mode</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> input_batch, target_batch <span class="keyword">in</span> train_loader:</span><br><span class="line">            optimizer.zero_grad() <span class="comment"># Reset loss gradients from previous batch iteration</span></span><br><span class="line">            loss = calc_loss_batch(input_batch, target_batch, model, device)</span><br><span class="line">            loss.backward() <span class="comment"># Calculate loss gradients</span></span><br><span class="line">            optimizer.step() <span class="comment"># Update model weights using loss gradients</span></span><br><span class="line">            tokens_seen += input_batch.numel() <span class="comment"># Returns the total number of elements (or tokens) in the input_batch.</span></span><br><span class="line">            global_step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Optional evaluation step</span></span><br><span class="line">            <span class="keyword">if</span> global_step % eval_freq == <span class="number">0</span>: </span><br><span class="line">                train_loss, val_loss = evaluate_model(</span><br><span class="line">                    model, train_loader, val_loader, device, eval_iter)</span><br><span class="line">                train_losses.append(train_loss)</span><br><span class="line">                val_losses.append(val_loss)</span><br><span class="line">                track_tokens_seen.append(tokens_seen)</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;Ep <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span> (Step <span class="subst">&#123;global_step:06d&#125;</span>): &quot;</span></span><br><span class="line">                      <span class="string">f&quot;Train loss <span class="subst">&#123;train_loss:<span class="number">.3</span>f&#125;</span>, Val loss <span class="subst">&#123;val_loss:<span class="number">.3</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Print a sample text after each epoch</span></span><br><span class="line">        generate_and_print_sample(</span><br><span class="line">            model, tokenizer, device, start_context</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_losses, val_losses, track_tokens_seen</span><br></pre></td></tr></table></figure>

<p>开始指令微调，这里作为演示只训练一个epoch：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">start_time = time.time()</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.AdamW(model.parameters(), lr=<span class="number">0.00005</span>, weight_decay=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">num_epochs = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">train_losses, val_losses, tokens_seen = train_model_simple(</span><br><span class="line">    model, train_loader, val_loader, optimizer, device,</span><br><span class="line">    num_epochs=num_epochs, eval_freq=<span class="number">5</span>, eval_iter=<span class="number">5</span>,</span><br><span class="line">    start_context=format_input(val_data[<span class="number">0</span>]), tokenizer=tokenizer</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">end_time = time.time()</span><br><span class="line">execution_time_minutes = (end_time - start_time) / <span class="number">60</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Training completed in <span class="subst">&#123;execution_time_minutes:<span class="number">.2</span>f&#125;</span> minutes.&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>可视化loss曲线：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.ticker <span class="keyword">import</span> MaxNLocator</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_losses</span>(<span class="params">epochs_seen, tokens_seen, train_losses, val_losses</span>):</span></span><br><span class="line">    fig, ax1 = plt.subplots(figsize=(<span class="number">5</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot training and validation loss against epochs</span></span><br><span class="line">    ax1.plot(epochs_seen, train_losses, label=<span class="string">&quot;Training loss&quot;</span>)</span><br><span class="line">    ax1.plot(epochs_seen, val_losses, linestyle=<span class="string">&quot;-.&quot;</span>, label=<span class="string">&quot;Validation loss&quot;</span>)</span><br><span class="line">    ax1.set_xlabel(<span class="string">&quot;Epochs&quot;</span>)</span><br><span class="line">    ax1.set_ylabel(<span class="string">&quot;Loss&quot;</span>)</span><br><span class="line">    ax1.legend(loc=<span class="string">&quot;upper right&quot;</span>)</span><br><span class="line">    ax1.xaxis.set_major_locator(MaxNLocator(integer=<span class="literal">True</span>))  <span class="comment"># only show integer labels on x-axis</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create a second x-axis for tokens seen</span></span><br><span class="line">    ax2 = ax1.twiny()  <span class="comment"># Create a second x-axis that shares the same y-axis</span></span><br><span class="line">    ax2.plot(tokens_seen, train_losses, alpha=<span class="number">0</span>)  <span class="comment"># Invisible plot for aligning ticks</span></span><br><span class="line">    ax2.set_xlabel(<span class="string">&quot;Tokens seen&quot;</span>)</span><br><span class="line"></span><br><span class="line">    fig.tight_layout()  <span class="comment"># Adjust layout to make room</span></span><br><span class="line">    plt.savefig(<span class="string">&quot;loss-plot.pdf&quot;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">epochs_tensor = torch.linspace(<span class="number">0</span>, num_epochs, <span class="built_in">len</span>(train_losses))</span><br><span class="line">plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)</span><br></pre></td></tr></table></figure>
<p><img src="2.png"></p>
<p>可以看到，随着训练的进行，loss正常下降。</p>
<h1 id="六、推理"><a href="#六、推理" class="headerlink" title="六、推理"></a>六、推理</h1><p>现在，使用经过指令微调的模型进行一些推理测试：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i, entry <span class="keyword">in</span> tqdm(<span class="built_in">enumerate</span>(test_data), total=<span class="built_in">len</span>(test_data)):</span><br><span class="line"></span><br><span class="line">    input_text = format_input(entry)</span><br><span class="line"></span><br><span class="line">    token_ids = generate(</span><br><span class="line">        model=model,</span><br><span class="line">        idx=text_to_token_ids(input_text, tokenizer).to(device),</span><br><span class="line">        max_new_tokens=<span class="number">256</span>,</span><br><span class="line">        context_size=BASE_CONFIG[<span class="string">&quot;context_length&quot;</span>],</span><br><span class="line">        eos_id=<span class="number">50256</span></span><br><span class="line">    )</span><br><span class="line">    generated_text = token_ids_to_text(token_ids, tokenizer)</span><br><span class="line">    response_text = generated_text[<span class="built_in">len</span>(input_text):].replace(<span class="string">&quot;### Response:&quot;</span>, <span class="string">&quot;&quot;</span>).strip()</span><br><span class="line"></span><br><span class="line">    test_data[i][<span class="string">&quot;model_response&quot;</span>] = response_text</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(input_text)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;\nCorrect response:\n&gt;&gt; <span class="subst">&#123;entry[<span class="string">&#x27;output&#x27;</span>]&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;\nModel response:\n&gt;&gt; <span class="subst">&#123;response_text.strip()&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;-------------------------------------&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;instruction-data-with-response.json&quot;</span>, <span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> file:</span><br><span class="line">    json.dump(test_data, file, indent=<span class="number">4</span>)  <span class="comment"># &quot;indent&quot; for pretty-printing</span></span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">Below is an instruction that describes a task. Write a response that appropriately completes the request.</span><br><span class="line"></span><br><span class="line">### Instruction:</span><br><span class="line">Rewrite the sentence using a simile.</span><br><span class="line"></span><br><span class="line">### Input:</span><br><span class="line">The car is very fast.</span><br><span class="line"></span><br><span class="line">Correct response:</span><br><span class="line">&gt;&gt; The car is as fast as lightning.</span><br><span class="line"></span><br><span class="line">Model response:</span><br><span class="line">&gt;&gt; The car is as fast as a bullet.</span><br><span class="line">-------------------------------------</span><br><span class="line">Below is an instruction that describes a task. Write a response that appropriately completes the request.</span><br><span class="line"></span><br><span class="line">### Instruction:</span><br><span class="line">What type of cloud is typically associated with thunderstorms?</span><br><span class="line"></span><br><span class="line">Correct response:</span><br><span class="line">&gt;&gt; The type of cloud typically associated with thunderstorms is cumulonimbus.</span><br><span class="line"></span><br><span class="line">Model response:</span><br><span class="line">&gt;&gt; A thunderstorm is a type of cloud that typically forms in the atmosphere over a region of high pressure. It typically produces a strong wind that blows across the area, creating a dense, dense cloud.</span><br><span class="line">-------------------------------------</span><br><span class="line">Below is an instruction that describes a task. Write a response that appropriately completes the request.</span><br><span class="line"></span><br><span class="line">### Instruction:</span><br><span class="line">Name the author of &#x27;Pride and Prejudice&#x27;.</span><br><span class="line"></span><br><span class="line">Correct response:</span><br><span class="line">&gt;&gt; Jane Austen.</span><br><span class="line"></span><br><span class="line">Model response:</span><br><span class="line">&gt;&gt; The author of &#x27;Pride and Prejudice&#x27; is George Bernard Shaw.</span><br><span class="line">-------------------------------------</span><br></pre></td></tr></table></figure>

<p>这里，我们将模型的输出保存到了本地。在下一节，我们将使用这些输出结果，评估本节指令微调得到的LLM的性能，欢迎持续关注。</p>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/LLM/">LLM</a></div><div class="post_share"><div class="social-share" data-image="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2025/03/23/%E5%BE%AE%E8%B0%83LLM%E5%9E%83%E5%9C%BE%E9%82%AE%E4%BB%B6%E5%88%86%E7%B1%BB%E5%99%A8/"><img class="next-cover" src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">LLM微调：训练一个垃圾邮件分类器-大模型炼丹术(七)</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2025/03/11/LLM%E8%87%AA%E5%9B%9E%E5%BD%92%E9%A2%84%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E8%AF%A6%E8%A7%A3-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%BA%94)/" title="LLM自回归预训练过程详解-大模型炼丹术(五)"><img class="cover" src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-11</div><div class="title">LLM自回归预训练过程详解-大模型炼丹术(五)</div></div></a></div><div><a href="/2025/03/04/%E4%BB%8E%E5%8D%95%E5%A4%B4%E5%88%B0%E5%A4%9A%E5%A4%B4%EF%BC%8C%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%B8%89)/" title="从单头到多头，深度解析大模型的注意力机制-大模型炼丹术(三)"><img class="cover" src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-04</div><div class="title">从单头到多头，深度解析大模型的注意力机制-大模型炼丹术(三)</div></div></a></div><div><a href="/2025/02/20/%E4%BB%8Etokenizer%E8%AF%B4%E8%B5%B7%EF%BC%8C%E4%B8%BALLM%E8%87%AA%E5%9B%9E%E5%BD%92%E9%A2%84%E8%AE%AD%E7%BB%83%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE%E9%9B%86-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%B8%80)/" title="从tokenizer说起，为LLM自回归预训练准备数据集-大模型炼丹术(一)"><img class="cover" src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-02-20</div><div class="title">从tokenizer说起，为LLM自回归预训练准备数据集-大模型炼丹术(一)</div></div></a></div><div><a href="/2025/02/24/%E4%BB%8E%E7%A6%BB%E6%95%A3%E7%9A%84tokenID%E5%88%B0%E5%85%B7%E6%9C%89%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%E7%9A%84embedding-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%BA%8C)/" title="从离散的token IDs到具有语义信息的embedding-大模型炼丹术(二)"><img class="cover" src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-02-24</div><div class="title">从离散的token IDs到具有语义信息的embedding-大模型炼丹术(二)</div></div></a></div><div><a href="/2025/03/12/%E5%89%96%E6%9E%90LLM%E7%9A%84%E8%A7%A3%E7%A0%81%E7%AD%96%E7%95%A5-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E5%85%AD)/" title="剖析LLM的解码策略-大模型炼丹术(六)"><img class="cover" src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-12</div><div class="title">剖析LLM的解码策略-大模型炼丹术(六)</div></div></a></div><div><a href="/2025/03/07/%E5%8A%A8%E6%89%8B%E6%90%AD%E5%BB%BAGPT2%E6%9E%B6%E6%9E%84-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E5%9B%9B)/" title="动手搭建GPT2架构-大模型炼丹术(四)"><img class="cover" src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-07</div><div class="title">动手搭建GPT2架构-大模型炼丹术(四)</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://www.cdnjson.com/images/2021/11/27/_20210211193948.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">雨落诗山山亦奇</div><div class="author-info__description">本站为读研版&工作版博客，大学版移步 --> fuhanshi.github.io</div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">173</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">本站内容的最终版本将发布在微信公众号[南极Python]</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E4%BB%80%E4%B9%88%E6%98%AF%E6%8C%87%E4%BB%A4%E5%BE%AE%E8%B0%83%EF%BC%9F"><span class="toc-number">1.</span> <span class="toc-text">一、什么是指令微调？</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E6%8C%87%E4%BB%A4%E5%BE%AE%E8%B0%83%E7%9A%84%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F-%E9%99%84%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8B%E8%BD%BD%E9%93%BE%E6%8E%A5"><span class="toc-number">2.</span> <span class="toc-text">二、指令微调的数据格式(附数据集下载链接)</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E5%A6%82%E4%BD%95%E8%BD%AC%E6%88%90%E6%A8%A1%E5%9E%8B%E8%BE%93%E5%85%A5%E6%A0%BC%E5%BC%8F%EF%BC%9F"><span class="toc-number">3.</span> <span class="toc-text">三、如何转成模型输入格式？</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E6%9E%84%E5%BB%BA%E6%8C%87%E4%BB%A4%E5%BE%AE%E8%B0%83%E7%9A%84%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E5%99%A8"><span class="toc-number">4.</span> <span class="toc-text">四、构建指令微调的数据加载器</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%94%E3%80%81LLM%E7%9A%84%E6%8C%87%E4%BB%A4%E5%BE%AE%E8%B0%83"><span class="toc-number">5.</span> <span class="toc-text">五、LLM的指令微调</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%AD%E3%80%81%E6%8E%A8%E7%90%86"><span class="toc-number">6.</span> <span class="toc-text">六、推理</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/04/10/%E6%8C%87%E4%BB%A4%E5%BE%AE%E8%B0%83/" title="LLM指令微调：训练一个人工智能助手-大模型炼丹术(八)"><img src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="LLM指令微调：训练一个人工智能助手-大模型炼丹术(八)"/></a><div class="content"><a class="title" href="/2025/04/10/%E6%8C%87%E4%BB%A4%E5%BE%AE%E8%B0%83/" title="LLM指令微调：训练一个人工智能助手-大模型炼丹术(八)">LLM指令微调：训练一个人工智能助手-大模型炼丹术(八)</a><time datetime="2025-04-10T14:32:21.000Z" title="发表于 2025-04-10 22:32:21">2025-04-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/03/23/%E5%BE%AE%E8%B0%83LLM%E5%9E%83%E5%9C%BE%E9%82%AE%E4%BB%B6%E5%88%86%E7%B1%BB%E5%99%A8/" title="LLM微调：训练一个垃圾邮件分类器-大模型炼丹术(七)"><img src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="LLM微调：训练一个垃圾邮件分类器-大模型炼丹术(七)"/></a><div class="content"><a class="title" href="/2025/03/23/%E5%BE%AE%E8%B0%83LLM%E5%9E%83%E5%9C%BE%E9%82%AE%E4%BB%B6%E5%88%86%E7%B1%BB%E5%99%A8/" title="LLM微调：训练一个垃圾邮件分类器-大模型炼丹术(七)">LLM微调：训练一个垃圾邮件分类器-大模型炼丹术(七)</a><time datetime="2025-03-23T13:48:46.000Z" title="发表于 2025-03-23 21:48:46">2025-03-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/03/12/%E5%89%96%E6%9E%90LLM%E7%9A%84%E8%A7%A3%E7%A0%81%E7%AD%96%E7%95%A5-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E5%85%AD)/" title="剖析LLM的解码策略-大模型炼丹术(六)"><img src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="剖析LLM的解码策略-大模型炼丹术(六)"/></a><div class="content"><a class="title" href="/2025/03/12/%E5%89%96%E6%9E%90LLM%E7%9A%84%E8%A7%A3%E7%A0%81%E7%AD%96%E7%95%A5-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E5%85%AD)/" title="剖析LLM的解码策略-大模型炼丹术(六)">剖析LLM的解码策略-大模型炼丹术(六)</a><time datetime="2025-03-12T13:40:40.000Z" title="发表于 2025-03-12 21:40:40">2025-03-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/03/11/LLM%E8%87%AA%E5%9B%9E%E5%BD%92%E9%A2%84%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E8%AF%A6%E8%A7%A3-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%BA%94)/" title="LLM自回归预训练过程详解-大模型炼丹术(五)"><img src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="LLM自回归预训练过程详解-大模型炼丹术(五)"/></a><div class="content"><a class="title" href="/2025/03/11/LLM%E8%87%AA%E5%9B%9E%E5%BD%92%E9%A2%84%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E8%AF%A6%E8%A7%A3-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%BA%94)/" title="LLM自回归预训练过程详解-大模型炼丹术(五)">LLM自回归预训练过程详解-大模型炼丹术(五)</a><time datetime="2025-03-11T13:31:10.000Z" title="发表于 2025-03-11 21:31:10">2025-03-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/03/07/%E5%8A%A8%E6%89%8B%E6%90%AD%E5%BB%BAGPT2%E6%9E%B6%E6%9E%84-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E5%9B%9B)/" title="动手搭建GPT2架构-大模型炼丹术(四)"><img src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="动手搭建GPT2架构-大模型炼丹术(四)"/></a><div class="content"><a class="title" href="/2025/03/07/%E5%8A%A8%E6%89%8B%E6%90%AD%E5%BB%BAGPT2%E6%9E%B6%E6%9E%84-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E5%9B%9B)/" title="动手搭建GPT2架构-大模型炼丹术(四)">动手搭建GPT2架构-大模型炼丹术(四)</a><time datetime="2025-03-07T15:35:12.000Z" title="发表于 2025-03-07 23:35:12">2025-03-07</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By 雨落诗山山亦奇</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/fireworks.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>