<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>从离散的token IDs到具有语义信息的embedding-大模型炼丹术(二) | 南极Python</title><meta name="keywords" content="LLM"><meta name="author" content="雨落诗山山亦奇"><meta name="copyright" content="雨落诗山山亦奇"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="在完成了tokenization之后，我们已经可以将一个个的单词映射到对应的数字，称之为token ID，这些数字已经可以被计算机处理。然而，若直接将这些数字应用于模型训练，仍存在一些问题：  缺乏语义信息： Token ID 只是一个索引，本身不包含任何语义信息。例如，“cat” 可能被映射为 ID 1254，而 “dog” 是 ID 3920，这两个 ID 之间的数值关系是无意义的。直接使用它">
<meta property="og:type" content="article">
<meta property="og:title" content="从离散的token IDs到具有语义信息的embedding-大模型炼丹术(二)">
<meta property="og:url" content="http://yoursite.com/2025/02/24/%E4%BB%8E%E7%A6%BB%E6%95%A3%E7%9A%84tokenID%E5%88%B0%E5%85%B7%E6%9C%89%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%E7%9A%84embedding-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%BA%8C)/index.html">
<meta property="og:site_name" content="南极Python">
<meta property="og:description" content="在完成了tokenization之后，我们已经可以将一个个的单词映射到对应的数字，称之为token ID，这些数字已经可以被计算机处理。然而，若直接将这些数字应用于模型训练，仍存在一些问题：  缺乏语义信息： Token ID 只是一个索引，本身不包含任何语义信息。例如，“cat” 可能被映射为 ID 1254，而 “dog” 是 ID 3920，这两个 ID 之间的数值关系是无意义的。直接使用它">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg">
<meta property="article:published_time" content="2025-02-24T14:28:29.000Z">
<meta property="article:modified_time" content="2025-02-24T13:02:15.088Z">
<meta property="article:author" content="雨落诗山山亦奇">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg"><link rel="shortcut icon" href="https://www.cdnjson.com/images/2021/11/27/_20210211193948.png"><link rel="canonical" href="http://yoursite.com/2025/02/24/%E4%BB%8E%E7%A6%BB%E6%95%A3%E7%9A%84tokenID%E5%88%B0%E5%85%B7%E6%9C%89%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%E7%9A%84embedding-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%BA%8C)/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '从离散的token IDs到具有语义信息的embedding-大模型炼丹术(二)',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-02-24 21:02:15'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/StaticFile_HEXO@latest/butterfly/css/macblack.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://www.cdnjson.com/images/2021/11/27/_20210211193948.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">167</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div></div><hr/></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">南极Python</a></span><div id="menus"><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">从离散的token IDs到具有语义信息的embedding-大模型炼丹术(二)</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-02-24T14:28:29.000Z" title="发表于 2025-02-24 22:28:29">2025-02-24</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-02-24T13:02:15.088Z" title="更新于 2025-02-24 21:02:15">2025-02-24</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF/">大模型炼丹术</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">2.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>9分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="从离散的token IDs到具有语义信息的embedding-大模型炼丹术(二)"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>在完成了tokenization之后，我们已经可以将一个个的单词映射到对应的数字，称之为token ID，这些数字已经可以被计算机处理。然而，若直接将这些数字应用于模型训练，仍存在一些问题：</p>
<ol>
<li><p>缺乏语义信息：<br> Token ID 只是一个索引，本身不包含任何语义信息。例如，“cat” 可能被映射为 ID 1254，而 “dog” 是 ID 3920，这两个 ID 之间的数值关系是无意义的。直接使用它们可能会导致模型误解 token 之间的关联性。</p>
</li>
<li><p>整数之间的数值关系会误导模型：<br> 机器学习模型通常会学习数据之间的模式。如果直接输入 token ID，模型可能会误以为 ID 1254（”cat”）和 ID 3920（”dog”）之间存在某种数学关系（如加减乘除），但实际上 ID 只是索引，没有数值上的逻辑关系。</p>
</li>
<li><p>无法捕捉相似 token 的关系<br> 语义相近的 token 在 embedding 空间中应该具有接近的表示。例如，”king” 和 “queen” 应该在高维空间中比较接近，而 “apple” 和 “computer” 应该相距较远。然而，单纯的 token ID 无法提供这种分布信息。</p>
</li>
</ol>
<h1 id="一、什么是Token-Embedding"><a href="#一、什么是Token-Embedding" class="headerlink" title="一、什么是Token Embedding ?"></a>一、什么是Token Embedding ?</h1><p>Token Embedding（标记嵌入）是将离散的 token（如单词、子词或字符）转换为连续的向量表示的过程。在自然语言处理（NLP）任务中，神经网络无法直接处理文本，需要将文本转换为有语义特征的数字形式，而 Token Embedding 就是这一转换的核心步骤。</p>
<p>在PyTorch中，使用<code>torch.nn.Embedding</code>来构建embedding层，实际上，这是一个查找表，key是token id，value是token id对应的embedding 向量。</p>
<p>举个例子，假设词汇表大小是6，embedding 维度是3，那么，构建的embedding layer的weight 的shape是<code>6x3</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建embedding layer</span></span><br><span class="line">vocab_size = <span class="number">6</span></span><br><span class="line">output_dim = <span class="number">3</span></span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">123</span>)</span><br><span class="line">embedding_layer = torch.nn.Embedding(vocab_size, output_dim)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(embedding_layer.weight)<span class="comment"># 6x3</span></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Parameter containing:</span><br><span class="line">tensor([[ 0.3374, -0.1778, -0.1690],</span><br><span class="line">        [ 0.9178,  1.5810,  1.3010],</span><br><span class="line">        [ 1.2753, -0.2010, -0.1606],</span><br><span class="line">        [-0.4015,  0.9666, -1.1481],</span><br><span class="line">        [-1.1589,  0.3255, -0.6315],</span><br><span class="line">        [-2.8400, -0.7849, -1.4096]], requires_grad=True)</span><br></pre></td></tr></table></figure>
<p>这个就是embedding层的weight，也就是一个查找表，它有6行3列，6是词汇表大小，3是embedding层的维度。</p>
<p>假设输入句子转换到token id后如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">input_ids = torch.tensor([<span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>其中，2，3，5，1其实就是要在当前查找表中查询的索引，因此</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(embedding_layer(input_ids))</span><br></pre></td></tr></table></figure>
<p>的输出如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 1.2753, -0.2010, -0.1606],</span><br><span class="line">        [-0.4015,  0.9666, -1.1481],</span><br><span class="line">        [-2.8400, -0.7849, -1.4096],</span><br><span class="line">        [ 0.9178,  1.5810,  1.3010]], grad_fn=&lt;EmbeddingBackward0&gt;)</span><br></pre></td></tr></table></figure>

<p>到这里，我们已经可以将原始的输入文本转换成embedding向量，具体包括两个过程：</p>
<ul>
<li><p>1）先通过 Tokenization 将文本拆分为 token ID</p>
</li>
<li><p>2）然后通过 Token Embedding 层 获取 token 的连续表示，即embedding</p>
</li>
</ul>
<h1 id="二、如何训练Embedding层"><a href="#二、如何训练Embedding层" class="headerlink" title="二、如何训练Embedding层 ?"></a>二、如何训练Embedding层 ?</h1><p>Embedding 层本质上是一个可学习的查找表，它的核心是一个嵌入矩阵（Embedding Matrix），用于将离散的 token ID 映射到连续的高维向量。这个矩阵的大小通常是：</p>
<p>在训练过程中（比如做文本分类，有文本的类别标签），模型会自动更新 Embedding 矩阵的权重，使得具有相似语义的 token 向量接近，而不相关的 token 向量距离较远。</p>
<p>一般来说，在构建自己的任务时，会加载预训练的一些embedding，这样可以充分利用已有的大规模语料训练的语义信息，提高模型的性能，并加速收敛。</p>
<h1 id="三、nn-Embedding-和-nn-Linear的区别是什么"><a href="#三、nn-Embedding-和-nn-Linear的区别是什么" class="headerlink" title="三、nn.Embedding 和 nn.Linear的区别是什么 ?"></a>三、nn.Embedding 和 nn.Linear的区别是什么 ?</h1><p>Embedding 相当于一个固定输入为 one-hot 向量的 Linear 层。</p>
<p>假设 num_embeddings=10, embedding_dim=4，那么 Embedding 其实是一个形状为 [10, 4] 的权重矩阵 E。</p>
<p>如果用 Linear 模拟 Embedding：</p>
<p>先把 token ID 转换成 one-hot 向量（形状 [10]）。<br>然后进行 矩阵乘法（one-hot 只会选中对应的行，相当于输入的X是1，与weight matrix行相乘的结果就是weight matrix行）。</p>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个相当于 nn.Embedding 的 Linear 层</span></span><br><span class="line">embedding_as_linear = nn.Linear(in_features=<span class="number">10</span>, out_features=<span class="number">4</span>, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成一个 one-hot 形式的输入（假设词汇表大小 10）</span></span><br><span class="line">one_hot_input = torch.eye(<span class="number">10</span>)[[<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>]]  <span class="comment"># 选中索引 1、3、5</span></span><br><span class="line"><span class="built_in">print</span>(one_hot_input.shape)  <span class="comment"># (3, 10)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行计算</span></span><br><span class="line">word_vectors = embedding_as_linear(one_hot_input)</span><br><span class="line"><span class="built_in">print</span>(word_vectors.shape)  <span class="comment"># (3, 4)</span></span><br></pre></td></tr></table></figure>

<p>其中：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">embedding_as_linear.weight.T:</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[ 0.2629, -0.0249, -0.0409, -0.1259],</span><br><span class="line">        [-0.0418,  0.0100, -0.2939,  0.0127],</span><br><span class="line">        [-0.2674, -0.0493, -0.1956, -0.0738],</span><br><span class="line">        [-0.0907,  0.0497,  0.2699, -0.0348],</span><br><span class="line">        [-0.2227,  0.2818,  0.0189, -0.3083],</span><br><span class="line">        [ 0.0209,  0.1934, -0.2562,  0.1481],</span><br><span class="line">        [-0.0590,  0.1122,  0.0499,  0.2776],</span><br><span class="line">        [-0.1696,  0.0687,  0.2613,  0.1933],</span><br><span class="line">        [-0.0288,  0.0746, -0.2988, -0.2239],</span><br><span class="line">        [ 0.2996,  0.1222, -0.2129, -0.2549]], grad_fn=&lt;PermuteBackward0&gt;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">one_hot_input：</span><br><span class="line">tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],</span><br><span class="line">        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],</span><br><span class="line">        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">word_vectors:</span><br><span class="line">tensor([[-0.0418,  0.0100, -0.2939,  0.0127],</span><br><span class="line">        [-0.0907,  0.0497,  0.2699, -0.0348],</span><br><span class="line">        [ 0.0209,  0.1934, -0.2562,  0.1481]], grad_fn=&lt;MmBackward0&gt;)</span><br></pre></td></tr></table></figure>

<h1 id="四、为什么还需要Positinal-Embedding？"><a href="#四、为什么还需要Positinal-Embedding？" class="headerlink" title="四、为什么还需要Positinal Embedding？"></a>四、为什么还需要Positinal Embedding？</h1><p>位置编码（Positional Encoding，PE）是一种用于在无序输入（如 Transformer）中引入位置信息的技术，其编码得到的向量称之为Positinal Embedding。由于 Transformer 不使用循环（RNN）或卷积（CNN），它无法捕捉序列顺序，因此需要额外的信息来表示单词的顺序。</p>
<p>Transformer 采用自注意力机制（Self-Attention），它本质上是对输入进行加权求和，不考虑输入的顺序。例如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">句子1：“小猫在沙发上睡觉。”</span><br><span class="line">句子2：“沙发在小猫上睡觉。”</span><br></pre></td></tr></table></figure>
<p>在 LSTM/RNN 中，单词的顺序 会通过循环网络的隐状态传递体现出来，而在 Transformer 中，所有单词是同时处理的，所以：</p>
<ul>
<li>没有位置编码：模型会认为两句话的含义是一样的！</li>
<li>有位置编码：模型能够识别单词的顺序，从而理解句子的语义。</li>
</ul>
<h1 id="五、常见的位置编码方式"><a href="#五、常见的位置编码方式" class="headerlink" title="五、常见的位置编码方式"></a>五、常见的位置编码方式</h1><h2 id="1-按照参数是否可以学习来分类"><a href="#1-按照参数是否可以学习来分类" class="headerlink" title="1. 按照参数是否可以学习来分类"></a>1. 按照参数是否可以学习来分类</h2><p>按照参数是否是可学习的，可以分为：可学习的（Learnable）位置编码，固定的（Sinusoidal）位置编码。</p>
<h3 id="可学习的（Learnable）位置编码"><a href="#可学习的（Learnable）位置编码" class="headerlink" title="可学习的（Learnable）位置编码"></a>可学习的（Learnable）位置编码</h3><p>直接为每个位置训练一个可学习的向量，类似于 nn.Embedding，让模型自动学习最佳的位置信息。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设最大序列长度为 100，每个位置用 512 维向量表示</span></span><br><span class="line">max_len = <span class="number">100</span></span><br><span class="line">d_model = <span class="number">512</span>  <span class="comment"># 词向量维度</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 可学习的位置编码</span></span><br><span class="line">positional_embedding = nn.Embedding(max_len, d_model)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成 10 个 token 的位置信息</span></span><br><span class="line">positions = torch.arange(<span class="number">10</span>).unsqueeze(<span class="number">0</span>)  <span class="comment"># shape: (1, 10)</span></span><br><span class="line">pe = positional_embedding(positions)  <span class="comment"># shape: (1, 10, 512)</span></span><br></pre></td></tr></table></figure>
<ul>
<li>优点：训练时可以自动调整位置编码的权重；适用于特定任务（如 NLP），如果训练数据的序列长度固定，可以使用这种方法。</li>
<li>缺点：不能推广到比训练时更长的序列。<h3 id="固定的（Sinusoidal）位置编码"><a href="#固定的（Sinusoidal）位置编码" class="headerlink" title="固定的（Sinusoidal）位置编码"></a>固定的（Sinusoidal）位置编码</h3>Transformer 论文提出了一种固定的位置编码方法，利用正弦（sin）和余弦（cos）函数，使不同位置的编码具有唯一性，并且可以推广到更长的序列,其代码实现如下：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">positional_encoding</span>(<span class="params">seq_len, d_model</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    生成 Transformer 位置编码</span></span><br><span class="line"><span class="string">    :param seq_len: 序列长度</span></span><br><span class="line"><span class="string">    :param d_model: 词向量维度</span></span><br><span class="line"><span class="string">    :return: 位置编码矩阵 (seq_len, d_model)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    pe = np.zeros((seq_len, d_model))</span><br><span class="line">    position = np.arange(seq_len)[:, np.newaxis]  <span class="comment"># (seq_len, 1)</span></span><br><span class="line">    div_term = np.exp(np.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) * (-np.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line"></span><br><span class="line">    pe[:, <span class="number">0</span>::<span class="number">2</span>] = np.sin(position * div_term)  <span class="comment"># 偶数维度使用 sin</span></span><br><span class="line">    pe[:, <span class="number">1</span>::<span class="number">2</span>] = np.cos(position * div_term)  <span class="comment"># 奇数维度使用 cos</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> torch.tensor(pe, dtype=torch.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成 10 个 token 的位置编码，词向量维度 512</span></span><br><span class="line">pe = positional_encoding(<span class="number">10</span>, <span class="number">512</span>)</span><br><span class="line"><span class="built_in">print</span>(pe.shape)  <span class="comment">#  torch.Size([10, 512])</span></span><br></pre></td></tr></table></figure></li>
<li>优点：无需训练，可直接计算；可以推广到更长的序列，不会因为训练长度限制而失效。</li>
<li>缺点：对特定任务可能没有可学习的位置编码效果好。</li>
</ul>
<h2 id="2-按照位置关系分类"><a href="#2-按照位置关系分类" class="headerlink" title="2. 按照位置关系分类"></a>2. 按照位置关系分类</h2><p>在位置编码的讨论中，”绝对” 和 “相对” 是根据编码方式的依赖关系来区分的。它们的核心区别在于 位置编码所表示的内容：是单独表示每个 token 在序列中的具体位置（绝对），还是表示 token 之间的相对位置关系（相对）。</p>
<h3 id="绝对位置编码"><a href="#绝对位置编码" class="headerlink" title="绝对位置编码"></a>绝对位置编码</h3><p>绝对位置编码为每个 token 分配一个唯一的、固定的标识符，这个标识符表示该 token 在序列中的位置，其特点归纳如下：</p>
<ul>
<li>位置编码是根据每个 token 在序列中的位置生成的，不考虑 token 之间的相对距离或顺序。</li>
<li>每个位置都有一个固定的编码值，即每个位置的编码与其绝对位置直接相关。</li>
</ul>
<p>上面介绍的根据正弦（sin）和余弦（cos）函数设计的固定位置编码就是一种绝对位置编码。</p>
<h3 id="相对位置编码"><a href="#相对位置编码" class="headerlink" title="相对位置编码"></a>相对位置编码</h3><p>相对位置编码表示token 之间的相对位置，即某个 token 与其他 token 之间的相对位置关系，而不是每个 token 的绝对位置，其特点归纳如下：</p>
<ul>
<li>位置编码关注的是 token 之间的相对关系，而不依赖于它们的绝对位置。这样就能将模型从单一的固定序列长度中解耦，提供更大的灵活性。</li>
<li>相对位置编码通常不为每个位置定义一个唯一的标识符，而是为每对 token 之间的相对距离生成一个编码（如相对位置的偏置或因子）。</li>
</ul>
<h1 id="六、GPT使用哪种位置编码方式-？"><a href="#六、GPT使用哪种位置编码方式-？" class="headerlink" title="六、GPT使用哪种位置编码方式 ？"></a>六、GPT使用哪种位置编码方式 ？</h1><p><strong>GPT 系列使用的是三角函数的绝对位置编码。</strong></p>
<p>GPT 系列模型是基于自回归的 Transformer 架构，目的是生成序列（如文本）。使用绝对位置编码有助于模型理解 token 在序列中的顺序，以便在生成文本时考虑上下文信息。</p>
<p>相较于相对位置编码，绝对位置编码较为简单，适用于很多任务，尤其是在模型输入序列长度相对固定或有限的情况下。由于 GPT 系列通常处理的输入长度相对较短，绝对位置编码是足够有效的。</p>
<p>此外，Llama 系列模型和一些其他的模型采用了 旋转位置编码（RoPE, Rotary Position Embedding）。这种位置编码方式不同于传统的绝对位置编码和相对位置编码，它通过旋转的方式来处理序列中的位置信息。具体的实现细节这里暂时不进行讲解。</p>
<p>ok，现在已经将句子映射到了embedding，那么embedding在后续将会被如何处理呢？它在神经网络中的数据流向又是怎样的呢？所有内容将在下篇文章中继续介绍，欢迎持续关注。</p>
<p>参考：</p>
<ul>
<li>[1] <a target="_blank" rel="noopener" href="https://0809zheng.github.io/2022/07/01/posencode.html">https://0809zheng.github.io/2022/07/01/posencode.html</a></li>
<li>[2] <a target="_blank" rel="noopener" href="https://www.qin.news/jue-dui-wei-zhi-bian-ma-he-xiang-dui-wei-zhi-bian-ma/">https://www.qin.news/jue-dui-wei-zhi-bian-ma-he-xiang-dui-wei-zhi-bian-ma/</a></li>
</ul>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/LLM/">LLM</a></div><div class="post_share"><div class="social-share" data-image="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2025/02/20/%E4%BB%8Etokenizer%E8%AF%B4%E8%B5%B7%EF%BC%8C%E4%B8%BALLM%E8%87%AA%E5%9B%9E%E5%BD%92%E9%A2%84%E8%AE%AD%E7%BB%83%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE%E9%9B%86-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%B8%80)/"><img class="next-cover" src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">从tokenizer说起，为LLM自回归预训练准备数据集-大模型炼丹术(一)</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2025/02/20/%E4%BB%8Etokenizer%E8%AF%B4%E8%B5%B7%EF%BC%8C%E4%B8%BALLM%E8%87%AA%E5%9B%9E%E5%BD%92%E9%A2%84%E8%AE%AD%E7%BB%83%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE%E9%9B%86-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%B8%80)/" title="从tokenizer说起，为LLM自回归预训练准备数据集-大模型炼丹术(一)"><img class="cover" src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-02-20</div><div class="title">从tokenizer说起，为LLM自回归预训练准备数据集-大模型炼丹术(一)</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://www.cdnjson.com/images/2021/11/27/_20210211193948.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">雨落诗山山亦奇</div><div class="author-info__description">本站为读研版&工作版博客，大学版移步 --> fuhanshi.github.io</div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">167</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">本站内容的最终版本将发布在微信公众号[南极Python]</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E4%BB%80%E4%B9%88%E6%98%AFToken-Embedding"><span class="toc-number">1.</span> <span class="toc-text">一、什么是Token Embedding ?</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83Embedding%E5%B1%82"><span class="toc-number">2.</span> <span class="toc-text">二、如何训练Embedding层 ?</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%89%E3%80%81nn-Embedding-%E5%92%8C-nn-Linear%E7%9A%84%E5%8C%BA%E5%88%AB%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-number">3.</span> <span class="toc-text">三、nn.Embedding 和 nn.Linear的区别是什么 ?</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%98%E9%9C%80%E8%A6%81Positinal-Embedding%EF%BC%9F"><span class="toc-number">4.</span> <span class="toc-text">四、为什么还需要Positinal Embedding？</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E5%B8%B8%E8%A7%81%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E6%96%B9%E5%BC%8F"><span class="toc-number">5.</span> <span class="toc-text">五、常见的位置编码方式</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E6%8C%89%E7%85%A7%E5%8F%82%E6%95%B0%E6%98%AF%E5%90%A6%E5%8F%AF%E4%BB%A5%E5%AD%A6%E4%B9%A0%E6%9D%A5%E5%88%86%E7%B1%BB"><span class="toc-number">5.1.</span> <span class="toc-text">1. 按照参数是否可以学习来分类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%AF%E5%AD%A6%E4%B9%A0%E7%9A%84%EF%BC%88Learnable%EF%BC%89%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="toc-number">5.1.1.</span> <span class="toc-text">可学习的（Learnable）位置编码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%BA%E5%AE%9A%E7%9A%84%EF%BC%88Sinusoidal%EF%BC%89%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="toc-number">5.1.2.</span> <span class="toc-text">固定的（Sinusoidal）位置编码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%8C%89%E7%85%A7%E4%BD%8D%E7%BD%AE%E5%85%B3%E7%B3%BB%E5%88%86%E7%B1%BB"><span class="toc-number">5.2.</span> <span class="toc-text">2. 按照位置关系分类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%9D%E5%AF%B9%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="toc-number">5.2.1.</span> <span class="toc-text">绝对位置编码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%B8%E5%AF%B9%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="toc-number">5.2.2.</span> <span class="toc-text">相对位置编码</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%AD%E3%80%81GPT%E4%BD%BF%E7%94%A8%E5%93%AA%E7%A7%8D%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E6%96%B9%E5%BC%8F-%EF%BC%9F"><span class="toc-number">6.</span> <span class="toc-text">六、GPT使用哪种位置编码方式 ？</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/02/24/%E4%BB%8E%E7%A6%BB%E6%95%A3%E7%9A%84tokenID%E5%88%B0%E5%85%B7%E6%9C%89%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%E7%9A%84embedding-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%BA%8C)/" title="从离散的token IDs到具有语义信息的embedding-大模型炼丹术(二)"><img src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="从离散的token IDs到具有语义信息的embedding-大模型炼丹术(二)"/></a><div class="content"><a class="title" href="/2025/02/24/%E4%BB%8E%E7%A6%BB%E6%95%A3%E7%9A%84tokenID%E5%88%B0%E5%85%B7%E6%9C%89%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%E7%9A%84embedding-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%BA%8C)/" title="从离散的token IDs到具有语义信息的embedding-大模型炼丹术(二)">从离散的token IDs到具有语义信息的embedding-大模型炼丹术(二)</a><time datetime="2025-02-24T14:28:29.000Z" title="发表于 2025-02-24 22:28:29">2025-02-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/02/20/%E4%BB%8Etokenizer%E8%AF%B4%E8%B5%B7%EF%BC%8C%E4%B8%BALLM%E8%87%AA%E5%9B%9E%E5%BD%92%E9%A2%84%E8%AE%AD%E7%BB%83%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE%E9%9B%86-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%B8%80)/" title="从tokenizer说起，为LLM自回归预训练准备数据集-大模型炼丹术(一)"><img src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="从tokenizer说起，为LLM自回归预训练准备数据集-大模型炼丹术(一)"/></a><div class="content"><a class="title" href="/2025/02/20/%E4%BB%8Etokenizer%E8%AF%B4%E8%B5%B7%EF%BC%8C%E4%B8%BALLM%E8%87%AA%E5%9B%9E%E5%BD%92%E9%A2%84%E8%AE%AD%E7%BB%83%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE%E9%9B%86-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%B8%80)/" title="从tokenizer说起，为LLM自回归预训练准备数据集-大模型炼丹术(一)">从tokenizer说起，为LLM自回归预训练准备数据集-大模型炼丹术(一)</a><time datetime="2025-02-20T14:28:29.000Z" title="发表于 2025-02-20 22:28:29">2025-02-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/30/%E6%8A%8A%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E6%90%AC%E5%88%B0GPU-%E8%8B%B1%E4%BC%9F%E8%BE%BEDALI%E5%8A%A0%E9%80%9F%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/" title="把数据预处理搬到GPU-英伟达DALI加速数据预处理"><img src="https://ice.frostsky.com/2024/12/30/88a897ed803ad9cb7462f4320a31ac67.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="把数据预处理搬到GPU-英伟达DALI加速数据预处理"/></a><div class="content"><a class="title" href="/2024/12/30/%E6%8A%8A%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E6%90%AC%E5%88%B0GPU-%E8%8B%B1%E4%BC%9F%E8%BE%BEDALI%E5%8A%A0%E9%80%9F%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/" title="把数据预处理搬到GPU-英伟达DALI加速数据预处理">把数据预处理搬到GPU-英伟达DALI加速数据预处理</a><time datetime="2024-12-30T13:32:21.000Z" title="发表于 2024-12-30 21:32:21">2024-12-30</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/07/PyTorch%E8%BD%ACTensorRT-engine%E4%BF%9D%E5%A7%86%E7%BA%A7%E6%95%99%E7%A8%8B/Markdown%20_%20%E8%AE%A9%E6%8E%92%E7%89%88%E5%8F%98%20Nice/" title="无题"><img src="/img/tag1.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="无题"/></a><div class="content"><a class="title" href="/2024/12/07/PyTorch%E8%BD%ACTensorRT-engine%E4%BF%9D%E5%A7%86%E7%BA%A7%E6%95%99%E7%A8%8B/Markdown%20_%20%E8%AE%A9%E6%8E%92%E7%89%88%E5%8F%98%20Nice/" title="无题">无题</a><time datetime="2024-12-07T07:43:12.125Z" title="发表于 2024-12-07 15:43:12">2024-12-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/07/%E4%B8%87%E5%AD%97%E9%95%BF%E6%96%87%E5%85%A5%E9%97%A8%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/" title="万字长文入门扩散模型"><img src="https://ice.frostsky.com/2024/12/07/bf4bfcbe17c50e1fcbc8d26acfac57c8.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="万字长文入门扩散模型"/></a><div class="content"><a class="title" href="/2024/12/07/%E4%B8%87%E5%AD%97%E9%95%BF%E6%96%87%E5%85%A5%E9%97%A8%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/" title="万字长文入门扩散模型">万字长文入门扩散模型</a><time datetime="2024-12-07T06:51:51.000Z" title="发表于 2024-12-07 14:51:51">2024-12-07</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By 雨落诗山山亦奇</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/fireworks.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>