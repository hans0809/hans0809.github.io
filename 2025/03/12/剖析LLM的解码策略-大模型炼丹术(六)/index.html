<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>剖析LLM的解码策略-大模型炼丹术(六) | 南极Python</title><meta name="keywords" content="LLM"><meta name="author" content="雨落诗山山亦奇"><meta name="copyright" content="雨落诗山山亦奇"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="在使用训练好的LLM进行自回归预测下一个token时，我们会选择预测序列中最后一个token对应的预测tensor，作为解码操作的对象。 1234567# 获取模型的预测结果with torch.no_grad():  # 关闭梯度计算，加速推理    logits &#x3D; model(idx_cond)  # (batch, n_tokens, vocab_size)# 只关注最后一个时间步的预测结">
<meta property="og:type" content="article">
<meta property="og:title" content="剖析LLM的解码策略-大模型炼丹术(六)">
<meta property="og:url" content="http://yoursite.com/2025/03/12/%E5%89%96%E6%9E%90LLM%E7%9A%84%E8%A7%A3%E7%A0%81%E7%AD%96%E7%95%A5-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E5%85%AD)/index.html">
<meta property="og:site_name" content="南极Python">
<meta property="og:description" content="在使用训练好的LLM进行自回归预测下一个token时，我们会选择预测序列中最后一个token对应的预测tensor，作为解码操作的对象。 1234567# 获取模型的预测结果with torch.no_grad():  # 关闭梯度计算，加速推理    logits &#x3D; model(idx_cond)  # (batch, n_tokens, vocab_size)# 只关注最后一个时间步的预测结">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg">
<meta property="article:published_time" content="2025-03-12T13:40:40.000Z">
<meta property="article:modified_time" content="2025-03-12T13:42:13.028Z">
<meta property="article:author" content="雨落诗山山亦奇">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg"><link rel="shortcut icon" href="https://www.cdnjson.com/images/2021/11/27/_20210211193948.png"><link rel="canonical" href="http://yoursite.com/2025/03/12/%E5%89%96%E6%9E%90LLM%E7%9A%84%E8%A7%A3%E7%A0%81%E7%AD%96%E7%95%A5-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E5%85%AD)/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '剖析LLM的解码策略-大模型炼丹术(六)',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-03-12 21:42:13'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/StaticFile_HEXO@latest/butterfly/css/macblack.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://www.cdnjson.com/images/2021/11/27/_20210211193948.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">171</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div></div><hr/></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">南极Python</a></span><div id="menus"><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">剖析LLM的解码策略-大模型炼丹术(六)</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-03-12T13:40:40.000Z" title="发表于 2025-03-12 21:40:40">2025-03-12</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-03-12T13:42:13.028Z" title="更新于 2025-03-12 21:42:13">2025-03-12</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF/">大模型炼丹术</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">3.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>12分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="剖析LLM的解码策略-大模型炼丹术(六)"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>在使用训练好的LLM进行自回归预测下一个token时，我们会选择预测序列中最后一个token对应的预测tensor，作为解码操作的对象。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取模型的预测结果</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():  <span class="comment"># 关闭梯度计算，加速推理</span></span><br><span class="line">    logits = model(idx_cond)  <span class="comment"># (batch, n_tokens, vocab_size)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 只关注最后一个时间步的预测结果</span></span><br><span class="line"><span class="comment"># (batch, n_tokens, vocab_size) 变为 (batch, vocab_size)</span></span><br><span class="line">logit = logits[:, -<span class="number">1</span>, :]  </span><br></pre></td></tr></table></figure>
<p>此时的logit就是用于解码的tensor，batch中的每一个都对应词汇表长度大小<code>vocab_size</code>的一个向量。</p>
<p>如何对该向量进行解码，得到要预测的下一个单词呢？本文介绍几种不同的解码策略。</p>
<h1 id="一、贪心解码"><a href="#一、贪心解码" class="headerlink" title="一、贪心解码"></a>一、贪心解码</h1><p>我们之前的解码策略是直接给logit应用softmax函数，然后使用<code>argmax</code>取概率值最大的数值对应的索引作为预测的下一个token ID，最后根据token ID在词汇表中查找得到预测的下一个单词：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">next_token = torch.argmax(logit, dim=-<span class="number">1</span>)  <span class="comment"># 选择最大值对应的索引</span></span><br></pre></td></tr></table></figure>

<p>这其实就是贪心解码策略，这种方式确定性强，计算高效，但容易陷入重复模式，生成文本单调，因为模型总是选择概率值最大的。</p>
<h1 id="二、温度缩放-采样"><a href="#二、温度缩放-采样" class="headerlink" title="二、温度缩放+采样"></a>二、温度缩放+采样</h1><p>这种解码策略分为两步：首先用温度系数来控制生成的随机性，然后进行概率采样。</p>
<ul>
<li>第一步、温度缩放：将logit除以一个温度系数，得到缩放后的logit，然后应用softmax将其归一化成概率分布</li>
<li>第二步、概率采样：使用<code>torch.multinomial</code>采样得到预测token。其内部原理如下：<br>  1）计算累积概率分布，比如对于<code>probs=[0.1, 0.3, 0.4, 0.2]</code>，累积概率分布为：CDF=<code>[0.1, 0.4, 0.8, 1.0]</code>，<br>  这意味着：  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">采样值落在 [0.0, 0.1] → 选 索引 0</span><br><span class="line">采样值落在 (0.1, 0.4] → 选 索引 1</span><br><span class="line">采样值落在 (0.4, 0.8] → 选 索引 2</span><br><span class="line">采样值落在 (0.8, 1.0] → 选 索引 3</span><br></pre></td></tr></table></figure>
  2）生成一个(0,1]之间的随机数（如果num_samples=1，生成1个随机数），比如rand=0.35<br>  3）rand = 0.35落在 CDF=[0.1, 0.4, 0.8, 1.0] 的 索引 1 处，所以最终返回的采样索引是1。</li>
</ul>
<p>代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">temperature = <span class="number">0.8</span>  <span class="comment"># 设置温度</span></span><br><span class="line">scaled_logits = logit / temperature  <span class="comment"># 进行温度缩放</span></span><br><span class="line">probs = F.softmax(scaled_logits, dim=-<span class="number">1</span>)  <span class="comment"># 归一化成概率分布</span></span><br><span class="line"></span><br><span class="line">next_token = torch.multinomial(probs, num_samples=<span class="number">1</span>)  <span class="comment"># 按概率采样，返回的是索引</span></span><br></pre></td></tr></table></figure>

<p>现在，让我们用一个例子来直观感受一下这种解码策略。</p>
<p>假设使用tokenizer得到的词汇表vocab长度为9，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">vocab = &#123; </span><br><span class="line">    <span class="string">&quot;closer&quot;</span>: <span class="number">0</span>,</span><br><span class="line">    <span class="string">&quot;every&quot;</span>: <span class="number">1</span>, </span><br><span class="line">    <span class="string">&quot;effort&quot;</span>: <span class="number">2</span>, </span><br><span class="line">    <span class="string">&quot;forward&quot;</span>: <span class="number">3</span>,</span><br><span class="line">    <span class="string">&quot;inches&quot;</span>: <span class="number">4</span>,</span><br><span class="line">    <span class="string">&quot;moves&quot;</span>: <span class="number">5</span>, </span><br><span class="line">    <span class="string">&quot;pizza&quot;</span>: <span class="number">6</span>,</span><br><span class="line">    <span class="string">&quot;toward&quot;</span>: <span class="number">7</span>,</span><br><span class="line">    <span class="string">&quot;you&quot;</span>: <span class="number">8</span>,</span><br><span class="line">&#125; </span><br><span class="line"></span><br><span class="line">inverse_vocab = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> vocab.items()&#125;</span><br></pre></td></tr></table></figure>

<p>假设模型已经预测的下一个token的logit如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">next_token_logits = torch.tensor(</span><br><span class="line">[<span class="number">4.51</span>, <span class="number">0.89</span>, -<span class="number">1.90</span>, <span class="number">6.75</span>, <span class="number">1.63</span>, -<span class="number">1.62</span>, -<span class="number">1.89</span>, <span class="number">6.28</span>, <span class="number">1.79</span>]</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>使用不同的温度缩放系数对logit进行缩放，并进行可视化展示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax_with_temperature</span>(<span class="params">logits, temperature</span>):</span></span><br><span class="line">    scaled_logits = logits / temperature</span><br><span class="line">    <span class="keyword">return</span> torch.softmax(scaled_logits, dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 温度值</span></span><br><span class="line">temperatures = [<span class="number">1</span>, <span class="number">0.1</span>, <span class="number">5</span>] </span><br><span class="line"></span><br><span class="line">scaled_probas = [softmax_with_temperature(next_token_logits, T) <span class="keyword">for</span> T <span class="keyword">in</span> temperatures]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Plotting</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">x = torch.arange(<span class="built_in">len</span>(vocab))</span><br><span class="line">bar_width = <span class="number">0.15</span></span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">5</span>, <span class="number">3</span>))</span><br><span class="line"><span class="keyword">for</span> i, T <span class="keyword">in</span> <span class="built_in">enumerate</span>(temperatures):</span><br><span class="line">    rects = ax.bar(x + i * bar_width, scaled_probas[i], bar_width, label=<span class="string">f&#x27;Temperature = <span class="subst">&#123;T&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;Probability&#x27;</span>)</span><br><span class="line">ax.set_xticks(x)</span><br><span class="line">ax.set_xticklabels(vocab.keys(), rotation=<span class="number">90</span>)</span><br><span class="line">ax.legend()</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="output.png"></p>
<p>可以看到，当温度值为1时，概率分布保持不变；当温度降低至0.1 时，forward以外的所有备选项的概率几乎降为0，此时进行概率采样时，几乎必然选择forward；而当温度升高到5时，多个备选项的概率趋于相近，使得采样结果更加随机。这正是温度缩放的作用：降低温度增强确定性，提高温度提升多样性。</p>
<h1 id="三、Tok-k采样"><a href="#三、Tok-k采样" class="headerlink" title="三、Tok-k采样"></a>三、Tok-k采样</h1><p>上述的温度缩放+采样策略，通过增大温度缩放系数可以提升生成结果的多样性。但是，这会导致某些不符合逻辑的token被采样，从而破坏生成结果。</p>
<p>为了解决这一问题，我们可以把要采样的备选项token按照概率值从大到小排序，只取前k个作为采样对象，这便是Tok-k采样的解码策略。</p>
<p>Top-k采样是一种控制生成文本质量的解码策略，它的核心思想是：<br>只在最高概率的k个token中进行采样，忽略其他低概率token，从而提高生成文本的连贯性和合理性。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">top_k_sampling</span>(<span class="params">logits, k=<span class="number">10</span></span>):</span></span><br><span class="line">    <span class="comment"># 1️ 计算 softmax 概率</span></span><br><span class="line">    probas = torch.softmax(logits, dim=-<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 2️ 取出 top-k 最高概率的 token</span></span><br><span class="line">    top_k_values, top_k_indices = torch.topk(probas, k)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3️ 归一化 top-k token 的概率</span></span><br><span class="line">    top_k_probs = top_k_values / top_k_values.<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4 在 top-k 范围内采样</span></span><br><span class="line">    next_token_id = torch.multinomial(top_k_probs, num_samples=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5️ 取出最终的 token ID</span></span><br><span class="line">    next_token = top_k_indices[next_token_id]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> next_token.item()</span><br></pre></td></tr></table></figure>

<h1 id="四、Top-k-温度缩放-采样"><a href="#四、Top-k-温度缩放-采样" class="headerlink" title="四、Top-k+温度缩放+采样"></a>四、Top-k+温度缩放+采样</h1><p>在执行完Top-k采样后，再执行温度系数缩放+采样的策略，直接上代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate</span>(<span class="params">model, idx, max_new_tokens, context_size, temperature=<span class="number">0.0</span>, top_k=<span class="literal">None</span>, eos_id=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(max_new_tokens):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1. 获取logits，shape为[batch_size,vocab_num]</span></span><br><span class="line">        idx_cond = idx[:, -context_size:]</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            logits = model(idx_cond)</span><br><span class="line">        logits = logits[:, -<span class="number">1</span>, :]<span class="comment"># [batch_size,vocab_num]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2. 执行top-k采样</span></span><br><span class="line">        <span class="keyword">if</span> top_k <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            top_logits, _ = torch.topk(logits, top_k)</span><br><span class="line">            min_val = top_logits[:, -<span class="number">1</span>]</span><br><span class="line">            <span class="comment"># 把其他未被采样的vocab_num-tok_k个元素置为-inf</span></span><br><span class="line">            logits = torch.where(logits &lt; min_val, torch.tensor(<span class="built_in">float</span>(<span class="string">&quot;-inf&quot;</span>)).to(logits.device), logits)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3. 执行温度缩放+采样</span></span><br><span class="line">        <span class="keyword">if</span> temperature &gt; <span class="number">0.0</span>:</span><br><span class="line">            logits = logits / temperature</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Apply softmax 获得概率值</span></span><br><span class="line">            probs = torch.softmax(logits, dim=-<span class="number">1</span>)  <span class="comment"># (batch_size, context_len)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 多项式采样</span></span><br><span class="line">            idx_next = torch.multinomial(probs, num_samples=<span class="number">1</span>)  <span class="comment"># (batch_size, 1)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果没有设置温度缩放系数，执行贪婪解码策略</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            idx_next = torch.argmax(logits, dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)  <span class="comment"># (batch_size, 1)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> idx_next == eos_id:  <span class="comment"># 遇到终止符，提前终止生成，即使还没有达到max_new_tokens次</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 准备下一个token的预测</span></span><br><span class="line">        idx = torch.cat((idx, idx_next), dim=<span class="number">1</span>)  <span class="comment"># (batch_size, num_tokens+1)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> idx</span><br></pre></td></tr></table></figure>

<h1 id="五、Top-p采样"><a href="#五、Top-p采样" class="headerlink" title="五、Top-p采样"></a>五、Top-p采样</h1><p>Top-p是一种改进的文本生成采样方法，与Top-k采样相比，它不限制候选token数量，而是动态选择概率总和达到p（累积概率阈值）的token进行采样。</p>
<p>Top-p采样的步骤如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1.先对所有logits进行softmax归一化，得到概率probs。</span><br><span class="line">2.按概率值从大到小排序，并计算累积概率cumsum(probs)。</span><br><span class="line">3.只保留累积概率总和≤p的token，其他设为0。</span><br><span class="line">4.重新归一化概率分布，使其总和为1。</span><br><span class="line">5.根据这个筛选后的概率分布进行随机采样。</span><br></pre></td></tr></table></figure>
<p>相应代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">top_p_sampling</span>(<span class="params">logits, top_p=<span class="number">0.9</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    实现 Top-p 采样 (Nucleus Sampling)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param logits: (batch_size, vocab_size)，模型输出的 logits</span></span><br><span class="line"><span class="string">    :param top_p: 保留的累计概率阈值</span></span><br><span class="line"><span class="string">    :return: 采样出的 token ID</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 1️ 计算 softmax 归一化的概率</span></span><br><span class="line">    probs = torch.softmax(logits, dim=-<span class="number">1</span>)  <span class="comment"># (batch_size, vocab_size)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2️ 按概率值降序排序，得到索引和排序后的概率</span></span><br><span class="line">    sorted_probs, sorted_indices = torch.sort(probs, descending=<span class="literal">True</span>)  <span class="comment"># (batch_size, vocab_size)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3️ 计算累积概率</span></span><br><span class="line">    cumulative_probs = torch.cumsum(sorted_probs, dim=-<span class="number">1</span>)  <span class="comment"># (batch_size, vocab_size)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4️ 找到使累积概率 &gt; p 的第一个位置</span></span><br><span class="line">    cutoff_mask = cumulative_probs &gt; top_p  <span class="comment"># (batch_size, vocab_size)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 确保至少保留一个 token</span></span><br><span class="line">    cutoff_mask[:, <span class="number">0</span>] = <span class="literal">False</span>  </span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5️ 过滤掉累计概率之外的 token（设为 0）</span></span><br><span class="line">    sorted_probs[cutoff_mask] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 6️ 重新归一化</span></span><br><span class="line">    sorted_probs = sorted_probs / sorted_probs.<span class="built_in">sum</span>(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 7️ 从剩下的 token 中按概率进行随机采样</span></span><br><span class="line">    sampled_index = torch.multinomial(sorted_probs, num_samples=<span class="number">1</span>)  <span class="comment"># (batch_size, 1)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 8️ 还原到原始词汇表索引</span></span><br><span class="line">    next_token = sorted_indices.gather(dim=-<span class="number">1</span>, index=sampled_index)  <span class="comment"># (batch_size, 1)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> next_token</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="六、Top-k-Top-p-温度缩放-采样"><a href="#六、Top-k-Top-p-温度缩放-采样" class="headerlink" title="六、Top-k + Top-p + 温度缩放 + 采样"></a>六、Top-k + Top-p + 温度缩放 + 采样</h1><p>在LLM进行自回归预测时，如何从logits（未归一化的预测值）中采样下一个token直接影响文本的流畅性、创造力和稳定性。</p>
<p>这里，我们结合上面所讲的温度缩放、Top-k采样和Top-p采样，实现一个完整的 高质量文本生成策略。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_next_token</span>(<span class="params">logits, temperature=<span class="number">1.0</span>, top_k=<span class="literal">None</span>, top_p=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    从 logits 中采样下一个 token，结合温度缩放、Top-k 和 Top-p 采样。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param logits: (batch_size, vocab_size)，模型的预测输出</span></span><br><span class="line"><span class="string">    :param temperature: 控制模型输出的随机性，越低越确定，越高越多样化</span></span><br><span class="line"><span class="string">    :param top_k: 仅保留前 k 个最高概率的 token</span></span><br><span class="line"><span class="string">    :param top_p: 仅保留累积概率达到 p 的 token</span></span><br><span class="line"><span class="string">    :return: 采样得到的 token ID</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 1️ 温度缩放：调整 logits</span></span><br><span class="line">    <span class="keyword">if</span> temperature &gt; <span class="number">0.0</span>:</span><br><span class="line">        logits = logits / temperature  <span class="comment"># 增加或降低 logits 差距</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 2️  应用 Top-k 采样（如果指定了 top_k）</span></span><br><span class="line">    <span class="keyword">if</span> top_k <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        top_logits, _ = torch.topk(logits, top_k)  <span class="comment"># 取 top-k 最大的 logits</span></span><br><span class="line">        min_logit = top_logits[:, -<span class="number">1</span>]  <span class="comment"># 取第 k 大的值</span></span><br><span class="line">        logits[logits &lt; min_logit] = <span class="built_in">float</span>(<span class="string">&quot;-inf&quot;</span>)  <span class="comment"># 低于第 k 大的值全部设为 -inf</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 3️  应用 Top-p 采样（如果指定了 top_p）</span></span><br><span class="line">    <span class="keyword">if</span> top_p <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># 计算 softmax 概率</span></span><br><span class="line">        probs = torch.softmax(logits, dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 按概率排序</span></span><br><span class="line">        sorted_probs, sorted_indices = torch.sort(probs, descending=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算累积概率</span></span><br><span class="line">        cumulative_probs = torch.cumsum(sorted_probs, dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 找到累积概率 &gt; top_p 的第一个位置</span></span><br><span class="line">        cutoff_mask = cumulative_probs &gt; top_p</span><br><span class="line">        cutoff_mask[:, <span class="number">0</span>] = <span class="literal">False</span>  <span class="comment"># 确保至少保留 1 个 token</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 过滤掉超过 top_p 的 token</span></span><br><span class="line">        sorted_probs[cutoff_mask] = <span class="number">0</span></span><br><span class="line">        sorted_probs = sorted_probs / sorted_probs.<span class="built_in">sum</span>(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)  <span class="comment"># 归一化</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 进行 multinomial 采样</span></span><br><span class="line">        sampled_index = torch.multinomial(sorted_probs, num_samples=<span class="number">1</span>)</span><br><span class="line">        next_token = sorted_indices.gather(dim=-<span class="number">1</span>, index=sampled_index)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 4️  如果不使用 Top-p，则直接用 softmax 进行 multinomial 采样</span></span><br><span class="line">        probs = torch.softmax(logits, dim=-<span class="number">1</span>)</span><br><span class="line">        next_token = torch.multinomial(probs, num_samples=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> next_token  <span class="comment"># (batch_size, 1)</span></span><br></pre></td></tr></table></figure>


<p>在本文的最后，我们对上面介绍的LLM解码策略总结如下表：<br>| 解码策略                     | 作用                                               | 适用场景                         | 局限性                             | 代码示例                                           |<br>|——————————|—————————————————-|———————————-|————————————|————————————————–|<br>| <strong>Greedy Search</strong>             | 直接选择概率最高的 token 作为下一个输出，生成结果确定。  | 需要高确定性的任务，如机器翻译、问答系统  | 容易陷入局部最优，生成的文本缺乏多样性，可能重复。 | <code>idx_next = torch.argmax(logits, dim=-1, keepdim=True)</code> |<br>| <strong>Top-k Sampling</strong>            | 只从 <strong>k</strong> 个最可能的候选中随机选择下一个 token。       | 用于生成更具创意和多样性的文本         | 可能产生语法不合理的句子，但能增加多样性。   | <code>top_logits, _ = torch.topk(logits, top_k)</code> <br> <code>logits = torch.where(logits &lt; min_val, torch.tensor(float(&quot;-inf&quot;)).to(logits.device), logits)</code> <br> <code>idx_next = torch.multinomial(probs, num_samples=1)</code> |<br>| <strong>Top-p Sampling</strong>            | 使用累积概率 <strong>p</strong> 进行采样，保证概率总和不超过 <strong>p</strong>。  | 生成更加多样且合理的文本                 | 计算复杂度较高，但能更好控制多样性和连贯性。 | <code>sorted_logits, sorted_indices = torch.sort(logits, descending=True)</code> <br> <code>cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)</code> <br> <code>top_p_mask = cumulative_probs &lt;= p</code> <br> <code>idx_next = torch.multinomial(probs, num_samples=1)</code> |<br>| <strong>Temperature Sampling</strong>      | 通过缩放 logits，控制生成的文本多样性。低温度时输出更确定，高温度时生成更多样本。 | 需要控制生成多样性或随机性的任务       | 温度过低可能导致输出单一，过高则可能生成无意义的文本。 | <code>logits = logits / temperature</code> <br> <code>probs = torch.softmax(logits, dim=-1)</code> <br> <code>idx_next = torch.multinomial(probs, num_samples=1)</code> |<br>| <strong>Combination (Top-k + Temperature)</strong> | 结合 <strong>Top-k</strong> 和 <strong>Temperature</strong>，先筛选候选中的前 <strong>k</strong> 个，再根据温度调节概率分布。 | 保持生成文本的多样性，同时避免生成过于随机的文本。 | 可能生成不太连贯的文本，需要适度调节参数。 | <code>top_logits, _ = torch.topk(logits, top_k)</code> <br> <code>logits = torch.where(logits &lt; min_val, torch.tensor(float(&quot;-inf&quot;)).to(logits.device), logits)</code> <br> <code>logits = logits / temperature</code> <br> <code>probs = torch.softmax(logits, dim=-1)</code> <br> <code>idx_next = torch.multinomial(probs, num_samples=1)</code> |<br>| <strong>Combination (Top-p + Temperature)</strong> | 结合 <strong>Top-p</strong> 和 <strong>Temperature</strong>，先筛选累计概率小于 <strong>p</strong> 的 token，再进行温度缩放。 | 更好地控制文本生成的多样性和合理性，避免极端的随机性。 | 可能导致生成的文本过于保守，丧失创意。 | <code>sorted_logits, sorted_indices = torch.sort(logits, descending=True)</code> <br> <code>cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)</code> <br> <code>top_p_mask = cumulative_probs &lt;= p</code> <br> <code>logits = logits / temperature</code> <br> <code>probs = torch.softmax(logits, dim=-1)</code> <br> <code>idx_next = torch.multinomial(probs, num_samples=1)</code> |<br>| <strong>Combination (Top-k + Top-p + Temperature)</strong> | 结合 <strong>Top-k</strong>、<strong>Top-p</strong> 和 <strong>Temperature</strong>，先筛选前 <strong>k</strong> 个候选、然后选择累计概率不超过 <strong>p</strong> 的候选，再使用温度调整概率分布。 | 能够在增加多样性的同时保证生成的文本合乎逻辑且具有较高的质量。 | 需要仔细调整 <strong>k</strong>、<strong>p</strong> 和温度，以平衡生成的多样性和合理性。 | <code>top_logits, _ = torch.topk(logits, top_k)</code> <br> <code>logits = torch.where(logits &lt; min_val, torch.tensor(float(&quot;-inf&quot;)).to(logits.device), logits)</code> <br> <code>sorted_logits, sorted_indices = torch.sort(logits, descending=True)</code> <br> <code>cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)</code> <br> <code>top_p_mask = cumulative_probs &lt;= p</code> <br> <code>logits = logits / temperature</code> <br> <code>probs = torch.softmax(logits, dim=-1)</code> <br> <code>idx_next = torch.multinomial(probs, num_samples=1)</code> |</p>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/LLM/">LLM</a></div><div class="post_share"><div class="social-share" data-image="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2025/03/11/LLM%E8%87%AA%E5%9B%9E%E5%BD%92%E9%A2%84%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E8%AF%A6%E8%A7%A3-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%BA%94)/"><img class="next-cover" src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">LLM自回归预训练过程详解-大模型炼丹术(五)</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2025/03/11/LLM%E8%87%AA%E5%9B%9E%E5%BD%92%E9%A2%84%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E8%AF%A6%E8%A7%A3-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%BA%94)/" title="LLM自回归预训练过程详解-大模型炼丹术(五)"><img class="cover" src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-11</div><div class="title">LLM自回归预训练过程详解-大模型炼丹术(五)</div></div></a></div><div><a href="/2025/02/20/%E4%BB%8Etokenizer%E8%AF%B4%E8%B5%B7%EF%BC%8C%E4%B8%BALLM%E8%87%AA%E5%9B%9E%E5%BD%92%E9%A2%84%E8%AE%AD%E7%BB%83%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE%E9%9B%86-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%B8%80)/" title="从tokenizer说起，为LLM自回归预训练准备数据集-大模型炼丹术(一)"><img class="cover" src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-02-20</div><div class="title">从tokenizer说起，为LLM自回归预训练准备数据集-大模型炼丹术(一)</div></div></a></div><div><a href="/2025/03/04/%E4%BB%8E%E5%8D%95%E5%A4%B4%E5%88%B0%E5%A4%9A%E5%A4%B4%EF%BC%8C%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%B8%89)/" title="从单头到多头，深度解析大模型的注意力机制-大模型炼丹术(三)"><img class="cover" src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-04</div><div class="title">从单头到多头，深度解析大模型的注意力机制-大模型炼丹术(三)</div></div></a></div><div><a href="/2025/02/24/%E4%BB%8E%E7%A6%BB%E6%95%A3%E7%9A%84tokenID%E5%88%B0%E5%85%B7%E6%9C%89%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%E7%9A%84embedding-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%BA%8C)/" title="从离散的token IDs到具有语义信息的embedding-大模型炼丹术(二)"><img class="cover" src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-02-24</div><div class="title">从离散的token IDs到具有语义信息的embedding-大模型炼丹术(二)</div></div></a></div><div><a href="/2025/03/07/%E5%8A%A8%E6%89%8B%E6%90%AD%E5%BB%BAGPT2%E6%9E%B6%E6%9E%84-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E5%9B%9B)/" title="动手搭建GPT2架构-大模型炼丹术(四)"><img class="cover" src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-07</div><div class="title">动手搭建GPT2架构-大模型炼丹术(四)</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://www.cdnjson.com/images/2021/11/27/_20210211193948.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">雨落诗山山亦奇</div><div class="author-info__description">本站为读研版&工作版博客，大学版移步 --> fuhanshi.github.io</div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">171</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">本站内容的最终版本将发布在微信公众号[南极Python]</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E8%B4%AA%E5%BF%83%E8%A7%A3%E7%A0%81"><span class="toc-number">1.</span> <span class="toc-text">一、贪心解码</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E6%B8%A9%E5%BA%A6%E7%BC%A9%E6%94%BE-%E9%87%87%E6%A0%B7"><span class="toc-number">2.</span> <span class="toc-text">二、温度缩放+采样</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%89%E3%80%81Tok-k%E9%87%87%E6%A0%B7"><span class="toc-number">3.</span> <span class="toc-text">三、Tok-k采样</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%9B%E3%80%81Top-k-%E6%B8%A9%E5%BA%A6%E7%BC%A9%E6%94%BE-%E9%87%87%E6%A0%B7"><span class="toc-number">4.</span> <span class="toc-text">四、Top-k+温度缩放+采样</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%94%E3%80%81Top-p%E9%87%87%E6%A0%B7"><span class="toc-number">5.</span> <span class="toc-text">五、Top-p采样</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%AD%E3%80%81Top-k-Top-p-%E6%B8%A9%E5%BA%A6%E7%BC%A9%E6%94%BE-%E9%87%87%E6%A0%B7"><span class="toc-number">6.</span> <span class="toc-text">六、Top-k + Top-p + 温度缩放 + 采样</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/03/12/%E5%89%96%E6%9E%90LLM%E7%9A%84%E8%A7%A3%E7%A0%81%E7%AD%96%E7%95%A5-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E5%85%AD)/" title="剖析LLM的解码策略-大模型炼丹术(六)"><img src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="剖析LLM的解码策略-大模型炼丹术(六)"/></a><div class="content"><a class="title" href="/2025/03/12/%E5%89%96%E6%9E%90LLM%E7%9A%84%E8%A7%A3%E7%A0%81%E7%AD%96%E7%95%A5-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E5%85%AD)/" title="剖析LLM的解码策略-大模型炼丹术(六)">剖析LLM的解码策略-大模型炼丹术(六)</a><time datetime="2025-03-12T13:40:40.000Z" title="发表于 2025-03-12 21:40:40">2025-03-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/03/11/LLM%E8%87%AA%E5%9B%9E%E5%BD%92%E9%A2%84%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E8%AF%A6%E8%A7%A3-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%BA%94)/" title="LLM自回归预训练过程详解-大模型炼丹术(五)"><img src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="LLM自回归预训练过程详解-大模型炼丹术(五)"/></a><div class="content"><a class="title" href="/2025/03/11/LLM%E8%87%AA%E5%9B%9E%E5%BD%92%E9%A2%84%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E8%AF%A6%E8%A7%A3-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%BA%94)/" title="LLM自回归预训练过程详解-大模型炼丹术(五)">LLM自回归预训练过程详解-大模型炼丹术(五)</a><time datetime="2025-03-11T13:31:10.000Z" title="发表于 2025-03-11 21:31:10">2025-03-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/03/07/%E5%8A%A8%E6%89%8B%E6%90%AD%E5%BB%BAGPT2%E6%9E%B6%E6%9E%84-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E5%9B%9B)/" title="动手搭建GPT2架构-大模型炼丹术(四)"><img src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="动手搭建GPT2架构-大模型炼丹术(四)"/></a><div class="content"><a class="title" href="/2025/03/07/%E5%8A%A8%E6%89%8B%E6%90%AD%E5%BB%BAGPT2%E6%9E%B6%E6%9E%84-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E5%9B%9B)/" title="动手搭建GPT2架构-大模型炼丹术(四)">动手搭建GPT2架构-大模型炼丹术(四)</a><time datetime="2025-03-07T15:35:12.000Z" title="发表于 2025-03-07 23:35:12">2025-03-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/03/04/%E4%BB%8E%E5%8D%95%E5%A4%B4%E5%88%B0%E5%A4%9A%E5%A4%B4%EF%BC%8C%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%B8%89)/" title="从单头到多头，深度解析大模型的注意力机制-大模型炼丹术(三)"><img src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="从单头到多头，深度解析大模型的注意力机制-大模型炼丹术(三)"/></a><div class="content"><a class="title" href="/2025/03/04/%E4%BB%8E%E5%8D%95%E5%A4%B4%E5%88%B0%E5%A4%9A%E5%A4%B4%EF%BC%8C%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%B8%89)/" title="从单头到多头，深度解析大模型的注意力机制-大模型炼丹术(三)">从单头到多头，深度解析大模型的注意力机制-大模型炼丹术(三)</a><time datetime="2025-03-04T14:03:57.000Z" title="发表于 2025-03-04 22:03:57">2025-03-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/02/24/%E4%BB%8E%E7%A6%BB%E6%95%A3%E7%9A%84tokenID%E5%88%B0%E5%85%B7%E6%9C%89%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%E7%9A%84embedding-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%BA%8C)/" title="从离散的token IDs到具有语义信息的embedding-大模型炼丹术(二)"><img src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="从离散的token IDs到具有语义信息的embedding-大模型炼丹术(二)"/></a><div class="content"><a class="title" href="/2025/02/24/%E4%BB%8E%E7%A6%BB%E6%95%A3%E7%9A%84tokenID%E5%88%B0%E5%85%B7%E6%9C%89%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%E7%9A%84embedding-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%BA%8C)/" title="从离散的token IDs到具有语义信息的embedding-大模型炼丹术(二)">从离散的token IDs到具有语义信息的embedding-大模型炼丹术(二)</a><time datetime="2025-02-24T14:28:29.000Z" title="发表于 2025-02-24 22:28:29">2025-02-24</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By 雨落诗山山亦奇</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/fireworks.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>