<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#0078E7"><meta name="author" content="雨落诗山山亦奇"><meta name="copyright" content="雨落诗山山亦奇"><meta name="generator" content="Hexo 4.2.1"><meta name="theme" content="hexo-theme-yun"><title>那些年，我们一起实现的交叉熵 | 南极Python</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/star-markdown-css@0.1.25/dist/yun/yun-markdown.min.css"><script src="//at.alicdn.com/t/font_1140697_dxory92pb0h.js" async></script><script src="https://cdn.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>function initScrollReveal() {
  [".post-card",".post-content img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
}
document.addEventListener("DOMContentLoaded", initScrollReveal);
document.addEventListener("pjax:success", initScrollReveal);
</script><link rel="icon" href="/yun.svg"><link rel="mask-icon" href="/yun.svg" color="#0078E7"><link rel="alternate icon" href="/yun.ico"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="preload" href="/js/utils.js" as="script"><link rel="preload" href="/js/hexo-theme-yun.js" as="script"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><script id="yun-config">
    const Yun = window.Yun || {};
    window.CONFIG = {"hostname":"yoursite.com","root":"/","title":"云游君的小站","version":"1.6.3","mode":"auto","copycode":true,"page":{"isPost":true},"i18n":{"placeholder":"搜索...","empty":"找不到您查询的内容: ${query}","hits":"找到 ${hits} 条结果","hits_time":"找到 ${hits} 条结果（用时 ${time} 毫秒）"},"anonymous_image":"https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/avatar/none.jpg","say":{"api":"https://v1.hitokoto.cn","hitokoto":true},"fireworks":{"colors":["102, 167, 221","62, 131, 225","33, 78, 194"]}};
  </script><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script src="/js/utils.js"></script><script src="/js/hexo-theme-yun.js"></script><meta name="description" content="最近在做交叉熵的魔改，所以需要好好了解下交叉熵，遂有此文。 关于交叉熵的定义请自行百度，相信点进来的你对其基本概念不陌生。 本文将结合PyTorch，介绍离散形式的交叉熵在二分类以及多分类中的应用。注意，本文出现的二分类交叉熵和多分类交叉熵，本质上都是一个东西，二分类交叉熵可以看作是多分类交叉熵的一个特例，只不过在PyTorch中对应方法的实现方式不同（不同之处将在正文详细讲解）。 好了，废话少叙">
<meta property="og:type" content="article">
<meta property="og:title" content="那些年，我们一起实现的交叉熵">
<meta property="og:url" content="http://yoursite.com/2021/04/28/%E9%82%A3%E4%BA%9B%E5%B9%B4%EF%BC%8C%E6%88%91%E4%BB%AC%E4%B8%80%E8%B5%B7%E5%AE%9E%E7%8E%B0%E7%9A%84%E4%BA%A4%E5%8F%89%E7%86%B5/index.html">
<meta property="og:site_name" content="南极Python">
<meta property="og:description" content="最近在做交叉熵的魔改，所以需要好好了解下交叉熵，遂有此文。 关于交叉熵的定义请自行百度，相信点进来的你对其基本概念不陌生。 本文将结合PyTorch，介绍离散形式的交叉熵在二分类以及多分类中的应用。注意，本文出现的二分类交叉熵和多分类交叉熵，本质上都是一个东西，二分类交叉熵可以看作是多分类交叉熵的一个特例，只不过在PyTorch中对应方法的实现方式不同（不同之处将在正文详细讲解）。 好了，废话少叙">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://yoursite.com/2021/04/28/%E9%82%A3%E4%BA%9B%E5%B9%B4%EF%BC%8C%E6%88%91%E4%BB%AC%E4%B8%80%E8%B5%B7%E5%AE%9E%E7%8E%B0%E7%9A%84%E4%BA%A4%E5%8F%89%E7%86%B5/1.png">
<meta property="og:image" content="http://yoursite.com/2021/04/28/%E9%82%A3%E4%BA%9B%E5%B9%B4%EF%BC%8C%E6%88%91%E4%BB%AC%E4%B8%80%E8%B5%B7%E5%AE%9E%E7%8E%B0%E7%9A%84%E4%BA%A4%E5%8F%89%E7%86%B5/2.png">
<meta property="og:image" content="http://yoursite.com/2021/04/28/%E9%82%A3%E4%BA%9B%E5%B9%B4%EF%BC%8C%E6%88%91%E4%BB%AC%E4%B8%80%E8%B5%B7%E5%AE%9E%E7%8E%B0%E7%9A%84%E4%BA%A4%E5%8F%89%E7%86%B5/3.png">
<meta property="og:image" content="http://yoursite.com/2021/04/28/%E9%82%A3%E4%BA%9B%E5%B9%B4%EF%BC%8C%E6%88%91%E4%BB%AC%E4%B8%80%E8%B5%B7%E5%AE%9E%E7%8E%B0%E7%9A%84%E4%BA%A4%E5%8F%89%E7%86%B5/4.png">
<meta property="og:image" content="http://yoursite.com/2021/04/28/%E9%82%A3%E4%BA%9B%E5%B9%B4%EF%BC%8C%E6%88%91%E4%BB%AC%E4%B8%80%E8%B5%B7%E5%AE%9E%E7%8E%B0%E7%9A%84%E4%BA%A4%E5%8F%89%E7%86%B5/5.png">
<meta property="og:image" content="http://yoursite.com/2021/04/28/%E9%82%A3%E4%BA%9B%E5%B9%B4%EF%BC%8C%E6%88%91%E4%BB%AC%E4%B8%80%E8%B5%B7%E5%AE%9E%E7%8E%B0%E7%9A%84%E4%BA%A4%E5%8F%89%E7%86%B5/7.png">
<meta property="og:image" content="http://yoursite.com/2021/04/28/%E9%82%A3%E4%BA%9B%E5%B9%B4%EF%BC%8C%E6%88%91%E4%BB%AC%E4%B8%80%E8%B5%B7%E5%AE%9E%E7%8E%B0%E7%9A%84%E4%BA%A4%E5%8F%89%E7%86%B5/8.png">
<meta property="article:published_time" content="2021-04-28T10:59:03.000Z">
<meta property="article:modified_time" content="2021-04-29T05:16:14.711Z">
<meta property="article:author" content="雨落诗山山亦奇">
<meta property="article:tag" content="DL">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/2021/04/28/%E9%82%A3%E4%BA%9B%E5%B9%B4%EF%BC%8C%E6%88%91%E4%BB%AC%E4%B8%80%E8%B5%B7%E5%AE%9E%E7%8E%B0%E7%9A%84%E4%BA%A4%E5%8F%89%E7%86%B5/1.png"><script src="/js/ui/mode.js"></script></head><body><script defer src="https://cdn.jsdelivr.net/npm/animejs@latest"></script><script defer src="/js/ui/fireworks.js"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="/js/sidebar.js"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="文章目录"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-list-ordered"></use></svg></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="站点概览"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-passport-line"></use></svg></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="雨落诗山山亦奇"><img width="96" loading="lazy" src="/yun.png" alt="雨落诗山山亦奇"></a><div class="site-author-name"><a href="/about/">雨落诗山山亦奇</a></div><span class="site-name">南极Python</span><sub class="site-subtitle">Python|机器学习|深度学习</sub><div class="site-desciption">昨夜星辰昨夜风</div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="首页"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-home-4-line"></use></svg></span></a><div class="site-state-item"><a href="/archives/" title="归档"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-archive-line"></use></svg></span><span class="site-state-item-count">134</span></a></div><div class="site-state-item"><a href="/categories/" title="分类"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-2-line"></use></svg></span><span class="site-state-item-count">9</span></a></div><div class="site-state-item"><a href="/tags/" title="标签"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="site-state-item-count">13</span></a></div><a class="site-state-item hty-icon-button" href="https://yun.yunyoujun.cn" target="_blank" rel="noopener" title="文档"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-settings-line"></use></svg></span></a></nav><hr style="margin-bottom:0.5rem"><div class="links-of-author"><a class="links-of-author-item hty-icon-button" rel="noopener" href="/atom.xml" title="RSS" target="_blank" style="color:orange"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-rss-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://qm.qq.com/cgi-bin/qm/qr?k=kZJzggTTCf4SpvEQ8lXWoi5ZjhAx0ILZ&amp;jump_from=webapi" title="QQ 群 1050458482" target="_blank" style="color:#12B7F5"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-qq-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://github.com/YunYouJun" title="GitHub" target="_blank" style="color:#6e5494"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-github-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://weibo.com/jizhideyunyoujun" title="微博" target="_blank" style="color:#E6162D"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-weibo-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://www.douban.com/people/yunyoujun/" title="豆瓣" target="_blank" style="color:#007722"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-douban-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://music.163.com/#/user/home?id=247102977" title="网易云音乐" target="_blank" style="color:#C20C0C"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-netease-cloud-music-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://www.zhihu.com/people/yunyoujun/" title="知乎" target="_blank" style="color:#0084FF"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-zhihu-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://space.bilibili.com/1579790" title="哔哩哔哩" target="_blank" style="color:#FF8EB3"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-bilibili-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/about/white-qrcode-and-search.jpg" title="微信公众号" target="_blank" style="color:#1AAD19"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-wechat-2-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://twitter.com/YunYouJun" title="Twitter" target="_blank" style="color:#1da1f2"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-twitter-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://t.me/elpsycn" title="Telegram Channel" target="_blank" style="color:#0088CC"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-telegram-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="mailto:me@yunyoujun.cn" title="E-Mail" target="_blank" style="color:#8E71C1"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-mail-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://travellings.link" title="Travelling" target="_blank" style="color:var(--hty-text-color)"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-train-line"></use></svg></a></div><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="/links/" title="我的小伙伴们" style="color:dodgerblue"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-genderless-line"></use></svg></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-contrast-2-line"></use></svg></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#二分类交叉熵"><span class="toc-number">1.</span> <span class="toc-text">二分类交叉熵</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#多分类交叉熵"><span class="toc-number">2.</span> <span class="toc-text">多分类交叉熵</span></a></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="hty-card post-block" itemscope itemtype="https://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/04/28/%E9%82%A3%E4%BA%9B%E5%B9%B4%EF%BC%8C%E6%88%91%E4%BB%AC%E4%B8%80%E8%B5%B7%E5%AE%9E%E7%8E%B0%E7%9A%84%E4%BA%A4%E5%8F%89%E7%86%B5/"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="雨落诗山山亦奇"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="南极Python"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">那些年，我们一起实现的交叉熵</h1><div class="post-meta"><div class="post-time" style="display:inline-block"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-calendar-line"></use></svg></span> <time title="创建时间：2021-04-28 18:59:03" itemprop="dateCreated datePublished" datetime="2021-04-28T18:59:03+08:00">2021-04-28</time><span class="post-meta-divider">-</span><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-calendar-2-line"></use></svg></span> <time title="修改时间：2021-04-29 13:16:14" itemprop="dateModified" datetime="2021-04-29T13:16:14+08:00">2021-04-29</time></div><div class="post-classify"><span class="post-category"> <span class="post-meta-item-icon" style="margin-right:3px;"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-line"></use></svg></span><span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a class="category-item" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" style="--text-color:var(--hty-text-color)" itemprop="url" rel="index"><span itemprop="text">深度学习笔记</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag-item" href="/tags/DL/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">DL</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body" style="--smc-primary:#0078E7;"><p>最近在做交叉熵的魔改，所以需要好好了解下交叉熵，遂有此文。</p>
<p>关于交叉熵的定义请自行百度，相信点进来的你对其基本概念不陌生。</p>
<p>本文将结合PyTorch，介绍离散形式的交叉熵在二分类以及多分类中的应用。注意，本文出现的二分类交叉熵和多分类交叉熵，本质上都是一个东西，二分类交叉熵可以看作是多分类交叉熵的一个特例，只不过在PyTorch中对应方法的实现方式不同（不同之处将在正文详细讲解）。</p>
<p>好了，废话少叙，正文开始~</p>
<h2 id="二分类交叉熵"><a href="#二分类交叉熵" class="headerlink" title="二分类交叉熵"></a>二分类交叉熵</h2><p>$$L=-\frac1N \sum_{i=1}^{N}[y_ilog(p_i)+(1-y_i)log(1-p_i)]$$<br>其中，$N$是总样本数，$y_i$是第$i$个样本的所属类别，$p_i$是第$i$个样本的预测值，一般来说，它是一个概率值。</p>
<p>上栗子：</p>
<table>
<thead>
<tr>
<th align="center">.</th>
<th align="center">$y_i$</th>
<th align="center">$p_i$</th>
</tr>
</thead>
<tbody><tr>
<td align="center">第1个样本</td>
<td align="center">1</td>
<td align="center">0.8</td>
</tr>
<tr>
<td align="center">第1个样本</td>
<td align="center">0</td>
<td align="center">0.2</td>
</tr>
<tr>
<td align="center">第1个样本</td>
<td align="center">0</td>
<td align="center">0.4</td>
</tr>
</tbody></table>
<p>按照上面的公式，交叉熵计算如下：<br>$$L=\frac13[(1<em>log0.8+(1-1)</em>log(1-0.8))+(0<em>log0.2+(1-0)</em>log(1-0.2))+(0<em>log0.4+(1-0)</em>log(1-0.4))]=0.319$$<br><img src="./1.png" alt="Alt text" loading="lazy"></p>
<p>其实，在PyTorch中已经内置了<code>BCELoss</code>，它的主要用途是计算二分类问题的交叉熵，我们可以调用该方法，并将结果与上面手动计算的结果做个比较：<br><img src="./2.png" alt="Alt text" loading="lazy"></p>
<p>嗯，结果是一致的。</p>
<p>需要注意的是，输入<code>BCELoss</code>中的预测值应该是个概率$p_i$。</p>
<p>上面的栗子直接给出了预测的$p_i$，这是符合要求的。但在更一般的二分类问题中，网络的输出取值是整个实数域(可正可负可为0)。</p>
<p>为了由这种输出值得到对应的$p_i$，你可以在网络的输出层之后新加一个<code>Sigmoid</code>层，这样便可以将输出值的取值规范到0和1之间，这就是交叉熵公式中的$p_i$。</p>
<p>当然，你也可以不更改网络输出，而是在将输出值送入交叉熵公式进行性计算之前，手动用<code>Simgmoid</code>函数做一个映射。</p>
<p>在PyTorch中，甚至提供了<code>BCEWithLogitsLoss</code>方法，它可以直接将输入的值规范到0和1 之间，相当于将<code>Sigmoid</code>和<code>BCELoss</code>集成在了一个方法中。</p>
<p>还是举个栗子来具体进行说明：假设pred是shape为[4,2]的tensor，其中4代表样本个数，2代表该样本分别属于两个类别的概率（前提是规范到了0和1之间，否则就是两个实数域上的值，记住，现在我们讨论的是二分类）；target是shape为[4]的tensor，4即样本数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pred=torch.randn(<span class="number">4</span>,<span class="number">2</span>)<span class="comment">#预测值</span></span><br><span class="line">target=torch.rand(<span class="number">4</span>).random_(<span class="number">0</span>,<span class="number">2</span>)<span class="comment">#真实类别标签</span></span><br></pre></td></tr></table></figure>

<p>在使用任何一种方法之前，都需要先对target做独热编码，否则target和pred维度不匹配：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将target进行独热编码</span></span><br><span class="line">onehot_target=torch.eye(<span class="number">2</span>)[target.long(), :]</span><br></pre></td></tr></table></figure>
<p>在做编码前，target看起来长这样：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([0., 1., 1., 1.])</span><br></pre></td></tr></table></figure>
<p>编码后，target变成了这样：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 0.],</span><br><span class="line">        [0., 1.],</span><br><span class="line">        [0., 1.],</span><br><span class="line">        [0., 1.]])</span><br></pre></td></tr></table></figure>
<p>现在，target的shape也是[4,2]了，和pred的shape一样，所以下面可以开始计算交叉熵了。</p>
<ul>
<li><p>使用<code>Sigmoid</code>和<code>BCELoss</code>计算交叉熵</p>
<p>  先使用<code>nn.Sigmoid</code>做一下映射：<br>  <img src="./3.png" alt="Alt text" loading="lazy"><br>  可以看到，映射后的取值已经被规范到了0和1之间。</p>
<p>  然后使用<code>BCELoss</code>进行计算：<br>  <img src="./4.png" alt="Alt text" loading="lazy"></p>
</li>
</ul>
<ul>
<li>只使用<code>BCELossWithLogits</code>计算交叉熵<br>  <img src="./5.png" alt="Alt text" loading="lazy"></li>
</ul>
<p>两种方法的计算结果完全一致。不过官方建议使用<code>BCELossWithLogits</code>，理由是能够提升数值计算稳定性。</p>
<p>以后，当你使用PyTorch内置的二分类交叉熵损失函数时，只要保证输入的预测值和真实标签的维度一致（N,…），且输入的预测值是一个概率即可。满足这两点，一般就能避免常见的错误了。<br><img src="./7.png" alt="Alt text" loading="lazy"><br>(BCELoss的使用)</p>
<p>关于二分类交叉熵的介绍就到这里，接下来介绍多分类交叉熵。</p>
<h2 id="多分类交叉熵"><a href="#多分类交叉熵" class="headerlink" title="多分类交叉熵"></a>多分类交叉熵</h2><p>$$L=-{\frac1N} {\sum_{i=1}^{N}}  {\sum_{c=1}^{K}} y_{ic}log(p_{ic})$$<br>其中，N代表样本数，K代表类别数，$p_{ic}$代表第i个样本属于类别c的概率，$\sum_{c=1}^{K}p_{ic}=1,i=1,2,…,N$，$y_{ic}\in {0,1}$，可以看作一个one-hot编码（若第i个样本属于类别c，则对应位置的$y_ic$取1，否则取0）。</p>
<p>这个公式乍看上去有点复杂，其实不难。不妨取第$i$个样本，计算这个样本的交叉熵，公式如下：<br>$$L_i=\sum_{c=1}^{K}y_{ic}log(p_{ic})$$</p>
<p>假设N=2, K=3，即总共3个样本，3个类别，样本的数据如下</p>
<p>|.      |    $y_{i1}$|   $y_{i2}$   |$y_{i3}$|$p_{i1}$|$p_{i2}$|$p_{i3}$|<br>| :——–: | :——–:| :——: |:——:|<br>| 第1个样本   |   0|  1  |0|0.2|0.3|0.5<br>| 第2个样本   |   1|  0 |0|0.3|0.2|0.5<br>| 第3个样本   |   0|  0  |1|0.4|0.4|0.2</p>
<p>$$L_1=0<em>log(0.2)+1</em>log(0.3)+0<em>log(0.5)=-1.2039$$<br>$$L_2=1</em>log(0.3)+0<em>log(0.2)+0</em>log(0.5)=-1.2039$$<br>$$L_3=0<em>log(0.4)+0</em>log(0.4)+0*log(0.2)=-1.6094$$</p>
<p>看吧，最终的交叉熵只不过是做了N这样的计算，然后平均一下，加个负号：<br>$$L=-\frac13(L_1+L_2+L_3)=1.3391$$</p>
<p>你可能已经发现，这里的$p_{ic},c=1,2,3$之和为1。没错，这是网络的输出做了softmax后得到的结果。在上一部分<strong>关于二分类的问题中，输入交叉熵公式的网络预测值必须经过<code>Sigmoid</code>进行映射，而在这里的多分类问题中，需要将<code>Sigmoid</code>替换成<code>Softmax</code>，这是两者的一个重要区别！</strong></p>
<p>现在让我们用代码来实现上面的计算过程：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#预测值，假设已做softmax</span></span><br><span class="line">pred=torch.tensor([[<span class="number">0.2</span>,<span class="number">0.3</span>,<span class="number">0.5</span>],[<span class="number">0.3</span>,<span class="number">0.2</span>,<span class="number">0.5</span>],[<span class="number">0.4</span>,<span class="number">0.4</span>,<span class="number">0.2</span>]])</span><br><span class="line"><span class="comment">#真实类别标签</span></span><br><span class="line">target=torch.tensor([<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>])</span><br><span class="line"><span class="comment"># 对真实类别标签做 独热编码</span></span><br><span class="line">one_hot = F.one_hot(target).float()</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">one_hot:</span></span><br><span class="line"><span class="string">tensor([[0., 1., 0.],</span></span><br><span class="line"><span class="string">        [1., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 1.]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment">#对预测值取log</span></span><br><span class="line">log=torch.log(pred)</span><br><span class="line"><span class="comment">#计算最终的结果</span></span><br><span class="line">res=-torch.sum(one_hot*log)/target.shape[<span class="number">0</span>]</span><br><span class="line">print(res)<span class="comment"># tensor(1.3391)</span></span><br></pre></td></tr></table></figure>
<p>这和我们之前手动计算的结果是一样的。代码很简单，只需注意代码中的<code>one_hot*log</code>是逐元素做乘法。</p>
<p>以上是其内部实现原理。在实际使用时，为了方便，PyTorch已经封装好了以上过程，你只需要调用一下相应的方法或函数即可。</p>
<p>在PyTorch中，有一个叫做<code>nll_loss</code>的函数，可以帮助我们更快的实现上述计算，此时无需对target进行独热编码，于是代码可简化如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="comment">#预测值，已做softmax</span></span><br><span class="line">pred=torch.tensor([[<span class="number">0.2</span>,<span class="number">0.3</span>,<span class="number">0.5</span>],[<span class="number">0.3</span>,<span class="number">0.2</span>,<span class="number">0.5</span>],[<span class="number">0.4</span>,<span class="number">0.4</span>,<span class="number">0.2</span>]])</span><br><span class="line"><span class="comment">#真实类别标签,此时无需再做one_hot，因为nll_loss会自动做</span></span><br><span class="line">target=torch.tensor([<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>])</span><br><span class="line"><span class="comment">#对预测值取log</span></span><br><span class="line">log=torch.log(pred)</span><br><span class="line"><span class="comment">#计算最终的结果</span></span><br><span class="line">res=F.nll_loss(log, target)</span><br><span class="line">print(res)<span class="comment"># tensor(1.3391)</span></span><br></pre></td></tr></table></figure>

<p>等等，还没完。在PyTorch中，最常用于多分类问题的，是<code>CrossEntropyLoss</code>.</p>
<p>它可以看作是<code>softmax</code>+<code>log</code>+<code>nll_loss</code>的集成。</p>
<p>上面的栗子中的预测值是已经做完softmax之后的，为了说明<code>CrossEntropyLoss</code>的原理，我们换一个预测值没有做过softmax的新栗子，这种栗子也是我们通常会遇到的情况：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#4个样本，3分类</span></span><br><span class="line">pred=torch.rand(<span class="number">4</span>,<span class="number">3</span>)</span><br><span class="line"><span class="comment">#真实类别标签</span></span><br><span class="line">target=torch.tensor([<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>])</span><br></pre></td></tr></table></figure>
<p>先按照<code>softmax</code>+<code>log</code>+<code>nll_loss</code>的步骤走一遍：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">logsoftmax=F.log_softmax(pred)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">logsoftmax:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">tensor([[-0.8766, -1.4375, -1.0605],</span></span><br><span class="line"><span class="string">        [-1.0188, -0.9754, -1.3397],</span></span><br><span class="line"><span class="string">        [-0.8926, -1.0962, -1.3615],</span></span><br><span class="line"><span class="string">        [-1.0364, -0.8817, -1.4645]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">res=F.nll_loss(logsoftmax,target)</span><br><span class="line">pritnt(res)<span class="comment">#tensor(1.0523)</span></span><br></pre></td></tr></table></figure>

<p>直接使用<code>CrossEntropyLoss</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">res=F.cross_entropy(pred, target)</span><br><span class="line">print(res)<span class="comment">#tensor(1.0523)</span></span><br></pre></td></tr></table></figure>
<p>结果是一样的。</p>
<p><img src="./8.png" alt="Alt text" loading="lazy"><br>(CrossEntropyLoss的使用)</p>
<p>参考：</p>
<ul>
<li>[1] <a href="https://zhuanlan.zhihu.com/p/35709485" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/35709485</a></li>
<li>[2] <a href="https://zhuanlan.zhihu.com/p/159477597" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/159477597</a></li>
</ul>
</div><div id="reward-container"><span class="hty-icon-button button-glow" id="reward-button" title="打赏" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === &quot;none&quot;) ? &quot;block&quot; : &quot;none&quot;;"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-hand-coin-line"></use></svg></span><div id="reward-comment">I'm so cute. Please give me money.</div><div id="qr" style="display:none;"><div style="display:inline-block"><a href="https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/donate/alipay-qrcode.jpg" target="_blank" rel="noopener"><img loading="lazy" src="https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/donate/alipay-qrcode.jpg" alt="支付宝" title="支付宝"></a><div><span style="color:#00A3EE"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-alipay-line"></use></svg></span></div></div><div style="display:inline-block"><a href="https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/donate/qqpay-qrcode.png" target="_blank" rel="noopener"><img loading="lazy" src="https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/donate/qqpay-qrcode.png" alt="QQ 支付" title="QQ 支付"></a><div><span style="color:#12B7F5"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-qq-line"></use></svg></span></div></div><div style="display:inline-block"><a href="https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/donate/wechatpay-qrcode.jpg" target="_blank" rel="noopener"><img loading="lazy" src="https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/donate/wechatpay-qrcode.jpg" alt="微信支付" title="微信支付"></a><div><span style="color:#2DC100"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-wechat-pay-line"></use></svg></span></div></div></div></div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>雨落诗山山亦奇</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="http://yoursite.com/2021/04/28/%E9%82%A3%E4%BA%9B%E5%B9%B4%EF%BC%8C%E6%88%91%E4%BB%AC%E4%B8%80%E8%B5%B7%E5%AE%9E%E7%8E%B0%E7%9A%84%E4%BA%A4%E5%8F%89%E7%86%B5/" title="那些年，我们一起实现的交叉熵">http://yoursite.com/2021/04/28/%E9%82%A3%E4%BA%9B%E5%B9%B4%EF%BC%8C%E6%88%91%E4%BB%AC%E4%B8%80%E8%B5%B7%E5%AE%9E%E7%8E%B0%E7%9A%84%E4%BA%A4%E5%8F%89%E7%86%B5/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均默认采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><svg class="icon"><use xlink:href="#icon-creative-commons-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-by-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-nc-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-sa-line"></use></svg></a> 许可协议。</li></ul></section></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/2021/05/01/%E9%AD%94%E6%94%B9%E4%BA%A4%E5%8F%89%E7%86%B5/" rel="prev" title="魔改交叉熵"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-left-s-line"></use></svg><span class="post-nav-text">魔改交叉熵</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/2021/04/09/FaceFromX/" rel="next" title="FaceFromX"><span class="post-nav-text">FaceFromX</span><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-right-s-line"></use></svg></a></div></div></div><div class="hty-card" id="comment"><div class="comment-tooltip text-center"><span>要不要和我说些什么？</span><br></div></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2019 – 2021 </span><span class="with-love" id="animate"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-cloud-line"></use></svg></span><span class="author"> 雨落诗山山亦奇</span></div><div class="powered"><span>由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v4.2.1</span><span class="footer-separator">|</span><span>主题 - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v1.6.3</span></div></footer><a class="hty-icon-button" id="back-to-top" aria-label="back-to-top" href="#"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-up-s-line"></use></svg><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#0078E7" stroke-width="2" stroke-linecap="round"></circle></svg></a></div></body></html>