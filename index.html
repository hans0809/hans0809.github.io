<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>南极Python - Python|机器学习|深度学习</title><meta name="keywords" content="Python|机器学习|深度学习|生活感悟"><meta name="author" content="雨落诗山山亦奇"><meta name="copyright" content="雨落诗山山亦奇"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="本站为读研版&amp;工作版博客，大学版移步 --&gt; fuhanshi.github.io">
<meta property="og:type" content="website">
<meta property="og:title" content="南极Python">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="南极Python">
<meta property="og:description" content="本站为读研版&amp;工作版博客，大学版移步 --&gt; fuhanshi.github.io">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://www.cdnjson.com/images/2021/11/27/_20210211193948.png">
<meta property="article:author" content="雨落诗山山亦奇">
<meta property="article:tag" content="Python|机器学习|深度学习|生活感悟">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.cdnjson.com/images/2021/11/27/_20210211193948.png"><link rel="shortcut icon" href="https://www.cdnjson.com/images/2021/11/27/_20210211193948.png"><link rel="canonical" href="http://yoursite.com/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '南极Python',
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2025-04-14 21:28:34'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/StaticFile_HEXO@latest/butterfly/css/macblack.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://www.cdnjson.com/images/2021/11/27/_20210211193948.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">173</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div></div><hr/></div></div><div class="page" id="body-wrap"><header class="full_page" id="page-header" style="background-image: url('/img/tag1.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">南极Python</a></span><div id="menus"><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="site-info"><h1 id="site-title">南极Python</h1><div id="site-subtitle"><span id="subtitle"></span></div></div><div id="scroll-down"><i class="fas fa-angle-down scroll-down-effects"></i></div></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="post_cover left"><a href="/2025/04/10/%E6%8C%87%E4%BB%A4%E5%BE%AE%E8%B0%83/" title="LLM指令微调：训练一个人工智能助手-大模型炼丹术(八)"><img class="post_bg" src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="LLM指令微调：训练一个人工智能助手-大模型炼丹术(八)"></a></div><div class="recent-post-info"><a class="article-title" href="/2025/04/10/%E6%8C%87%E4%BB%A4%E5%BE%AE%E8%B0%83/" title="LLM指令微调：训练一个人工智能助手-大模型炼丹术(八)">LLM指令微调：训练一个人工智能助手-大模型炼丹术(八)</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2025-04-10T14:32:21.000Z" title="发表于 2025-04-10 22:32:21">2025-04-10</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF/">大模型炼丹术</a></span></div><div class="content">在上一篇文章中，我们通过对预训练的 GPT-2 进行微调，得到了一个垃圾邮件分类器。事实上，这种方式是使用 GPT-2 的网络作为 backbone，在其输出后接一个分类头，来完成二分类任务。
在本文中，我们将介绍另一种微调方式：指令微调（Instruction Tuning）。
通过指令微调，我们可以打造一个对话机器人，就像你一直在使用的各种大语言模型应用那样 —— 它能够接收用户的自然语言指令，并输出相应的回复。
一、什么是指令微调？指令微调（Instruction Tuning） 是一种让预训练语言模型学会“听懂人话”的方法。它的核心思想是：通过监督微调（Supervised Fine-Tuning, SFT），让模型学习从「指令（Instruction）」到「输出（Response）」的映射。
这种方式与传统的分类、回归等任务不同，指令微调的数据格式通常是自然语言对话格式：
1234用户：请告诉我Python中如何定义一个函数？助手：你可以使用`def`关键词，例如：def my_function():    print(&quot;Hello World&quot;)

在训 ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/2025/03/23/%E5%BE%AE%E8%B0%83LLM%E5%9E%83%E5%9C%BE%E9%82%AE%E4%BB%B6%E5%88%86%E7%B1%BB%E5%99%A8/" title="LLM微调：训练一个垃圾邮件分类器-大模型炼丹术(七)"><img class="post_bg" src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="LLM微调：训练一个垃圾邮件分类器-大模型炼丹术(七)"></a></div><div class="recent-post-info"><a class="article-title" href="/2025/03/23/%E5%BE%AE%E8%B0%83LLM%E5%9E%83%E5%9C%BE%E9%82%AE%E4%BB%B6%E5%88%86%E7%B1%BB%E5%99%A8/" title="LLM微调：训练一个垃圾邮件分类器-大模型炼丹术(七)">LLM微调：训练一个垃圾邮件分类器-大模型炼丹术(七)</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2025-03-23T13:48:46.000Z" title="发表于 2025-03-23 21:48:46">2025-03-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF/">大模型炼丹术</a></span></div><div class="content">截止到现在，我们已经完成了LLM的整体架构搭建，是时候使用它来做一些下游的任务了。
我们所构建的LLM是GPT2，官方开源了它的预训练权重。如果只是使用GPT2实现文本续写等功能，可以直接加载预训练模型并进行推理。
然而，在实际的任务中，往往需要使用领域数据对LLM进行微调，以适配特定的下游任务，比如垃圾短信分类、对话生成、情感分析等。
本文使用一个垃圾邮件分类的任务，来说明如何基于预训练的GPT2在邮件数据集上进行微调，我们的目标是打造一个垃圾邮件分类器，输入一份邮件的内容，模型给出该邮件是否为垃圾邮件的分类结果。
一、准备垃圾邮件分类数据集在https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip下载垃圾邮件分类数据集，如下：
每一行是一个样本，其中第一列是label，空格后面的是label对应的邮件内容，label总共有两个值，表示这封邮件是否为垃圾邮件。
为乐便于后续数据预处理，将其读取为pandas的数据框格式：
12data_file_path = &quot;sms_spam_collect ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/2025/03/12/%E5%89%96%E6%9E%90LLM%E7%9A%84%E8%A7%A3%E7%A0%81%E7%AD%96%E7%95%A5-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E5%85%AD)/" title="剖析LLM的解码策略-大模型炼丹术(六)"><img class="post_bg" src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="剖析LLM的解码策略-大模型炼丹术(六)"></a></div><div class="recent-post-info"><a class="article-title" href="/2025/03/12/%E5%89%96%E6%9E%90LLM%E7%9A%84%E8%A7%A3%E7%A0%81%E7%AD%96%E7%95%A5-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E5%85%AD)/" title="剖析LLM的解码策略-大模型炼丹术(六)">剖析LLM的解码策略-大模型炼丹术(六)</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2025-03-12T13:40:40.000Z" title="发表于 2025-03-12 21:40:40">2025-03-12</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF/">大模型炼丹术</a></span></div><div class="content">在使用训练好的LLM进行自回归预测下一个token时，我们会选择预测序列中最后一个token对应的预测tensor，作为解码操作的对象。
1234567# 获取模型的预测结果with torch.no_grad():  # 关闭梯度计算，加速推理    logits = model(idx_cond)  # (batch, n_tokens, vocab_size)# 只关注最后一个时间步的预测结果# (batch, n_tokens, vocab_size) 变为 (batch, vocab_size)logit = logits[:, -1, :]  
此时的logit就是用于解码的tensor，batch中的每一个都对应词汇表长度大小vocab_size的一个向量。
如何对该向量进行解码，得到要预测的下一个单词呢？本文介绍几种不同的解码策略。
一、贪心解码我们之前的解码策略是直接给logit应用softmax函数，然后使用argmax取概率值最大的数值对应的索引作为预测的下一个token ID，最后根据token ID在词汇表中查找得到预测的下一个单词：
1next_token  ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/2025/03/11/LLM%E8%87%AA%E5%9B%9E%E5%BD%92%E9%A2%84%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E8%AF%A6%E8%A7%A3-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%BA%94)/" title="LLM自回归预训练过程详解-大模型炼丹术(五)"><img class="post_bg" src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="LLM自回归预训练过程详解-大模型炼丹术(五)"></a></div><div class="recent-post-info"><a class="article-title" href="/2025/03/11/LLM%E8%87%AA%E5%9B%9E%E5%BD%92%E9%A2%84%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E8%AF%A6%E8%A7%A3-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%BA%94)/" title="LLM自回归预训练过程详解-大模型炼丹术(五)">LLM自回归预训练过程详解-大模型炼丹术(五)</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2025-03-11T13:31:10.000Z" title="发表于 2025-03-11 21:31:10">2025-03-11</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF/">大模型炼丹术</a></span></div><div class="content">在前面的4篇文章中，我们已经完成了整个数据流向所需的模块构建，包括tokinizer，embedding，注意力机制，并串联得到了GPT2这个LLM架构。
现在，是时候准备开始训练我们的LLM了。
相比于前面发布的4篇文章，本文将更加偏重于代码实战。
一、准备自回归预训练数据集在开始编写训练脚本之前，我们需要先构建训练所需数据集。这里使用the-verdict.txt，这是在本系列一开始就作为示例使用的一本书。
1234567891011121314import osimport urllib.requestfile_path = &quot;the-verdict.txt&quot;url = &quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt&quot;if not os.path.exists(file_path):    with urllib.request.urlopen(url) as response: ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/2025/03/07/%E5%8A%A8%E6%89%8B%E6%90%AD%E5%BB%BAGPT2%E6%9E%B6%E6%9E%84-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E5%9B%9B)/" title="动手搭建GPT2架构-大模型炼丹术(四)"><img class="post_bg" src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="动手搭建GPT2架构-大模型炼丹术(四)"></a></div><div class="recent-post-info"><a class="article-title" href="/2025/03/07/%E5%8A%A8%E6%89%8B%E6%90%AD%E5%BB%BAGPT2%E6%9E%B6%E6%9E%84-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E5%9B%9B)/" title="动手搭建GPT2架构-大模型炼丹术(四)">动手搭建GPT2架构-大模型炼丹术(四)</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2025-03-07T15:35:12.000Z" title="发表于 2025-03-07 23:35:12">2025-03-07</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF/">大模型炼丹术</a></span></div><div class="content">在前面的3篇文章中，我们已经讲解了训练LLM所需的tokenizer，token/position编码，以及Transformer核心：注意力机制。现在是时候动手搭建GPT的网络架构了。
本文首先搭建GPT架构包含的🧍各个小组件，然后将这些组件串联起来，得到最终的GPT架构。
下图左侧是整个GPT2的架构图，中间是Transformer Block，右侧是我们之前实现的多头注意力层。
我们要搭建的是GPT-2，具有124M的参数量，相关的配置文件先放这儿：
123456789GPT_CONFIG_124M = &#123;    &quot;vocab_size&quot;: 50257,    # Vocabulary size    &quot;context_length&quot;: 1024, # Context length    &quot;emb_dim&quot;: 768,         # Embedding dimension    &quot;n_heads&quot;: 12,          # Number of attention heads   ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/2025/03/04/%E4%BB%8E%E5%8D%95%E5%A4%B4%E5%88%B0%E5%A4%9A%E5%A4%B4%EF%BC%8C%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%B8%89)/" title="从单头到多头，深度解析大模型的注意力机制-大模型炼丹术(三)"><img class="post_bg" src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="从单头到多头，深度解析大模型的注意力机制-大模型炼丹术(三)"></a></div><div class="recent-post-info"><a class="article-title" href="/2025/03/04/%E4%BB%8E%E5%8D%95%E5%A4%B4%E5%88%B0%E5%A4%9A%E5%A4%B4%EF%BC%8C%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%B8%89)/" title="从单头到多头，深度解析大模型的注意力机制-大模型炼丹术(三)">从单头到多头，深度解析大模型的注意力机制-大模型炼丹术(三)</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2025-03-04T14:03:57.000Z" title="发表于 2025-03-04 22:03:57">2025-03-04</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF/">大模型炼丹术</a></span></div><div class="content">在之前的两节内容中，我们已经将输入的句子通过tokenizer映射到了一个个的token ID，并进一步做了连续编码，得到了包含充分语义信息的embedding向量。
现在，让我们继续探索接下来的数据流向。
GPT模型的架构是一个类似Transformer解码器架构的网络，因此本文将从Transformer的核心组件“注意力机制”开始讲起。
一、 直观理解注意力这里通过一个例子展示来直观展现什么是注意力：使用前两节所讲的内容，假设已经将”Your journey starts with one step”这句话编码到embedding空间，embedding维度是3，如下：
12345678inputs = torch.tensor(  [[0.43, 0.15, 0.89], # Your     (x^1)   [0.55, 0.87, 0.66], # journey  (x^2)   [0.57, 0.85, 0.64], # starts   (x^3)   [0.22, 0.58, 0.33], # with     (x^4)   [0.77, 0.25, 0.10],  ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/2025/02/24/%E4%BB%8E%E7%A6%BB%E6%95%A3%E7%9A%84tokenID%E5%88%B0%E5%85%B7%E6%9C%89%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%E7%9A%84embedding-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%BA%8C)/" title="从离散的token IDs到具有语义信息的embedding-大模型炼丹术(二)"><img class="post_bg" src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="从离散的token IDs到具有语义信息的embedding-大模型炼丹术(二)"></a></div><div class="recent-post-info"><a class="article-title" href="/2025/02/24/%E4%BB%8E%E7%A6%BB%E6%95%A3%E7%9A%84tokenID%E5%88%B0%E5%85%B7%E6%9C%89%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%E7%9A%84embedding-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%BA%8C)/" title="从离散的token IDs到具有语义信息的embedding-大模型炼丹术(二)">从离散的token IDs到具有语义信息的embedding-大模型炼丹术(二)</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2025-02-24T14:28:29.000Z" title="发表于 2025-02-24 22:28:29">2025-02-24</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF/">大模型炼丹术</a></span></div><div class="content">在完成了tokenization之后，我们已经可以将一个个的单词映射到对应的数字，称之为token ID，这些数字已经可以被计算机处理。然而，若直接将这些数字应用于模型训练，仍存在一些问题：

缺乏语义信息： Token ID 只是一个索引，本身不包含任何语义信息。例如，“cat” 可能被映射为 ID 1254，而 “dog” 是 ID 3920，这两个 ID 之间的数值关系是无意义的。直接使用它们可能会导致模型误解 token 之间的关联性。

整数之间的数值关系会误导模型： 机器学习模型通常会学习数据之间的模式。如果直接输入 token ID，模型可能会误以为 ID 1254（”cat”）和 ID 3920（”dog”）之间存在某种数学关系（如加减乘除），但实际上 ID 只是索引，没有数值上的逻辑关系。

无法捕捉相似 token 的关系 语义相近的 token 在 embedding 空间中应该具有接近的表示。例如，”king” 和 “queen” 应该在高维空间中比较接近，而 “apple” 和 “computer” 应该相距较远。然而，单纯的 token ID 无法提供这种分 ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/2025/02/20/%E4%BB%8Etokenizer%E8%AF%B4%E8%B5%B7%EF%BC%8C%E4%B8%BALLM%E8%87%AA%E5%9B%9E%E5%BD%92%E9%A2%84%E8%AE%AD%E7%BB%83%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE%E9%9B%86-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%B8%80)/" title="从tokenizer说起，为LLM自回归预训练准备数据集-大模型炼丹术(一)"><img class="post_bg" src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="从tokenizer说起，为LLM自回归预训练准备数据集-大模型炼丹术(一)"></a></div><div class="recent-post-info"><a class="article-title" href="/2025/02/20/%E4%BB%8Etokenizer%E8%AF%B4%E8%B5%B7%EF%BC%8C%E4%B8%BALLM%E8%87%AA%E5%9B%9E%E5%BD%92%E9%A2%84%E8%AE%AD%E7%BB%83%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE%E9%9B%86-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%B8%80)/" title="从tokenizer说起，为LLM自回归预训练准备数据集-大模型炼丹术(一)">从tokenizer说起，为LLM自回归预训练准备数据集-大模型炼丹术(一)</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2025-02-20T14:28:29.000Z" title="发表于 2025-02-20 22:28:29">2025-02-20</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF/">大模型炼丹术</a></span></div><div class="content">本文首先介绍了如何从头开始实现一个自定义tokenizer，用于将原始文本数据转化为模型能够理解的格式。通过这个例子，来直观理解什么是tokenize；接着，分析这种tokenizer的优缺点，引出更常用的BPE；最后，基于BPE构建的tokenizer，构建用于GPT预训练时的数据加载器。
在阅读完本文后，你将学会如何构建用于GPT自回归预训练阶段的数据加载器，这将是你向着LLM训练迈出的第一步！
一、先动手，编写自定义tokenizerstep1. 读取语料读取the-verdict.txt:
12345with open(&quot;the-verdict.txt&quot;, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:    raw_text = f.read()    print(&quot;Total number of character:&quot;, len(raw_text))print(raw_text[:99])
输出：
12Total number of character: 20479I HAD alwa ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/2024/12/30/%E6%8A%8A%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E6%90%AC%E5%88%B0GPU-%E8%8B%B1%E4%BC%9F%E8%BE%BEDALI%E5%8A%A0%E9%80%9F%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/" title="把数据预处理搬到GPU-英伟达DALI加速数据预处理"><img class="post_bg" src="https://ice.frostsky.com/2024/12/30/88a897ed803ad9cb7462f4320a31ac67.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="把数据预处理搬到GPU-英伟达DALI加速数据预处理"></a></div><div class="recent-post-info"><a class="article-title" href="/2024/12/30/%E6%8A%8A%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E6%90%AC%E5%88%B0GPU-%E8%8B%B1%E4%BC%9F%E8%BE%BEDALI%E5%8A%A0%E9%80%9F%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/" title="把数据预处理搬到GPU-英伟达DALI加速数据预处理">把数据预处理搬到GPU-英伟达DALI加速数据预处理</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-12-30T13:32:21.000Z" title="发表于 2024-12-30 21:32:21">2024-12-30</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">深度学习笔记</a></span></div><div class="content">在执行模型前向推理时，往往涉及到一些列数据预处理操作，比如Resize，Normalize等，这些操作通常在CPU上完成，然后CPU将预处理后的图片传送到GPU上执行推理。
由于GPU的运算速度远快于CPU，所以能不能将这些数据预处理操作放到GPU上执行从而加快数据加载的速度呢？
NVIDIA DALI 可以！
NVIDIA DALI (Data Loading Library) 是一个加速数据加载和预处理的库，专为深度学习任务设计。它能将图像和视频的复杂预处理操作（尤其是在模型训练阶段，通常涉及大量的数据增强预处理操作）从 CPU 转移到 GPU 上，从而减少数据加载瓶颈，提升 GPU 的利用率。DALI 支持多种格式（如 JPEG、PNG、TFRecord 等），并能与主流深度学习框架（如 PyTorch 和 TensorFlow）无缝集成，使得数据预处理和模型前向推理可以高效并行进行。
本文介绍DALI的使用方法，以及如何将PyTorch的数据加载器替换成DALI的数据加载器，并测试加速效果。
安装NVIDIA DALI对于CUDA 11.x，执行如下命令行：
1pip inst ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/2024/12/07/PyTorch%E8%BD%ACTensorRT-engine%E4%BF%9D%E5%A7%86%E7%BA%A7%E6%95%99%E7%A8%8B/Markdown%20_%20%E8%AE%A9%E6%8E%92%E7%89%88%E5%8F%98%20Nice/" title="无题"><img class="post_bg" src="/img/tag1.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="无题"></a></div><div class="recent-post-info"><a class="article-title" href="/2024/12/07/PyTorch%E8%BD%ACTensorRT-engine%E4%BF%9D%E5%A7%86%E7%BA%A7%E6%95%99%E7%A8%8B/Markdown%20_%20%E8%AE%A9%E6%8E%92%E7%89%88%E5%8F%98%20Nice/" title="无题">无题</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-12-07T07:43:12.125Z" title="发表于 2024-12-07 15:43:12">2024-12-07</time></span></div><div class="content">


.anticon {
  display: inline-block;
  color: inherit;
  font-style: normal;
  line-height: 0;
  text-align: center;
  text-transform: none;
  vertical-align: -0.125em;
  text-rendering: optimizeLegibility;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

.anticon > * {
  line-height: 1;
}

.anticon svg {
  display: inline-block;
}

.anticon::before {
  display: none;
}

.anticon .anticon-icon {
  display: block;
}

.anticon[tabindex] {
  cursor: pointer;
}

.ant ...</div></div></div><nav id="pagination"><div class="pagination"><span class="page-number current">1</span><a class="page-number" href="/page/2/#content-inner">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/18/#content-inner">18</a><a class="extend next" rel="next" href="/page/2/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://www.cdnjson.com/images/2021/11/27/_20210211193948.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">雨落诗山山亦奇</div><div class="author-info__description">本站为读研版&工作版博客，大学版移步 --> fuhanshi.github.io</div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">173</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">本站内容的最终版本将发布在微信公众号[南极Python]</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/04/10/%E6%8C%87%E4%BB%A4%E5%BE%AE%E8%B0%83/" title="LLM指令微调：训练一个人工智能助手-大模型炼丹术(八)"><img src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="LLM指令微调：训练一个人工智能助手-大模型炼丹术(八)"/></a><div class="content"><a class="title" href="/2025/04/10/%E6%8C%87%E4%BB%A4%E5%BE%AE%E8%B0%83/" title="LLM指令微调：训练一个人工智能助手-大模型炼丹术(八)">LLM指令微调：训练一个人工智能助手-大模型炼丹术(八)</a><time datetime="2025-04-10T14:32:21.000Z" title="发表于 2025-04-10 22:32:21">2025-04-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/03/23/%E5%BE%AE%E8%B0%83LLM%E5%9E%83%E5%9C%BE%E9%82%AE%E4%BB%B6%E5%88%86%E7%B1%BB%E5%99%A8/" title="LLM微调：训练一个垃圾邮件分类器-大模型炼丹术(七)"><img src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="LLM微调：训练一个垃圾邮件分类器-大模型炼丹术(七)"/></a><div class="content"><a class="title" href="/2025/03/23/%E5%BE%AE%E8%B0%83LLM%E5%9E%83%E5%9C%BE%E9%82%AE%E4%BB%B6%E5%88%86%E7%B1%BB%E5%99%A8/" title="LLM微调：训练一个垃圾邮件分类器-大模型炼丹术(七)">LLM微调：训练一个垃圾邮件分类器-大模型炼丹术(七)</a><time datetime="2025-03-23T13:48:46.000Z" title="发表于 2025-03-23 21:48:46">2025-03-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/03/12/%E5%89%96%E6%9E%90LLM%E7%9A%84%E8%A7%A3%E7%A0%81%E7%AD%96%E7%95%A5-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E5%85%AD)/" title="剖析LLM的解码策略-大模型炼丹术(六)"><img src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="剖析LLM的解码策略-大模型炼丹术(六)"/></a><div class="content"><a class="title" href="/2025/03/12/%E5%89%96%E6%9E%90LLM%E7%9A%84%E8%A7%A3%E7%A0%81%E7%AD%96%E7%95%A5-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E5%85%AD)/" title="剖析LLM的解码策略-大模型炼丹术(六)">剖析LLM的解码策略-大模型炼丹术(六)</a><time datetime="2025-03-12T13:40:40.000Z" title="发表于 2025-03-12 21:40:40">2025-03-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/03/11/LLM%E8%87%AA%E5%9B%9E%E5%BD%92%E9%A2%84%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E8%AF%A6%E8%A7%A3-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%BA%94)/" title="LLM自回归预训练过程详解-大模型炼丹术(五)"><img src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="LLM自回归预训练过程详解-大模型炼丹术(五)"/></a><div class="content"><a class="title" href="/2025/03/11/LLM%E8%87%AA%E5%9B%9E%E5%BD%92%E9%A2%84%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E8%AF%A6%E8%A7%A3-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%BA%94)/" title="LLM自回归预训练过程详解-大模型炼丹术(五)">LLM自回归预训练过程详解-大模型炼丹术(五)</a><time datetime="2025-03-11T13:31:10.000Z" title="发表于 2025-03-11 21:31:10">2025-03-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/03/07/%E5%8A%A8%E6%89%8B%E6%90%AD%E5%BB%BAGPT2%E6%9E%B6%E6%9E%84-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E5%9B%9B)/" title="动手搭建GPT2架构-大模型炼丹术(四)"><img src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="动手搭建GPT2架构-大模型炼丹术(四)"/></a><div class="content"><a class="title" href="/2025/03/07/%E5%8A%A8%E6%89%8B%E6%90%AD%E5%BB%BAGPT2%E6%9E%B6%E6%9E%84-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E5%9B%9B)/" title="动手搭建GPT2架构-大模型炼丹术(四)">动手搭建GPT2架构-大模型炼丹术(四)</a><time datetime="2025-03-07T15:35:12.000Z" title="发表于 2025-03-07 23:35:12">2025-03-07</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>分类</span>
            <a class="card-more-btn" href="/categories/" title="查看更多">
    <i class="fas fa-angle-right"></i></a>
            </div>
            <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E5%91%93%E8%AF%AD%E7%B3%BB%E5%88%97%E8%BF%9E%E8%BD%BD/"><span class="card-category-list-name">呓语系列连载</span><span class="card-category-list-count">8</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF/"><span class="card-category-list-name">大模型炼丹术</span><span class="card-category-list-count">8</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"><span class="card-category-list-name">推荐系统</span><span class="card-category-list-count">18</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/"><span class="card-category-list-name">数据竞赛</span><span class="card-category-list-count">8</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"><span class="card-category-list-name">机器学习算法</span><span class="card-category-list-count">7</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><span class="card-category-list-name">深度学习笔记</span><span class="card-category-list-count">95</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E7%A5%9E%E5%A5%87%E7%9A%84Python/"><span class="card-category-list-name">神奇的Python</span><span class="card-category-list-count">4</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E8%B7%A8%E8%80%83%E5%B0%8F%E7%99%BD%E5%AD%A6%E5%88%B7%E9%A2%98/"><span class="card-category-list-name">跨考小白学刷题</span><span class="card-category-list-count">2</span></a></li>
            </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>标签</span></div><div class="card-tag-cloud"><a href="/tags/DL/" style="font-size: 1.5em; color: #99a9bf">DL</a> <a href="/tags/GAN/" style="font-size: 1.42em; color: #99a6b7">GAN</a> <a href="/tags/LLM/" style="font-size: 1.26em; color: #999fa8">LLM</a> <a href="/tags/ML/" style="font-size: 1.26em; color: #999fa8">ML</a> <a href="/tags/Python/" style="font-size: 1.42em; color: #99a6b7">Python</a> <a href="/tags/RL/" style="font-size: 1.1em; color: #999">RL</a> <a href="/tags/Spark/" style="font-size: 1.1em; color: #999">Spark</a> <a href="/tags/Transformer/" style="font-size: 1.1em; color: #999">Transformer</a> <a href="/tags/%E5%91%93%E8%AF%AD/" style="font-size: 1.26em; color: #999fa8">呓语</a> <a href="/tags/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/" style="font-size: 1.1em; color: #999">性能优化</a> <a href="/tags/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/" style="font-size: 1.1em; color: #999">扩散模型</a> <a href="/tags/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/" style="font-size: 1.34em; color: #99a3b0">推荐算法</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/" style="font-size: 1.18em; color: #999ca1">数据竞赛</a> <a href="/tags/%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86/" style="font-size: 1.1em; color: #999">模型推理</a> <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/" style="font-size: 1.1em; color: #999">计算机基础</a></div></div><div class="card-widget card-archives"><div class="item-headline"><i class="fas fa-archive"></i><span>归档</span><a class="card-more-btn" href="/archives/" title="查看更多">
    <i class="fas fa-angle-right"></i></a></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/04/"><span class="card-archive-list-date">四月 2025</span><span class="card-archive-list-count">1</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/03/"><span class="card-archive-list-date">三月 2025</span><span class="card-archive-list-count">5</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/02/"><span class="card-archive-list-date">二月 2025</span><span class="card-archive-list-count">2</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/12/"><span class="card-archive-list-date">十二月 2024</span><span class="card-archive-list-count">3</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/10/"><span class="card-archive-list-date">十月 2024</span><span class="card-archive-list-count">2</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/09/"><span class="card-archive-list-date">九月 2024</span><span class="card-archive-list-count">1</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/06/"><span class="card-archive-list-date">六月 2024</span><span class="card-archive-list-count">1</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/05/"><span class="card-archive-list-date">五月 2024</span><span class="card-archive-list-count">1</span></a></li></ul></div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>网站资讯</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">文章数目 :</div><div class="item-count">173</div></div><div class="webinfo-item"><div class="item-name">本站总字数 :</div><div class="item-count">370.1k</div></div><div class="webinfo-item"><div class="item-name">本站访客数 :</div><div class="item-count" id="busuanzi_value_site_uv"></div></div><div class="webinfo-item"><div class="item-name">本站总访问量 :</div><div class="item-count" id="busuanzi_value_site_pv"></div></div><div class="webinfo-item"><div class="item-name">最后更新时间 :</div><div class="item-count" id="last-push-date" data-lastPushDate="2025-04-14T13:28:33.914Z"></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/img/tag1.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By 雨落诗山山亦奇</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><div class="js-pjax"><script>function subtitleType () {
  if (true) { 
    window.typed = new Typed("#subtitle", {
      strings: ["期望始终为零，方差交给时间"],
      startDelay: 300,
      typeSpeed: 150,
      loop: true,
      backSpeed: 50
    })
  } else {
    document.getElementById("subtitle").innerHTML = '期望始终为零，方差交给时间'
  }
}

if (true) {
  if (typeof Typed === 'function') {
    subtitleType()
  } else {
    getScript('https://cdn.jsdelivr.net/npm/typed.js/lib/typed.min.js').then(subtitleType)
  }
} else {
  subtitleType()
}</script></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/fireworks.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>