<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>南极Python - Python|机器学习|深度学习</title><meta name="keywords" content="Python|机器学习|深度学习|生活感悟"><meta name="author" content="雨落诗山山亦奇"><meta name="copyright" content="雨落诗山山亦奇"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="本站为读研版&amp;工作版博客，大学版移步 --&gt; fuhanshi.github.io">
<meta property="og:type" content="website">
<meta property="og:title" content="南极Python">
<meta property="og:url" content="http://yoursite.com/page/11/index.html">
<meta property="og:site_name" content="南极Python">
<meta property="og:description" content="本站为读研版&amp;工作版博客，大学版移步 --&gt; fuhanshi.github.io">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://www.cdnjson.com/images/2021/11/27/_20210211193948.png">
<meta property="article:author" content="雨落诗山山亦奇">
<meta property="article:tag" content="Python|机器学习|深度学习|生活感悟">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.cdnjson.com/images/2021/11/27/_20210211193948.png"><link rel="shortcut icon" href="https://www.cdnjson.com/images/2021/11/27/_20210211193948.png"><link rel="canonical" href="http://yoursite.com/page/11/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '南极Python',
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2025-03-10 20:41:02'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/StaticFile_HEXO@latest/butterfly/css/macblack.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://www.cdnjson.com/images/2021/11/27/_20210211193948.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">169</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div></div><hr/></div></div><div class="page" id="body-wrap"><header class="full_page" id="page-header" style="background-image: url('/img/tag1.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">南极Python</a></span><div id="menus"><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="site-info"><h1 id="site-title">南极Python</h1><div id="site-subtitle"><span id="subtitle"></span></div></div><div id="scroll-down"><i class="fas fa-angle-down scroll-down-effects"></i></div></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="post_cover left"><a href="/2021/02/28/Transformer/" title="Transformer"><img class="post_bg" src="https://z3.ax1x.com/2021/04/05/cM6JT1.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Transformer"></a></div><div class="recent-post-info"><a class="article-title" href="/2021/02/28/Transformer/" title="Transformer">Transformer</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2021-02-28T08:13:30.000Z" title="发表于 2021-02-28 16:13:30">2021-02-28</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">深度学习笔记</a></span></div><div class="content">Transformer的结构Transformer的结构如上图所示，我们将其拆解为x个小部分，逐个部分用代码实现，然后再将各个部分联结起来，形成最终的Transformer。
关于Transformer的原理，网上已经有很多优质的文章了，这里我们关心其代码实现。对于其每一个子模块(以类的形式定义)，我们都会实例化一个对象，用具体的数值代入其中，把中间过程中产生的变量维度及相关信息打印出来，这些都体现在代码注释中，请留意。
Muti-Head AttentionMuti-Head Attention接收输入q,k,v，维度在这里都是$[4,3,512]$，输出维度也是$[4,3,512]$。
q和k的维度是一致的，而v可以和它们不一致，这里只是为了方便才将三者维度保持一致。
SelfAttention实现代码如下 (注意注释)
123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707 ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/2021/02/23/%E8%83%BD%E6%8A%8A%E5%86%AC%E5%A4%A9%E5%8F%98%E5%A4%8F%E5%A4%A9%EF%BC%8C%E8%8B%B9%E6%9E%9C%E5%8F%98%E6%88%90%E6%A9%98%E5%AD%90%E7%9A%84CycleGAN/" title="CycleGAN"><img class="post_bg" src="https://s3.ax1x.com/2021/03/01/6PNkPU.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="CycleGAN"></a></div><div class="recent-post-info"><a class="article-title" href="/2021/02/23/%E8%83%BD%E6%8A%8A%E5%86%AC%E5%A4%A9%E5%8F%98%E5%A4%8F%E5%A4%A9%EF%BC%8C%E8%8B%B9%E6%9E%9C%E5%8F%98%E6%88%90%E6%A9%98%E5%AD%90%E7%9A%84CycleGAN/" title="CycleGAN">CycleGAN</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2021-02-23T05:22:46.000Z" title="发表于 2021-02-23 13:22:46">2021-02-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">深度学习笔记</a></span></div><div class="content">
上图中，最左边一列是原图，右侧的4列是将原图转换成其他风格后的图像。这种转换被称为图像风格迁移。
实现图像风格迁移的方法有很多，这里我们介绍CycleGAN并用它来实现图像风格迁移。
CycleGAN的网络结构CycleGAN的网络结构如上图所示。它的训练数据集需要来自两个不同的域（就是两种不同风格的图像）：$A,B$；
CycleGAN包含两个生成器：$G_{AB},G_{BA}$，分别用于将A风格图像转换为B风格图像，以及将B风格图像转换为A风格图像；
同样，它也包含两个判别器：$D_{A},D_{B}$。
CycleGAN的损失函数在原始GAN损失函数的基础上，CycleGAN为了防止生成器偷懒（解释见下一段），增加了循环一致性损失，这个东西其实就是重构损失，以保证转换后的图像和原图像的内容一致性。
李宏毅老师的PPT中一幅图很形象的展示了循环一致性损失
现在举个例子解释偷懒 的含义：比如由A风格转换为B风格时，需要把转换后的图像（记作$B_{fake}$）与真实的B风格图像（记作$B_{real}$）一起喂入判别器$D_B$，而$D_B$只是希望$B_{real}$与真实B风 ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/2021/02/20/%E4%BB%8EGAN%E5%88%B0WGAN%E5%86%8D%E5%88%B0WGAN-GP/" title="从GAN到WGAN再到WGAN-GP"><img class="post_bg" src="https://s3.ax1x.com/2021/02/21/yo4C9A.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="从GAN到WGAN再到WGAN-GP"></a></div><div class="recent-post-info"><a class="article-title" href="/2021/02/20/%E4%BB%8EGAN%E5%88%B0WGAN%E5%86%8D%E5%88%B0WGAN-GP/" title="从GAN到WGAN再到WGAN-GP">从GAN到WGAN再到WGAN-GP</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2021-02-20T09:30:48.000Z" title="发表于 2021-02-20 17:30:48">2021-02-20</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">深度学习笔记</a></span></div><div class="content">之前介绍了GAN的原理，并使用celeba数据集训练了一个基于DCGAN的”假”人脸生成器（传送门戳我），这里我把它的生成效果图搬运过来了在GAN问世后，其出色的表现使得对于GAN的研究一时风生水起(至今还在持续)，越来越多关于GAN的研究成果被发表，GAN本身存在的缺陷也逐步被挖掘出来。
本文不会陷入繁杂的数学推导中，而是指出WGAN相比于原始GAN的改进之处，以及进一步提出的WGAN-GP，并动手用PyTorch进行实现。
WGANWGAN便是对于原始GAN的一种改进方案，它的作者用了大量篇幅指出了原始GAN的不足之处，并最终给出了自己的解决方案。虽然其中蕴含了大量的数学推导，但推导的结论却出乎意料的简单，或许这就是数学的魅力。
说完一堆废话后，来看看改进得到的WGAN相比于原始GAN有哪些改动，这里直接把WGAN作者给出的训练算法贴出来，然后做简要分析。
5和10分别给出了判别器和生成器的损失函数，相比于原始GAN 的损失函数，仅仅是去掉了log。如果不能一下子看出来，可以把原始GAN的目标函数搬过来对比下。
原始GAN的优化目标：
其中的E代表期望，由大数定律，我们可以用均值近 ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/2021/02/19/%E7%81%AB%E7%88%86%E5%85%A8%E7%90%83%E7%9A%84GAN%E7%A9%B6%E7%AB%9F%E6%98%AF%E4%BD%95%E6%96%B9%E7%A5%9E%E5%9C%A3/" title="火爆全球的GAN究竟是何方神圣?"><img class="post_bg" src="https://s3.ax1x.com/2021/02/20/y4571x.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="火爆全球的GAN究竟是何方神圣?"></a></div><div class="recent-post-info"><a class="article-title" href="/2021/02/19/%E7%81%AB%E7%88%86%E5%85%A8%E7%90%83%E7%9A%84GAN%E7%A9%B6%E7%AB%9F%E6%98%AF%E4%BD%95%E6%96%B9%E7%A5%9E%E5%9C%A3/" title="火爆全球的GAN究竟是何方神圣?">火爆全球的GAN究竟是何方神圣?</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2021-02-19T06:55:30.000Z" title="发表于 2021-02-19 14:55:30">2021-02-19</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">深度学习笔记</a></span></div><div class="content">故事时间从前有一个人，他希望通过制造假币来发家致富。
于是，他开始学习制造假币。
一开始，他的技术太菜，制作的假币刚流入市场就被警察发现了。
他不甘心，于是继续学习来提升造假币技术，这一次，假币并没有被发现，他很开心的数着钱。
可是，过了一段时间，敏锐的警察使用刚刚学习到的新知识，破获了他的假币。
但他还是不甘示弱，继续提升造假币的技术
警察也继续学习新的假币鉴别技术
就这样，他的造假币技术一直在提升，警察鉴别假币的技术也在不断提升
在互相抗衡很久以后，他的造假币技术到了炉火纯青的地步，以至于警察都难以鉴别。
GAN是什么？
生成对抗网络(Generative adversarial network, GAN)由生成器(一般用$G$表示)和判别器(一般用$D$表示)组成，常用于生成”假”的东西，比如假的文本，假的人脸图像等等，本文以图像生成为例进行叙述。
生成器负责将从某分布中随机采样的噪声$z$通过神经网络映射为”生成图像”$G(z)$；判别器负责鉴定给定的图像是真实图像$X$还是生成器生成的图像$G(z)$。
在上面的故事中，警察充当着判别器$D$的角色，而造假币的人充当着生成器$ ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/2021/02/14/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/" title="变分自编码器"><img class="post_bg" src="https://s3.ax1x.com/2021/02/19/yfN6AA.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="变分自编码器"></a></div><div class="recent-post-info"><a class="article-title" href="/2021/02/14/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/" title="变分自编码器">变分自编码器</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2021-02-14T11:49:02.000Z" title="发表于 2021-02-14 19:49:02">2021-02-14</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">深度学习笔记</a></span></div><div class="content">
Vae他有一些烦恼反正现在的年轻人都有许多烦恼那么多要思考那么多要寻找诱惑太多 不坚定就犯错了

哦哦，不对，不是这个Vae，也不是uae~

是接下来出场的VAE~
对比AE，引出VAE之前介绍了自编码器(AE)的原理(传送门)，当时讲到自编码器并不具有真正的生成能力，以图像为例来说，它只能将输入的图像$X$编码成隐向量$z$，然后将$z$作为解码器的输入，得到输出图像$X’$。如果我们尝试将与$z$的shape一致的”随机特征表示”输入解码器，那么得到的将是毫无意义的噪声图像。
变分自编码器(VAE)突破了这一限制！
先给出结论：在VAE中，只要随机特征表示（这里也将这些特征表示记作$z$）是从某些分布，如标准正态分布中采样得到的，那么将$z$输入解码器之后可以得到与训练集图像类似但不同于训练集中任何一张图像的新图像。
比如，训练集是手写数字图像，那么在训练完成后，将从标准正态分布中采样得到的$z$输入解码器，可以得到一些新的手写数字图像。
以上所说的特征表示$z$，被称为隐向量(latent vector)。
VAE究竟是如何做到这一点的呢？且往下看。
VAE的结构
变分自编码 ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/2021/02/11/%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/" title="自编码器"><img class="post_bg" src="https://s3.ax1x.com/2021/02/11/yBsqxJ.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="自编码器"></a></div><div class="recent-post-info"><a class="article-title" href="/2021/02/11/%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/" title="自编码器">自编码器</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2021-02-11T05:31:09.000Z" title="发表于 2021-02-11 13:31:09">2021-02-11</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">深度学习笔记</a></span></div><div class="content">自编码器的结构自编码器(Auto Encoder)是一种神经网络模型。它由两部分组成：编码器(Ecoder)和解码器(Decoder)。
编码器用于将输入数据(Input Data)进行编码，从而将输入数据映射到维度较低的隐空间(Latent Space)，得到被编码的数据(Encoded Data)。
解码器用于将隐空间中被编码的数据还原(解码)成”输入数据”。这里之所以打引号，是因为还原得到的”输入数据”相比于一开始的输入数据来说会有一些损失，所以并不是真正意义上的输入数据。
下图展示了自编码器的结构
自编码器的应用数据降维/特征提取从自编码器的结构很容易想到它的这一用途。在训练阶段，$X$经过编码器映射得到低维的$z$，$z$通过解码器还原出与$X$维度一致，内容相似的$X’$，通过反向传播来更新网络权重，以最小化输入$X$与输出$X’$之间的损失。
其中的$z$便是降维后的数据，因为如果可以通过$z$还原出输入$X$,那么可以说$z$已经学习到了$X$的大部分特征。
同时，$z$是一个被高度压缩(降维)的输入数据的表示，所以也可以将自编码器看作一个特征提取器，提取到的特征表示就 ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/2021/02/08/%E5%9B%9E%E5%BD%92%E6%A0%91/" title="回归树"><img class="post_bg" src="https://s3.ax1x.com/2021/02/11/yBySIK.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="回归树"></a></div><div class="recent-post-info"><a class="article-title" href="/2021/02/08/%E5%9B%9E%E5%BD%92%E6%A0%91/" title="回归树">回归树</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2021-02-08T10:52:31.000Z" title="发表于 2021-02-08 18:52:31">2021-02-08</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/">机器学习算法</a></span></div><div class="content">开篇在之前的决策树讲解（传送门）中，我们使用ID3算法生成了一棵决策树，并且在文章最后指出除了ID3算法，常用的决策树生成算法还有C4.5。
但是，无论是基于信息增益的ID3算法，还是基于信息增益比的C4.5算法，它们都只能处理分类问题，而对于回归问题就束手无策了。
现在，我们要介绍一种既可用于分类任务又可用于回归任务的决策树的生成算法：CART算法。
CART的全称是 classification and regression tree，译为分类与回归树，该算法由两步组成：(1)决策树生成；(2)决策树剪枝。
本文重点关心如何用决策树做回归任务，因此本文的主题是讲解回归树的生成原理及其Python实现。下面正文开始。
回归树的生成对于使用ID3算法或者C4.5算法生成的决策树，这棵树可能是多岔的，因为某个被选定用于划分数据集的特征的不同取值可能多于两个。而这里基于CART算法的决策树，无论是回归树还是分类树，它们都是二叉树。
以回归树为例，为什么是二叉树呢？这还得从回归树的生成原理讲起。
对于给定的训练集$(X,Y)$，其中$Y$是连续型变量，首先*按照某种方法选择某一个特征(这里记 ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/2021/02/04/AdaBoost/" title="AdaBoost"><img class="post_bg" src="https://s3.ax1x.com/2021/02/11/yBy7wt.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="AdaBoost"></a></div><div class="recent-post-info"><a class="article-title" href="/2021/02/04/AdaBoost/" title="AdaBoost">AdaBoost</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2021-02-04T12:51:04.000Z" title="发表于 2021-02-04 20:51:04">2021-02-04</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/">机器学习算法</a></span></div><div class="content">开篇AdaBoost是一种提升(boosting)方法。
你可能听过“众人拾柴火焰高”这句话，提升方法的思想与这句话的思想颇有相似之处。
一个人拾到的柴火，只能维持小的火苗；但一群人一起拾柴，供给同一火堆，那么这个火堆终将燃起熊熊大火。
对于给定的训练集，单个分类器的分类能力可能并不算好；但如果同时训练一堆分类器，让分类器们一起做判断，那么分类结果将会比任何一个单独的分类器做分类都要好。
上面的单个分类器被称为“弱分类器”，若干个“弱分类器”联合起来，就得到了“强分类器”。
AdaBoost作为最具代表性的提升方法，自然也蕴含着这种集体智慧的思想。至于其具体细节，且往下看。
AdaBoost 算法AdaBoost为训练集中每个样本设置一个可调整的权值，并且在每一轮训练结束后将被误分类的样本的权值加大，将被正确分类的样本的权值减小。这样被误分类的样本在下一轮分类时会更加受到关注。
假设经过了$M$轮训练，则会得到$M$个弱分类器。将这$M$个弱分类器通过加权求和（弱分类器分类能力相对越强，权值就越大）的方式联结起来，便得到了一个强分类器。
以上是AdBoost核心思想的文字描述，现在用数 ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/2021/02/02/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/" title="朴素贝叶斯"><img class="post_bg" src="https://s3.ax1x.com/2021/02/03/ylZ48s.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="朴素贝叶斯"></a></div><div class="recent-post-info"><a class="article-title" href="/2021/02/02/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/" title="朴素贝叶斯">朴素贝叶斯</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2021-02-02T11:03:29.000Z" title="发表于 2021-02-02 19:03:29">2021-02-02</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/">机器学习算法</a></span></div><div class="content">开篇正如其名，”朴素贝叶斯”原理”朴素”，实现简单，是一种常用的机器学习算法。
为何“朴素”？如何“学习”？如何分类？别急，咱们慢慢道来~
概率统计回忆录朴素贝叶斯也是贝叶斯方法的一种，提起贝叶斯，学过概率统计的你一定听说过条件概率公式，全概率公式和贝叶斯公式吧，忘记了也没关系，我们先来快速过一遍。
条件概率公式：
$$P(A|B)=\frac{P(A,B)}{P(B)}$$
全概率公式：
$$P(B)=\sum_{i=1}^{n}P(A_i)P(B|A_i)$$
贝叶斯公式:
$$P(A|B)=\frac{P(A,B)}{P(B)}=\frac{P(B|A)P(A)}{ \sum_{i=1}^{n}P(A_i)P(B|A_i)}$$
可以发现，贝叶斯公式其实就是由条件概率公式和全概率公式推导的，贝叶斯公式的分母是一个全概率公式，分子是一个条件概率公式。
后续的推导将会用到上面的公式。
朴素贝叶斯的训练（学习）方法朴素贝叶斯在使用训练数据进行“学习”时，其实是在学习数据的生成机制，具体点，是在学习特征$X$与标签$Y$的联合概率分布$P(X,Y)$ 。
再进一步解释：根据条件概率公式， ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/2021/01/31/%E5%86%B3%E7%AD%96%E6%A0%91/" title="决策树"><img class="post_bg" src="https://s3.ax1x.com/2021/02/02/ymz9Df.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="决策树"></a></div><div class="recent-post-info"><a class="article-title" href="/2021/01/31/%E5%86%B3%E7%AD%96%E6%A0%91/" title="决策树">决策树</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2021-01-31T08:13:35.000Z" title="发表于 2021-01-31 16:13:35">2021-01-31</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/">机器学习算法</a></span></div><div class="content">开篇决策树，可以看做一个if-else规则的集合，比如下图就是一棵决策树：

其中，圆圈代表特征，矩形代表最终的类别，圆圈和矩形都可以称为树的节点，决策树正是由这样一个个节点组成的。
本文的主要内容是从零开始构建一棵决策树，中间过程会涉及诸如熵，信息增益以及决策树等概念。
按照老规矩，先上栗子。
信贷决策问题这里有一份数据集，其中每个ID代表一个人，类别列代表是否同意这个人的贷款申请，中间列（年龄，有工作，有自己的房子，信贷情况）是4个特征，现在要求你利用这四个特征及类别标签，构建一棵决策树来决定是否同意贷款申请人的贷款申请。| ID| 年龄|  有工作  | 有自己的房子| 信贷情况 |类别|| ——–  | —–:  | :—-:  |:—–:|:—–:|:—–:|| 1     | 青年 |     否     |否|一般|否|2       |   青年  |   否   |否|好|否| 3        |    青年    |  是  |否|好|是|| 4        |    青年    |  是  |是|一般|是| 5        |    青年    |  否   ...</div></div></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/page/10/#content-inner"><i class="fas fa-chevron-left fa-fw"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/#content-inner">10</a><span class="page-number current">11</span><a class="page-number" href="/page/12/#content-inner">12</a><span class="space">&hellip;</span><a class="page-number" href="/page/17/#content-inner">17</a><a class="extend next" rel="next" href="/page/12/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://www.cdnjson.com/images/2021/11/27/_20210211193948.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">雨落诗山山亦奇</div><div class="author-info__description">本站为读研版&工作版博客，大学版移步 --> fuhanshi.github.io</div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">169</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">本站内容的最终版本将发布在微信公众号[南极Python]</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/03/07/%E5%8A%A8%E6%89%8B%E6%90%AD%E5%BB%BAGPT2%E6%9E%B6%E6%9E%84-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E5%9B%9B)/" title="动手搭建GPT2架构-大模型炼丹术(四)"><img src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="动手搭建GPT2架构-大模型炼丹术(四)"/></a><div class="content"><a class="title" href="/2025/03/07/%E5%8A%A8%E6%89%8B%E6%90%AD%E5%BB%BAGPT2%E6%9E%B6%E6%9E%84-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E5%9B%9B)/" title="动手搭建GPT2架构-大模型炼丹术(四)">动手搭建GPT2架构-大模型炼丹术(四)</a><time datetime="2025-03-07T15:35:12.000Z" title="发表于 2025-03-07 23:35:12">2025-03-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/03/04/%E4%BB%8E%E5%8D%95%E5%A4%B4%E5%88%B0%E5%A4%9A%E5%A4%B4%EF%BC%8C%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%B8%89)/" title="从单头到多头，深度解析大模型的注意力机制-大模型炼丹术(三)"><img src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="从单头到多头，深度解析大模型的注意力机制-大模型炼丹术(三)"/></a><div class="content"><a class="title" href="/2025/03/04/%E4%BB%8E%E5%8D%95%E5%A4%B4%E5%88%B0%E5%A4%9A%E5%A4%B4%EF%BC%8C%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%B8%89)/" title="从单头到多头，深度解析大模型的注意力机制-大模型炼丹术(三)">从单头到多头，深度解析大模型的注意力机制-大模型炼丹术(三)</a><time datetime="2025-03-04T14:03:57.000Z" title="发表于 2025-03-04 22:03:57">2025-03-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/02/24/%E4%BB%8E%E7%A6%BB%E6%95%A3%E7%9A%84tokenID%E5%88%B0%E5%85%B7%E6%9C%89%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%E7%9A%84embedding-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%BA%8C)/" title="从离散的token IDs到具有语义信息的embedding-大模型炼丹术(二)"><img src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="从离散的token IDs到具有语义信息的embedding-大模型炼丹术(二)"/></a><div class="content"><a class="title" href="/2025/02/24/%E4%BB%8E%E7%A6%BB%E6%95%A3%E7%9A%84tokenID%E5%88%B0%E5%85%B7%E6%9C%89%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%E7%9A%84embedding-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%BA%8C)/" title="从离散的token IDs到具有语义信息的embedding-大模型炼丹术(二)">从离散的token IDs到具有语义信息的embedding-大模型炼丹术(二)</a><time datetime="2025-02-24T14:28:29.000Z" title="发表于 2025-02-24 22:28:29">2025-02-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/02/20/%E4%BB%8Etokenizer%E8%AF%B4%E8%B5%B7%EF%BC%8C%E4%B8%BALLM%E8%87%AA%E5%9B%9E%E5%BD%92%E9%A2%84%E8%AE%AD%E7%BB%83%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE%E9%9B%86-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%B8%80)/" title="从tokenizer说起，为LLM自回归预训练准备数据集-大模型炼丹术(一)"><img src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="从tokenizer说起，为LLM自回归预训练准备数据集-大模型炼丹术(一)"/></a><div class="content"><a class="title" href="/2025/02/20/%E4%BB%8Etokenizer%E8%AF%B4%E8%B5%B7%EF%BC%8C%E4%B8%BALLM%E8%87%AA%E5%9B%9E%E5%BD%92%E9%A2%84%E8%AE%AD%E7%BB%83%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE%E9%9B%86-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%B8%80)/" title="从tokenizer说起，为LLM自回归预训练准备数据集-大模型炼丹术(一)">从tokenizer说起，为LLM自回归预训练准备数据集-大模型炼丹术(一)</a><time datetime="2025-02-20T14:28:29.000Z" title="发表于 2025-02-20 22:28:29">2025-02-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/30/%E6%8A%8A%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E6%90%AC%E5%88%B0GPU-%E8%8B%B1%E4%BC%9F%E8%BE%BEDALI%E5%8A%A0%E9%80%9F%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/" title="把数据预处理搬到GPU-英伟达DALI加速数据预处理"><img src="https://ice.frostsky.com/2024/12/30/88a897ed803ad9cb7462f4320a31ac67.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="把数据预处理搬到GPU-英伟达DALI加速数据预处理"/></a><div class="content"><a class="title" href="/2024/12/30/%E6%8A%8A%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E6%90%AC%E5%88%B0GPU-%E8%8B%B1%E4%BC%9F%E8%BE%BEDALI%E5%8A%A0%E9%80%9F%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/" title="把数据预处理搬到GPU-英伟达DALI加速数据预处理">把数据预处理搬到GPU-英伟达DALI加速数据预处理</a><time datetime="2024-12-30T13:32:21.000Z" title="发表于 2024-12-30 21:32:21">2024-12-30</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>分类</span>
            <a class="card-more-btn" href="/categories/" title="查看更多">
    <i class="fas fa-angle-right"></i></a>
            </div>
            <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E5%91%93%E8%AF%AD%E7%B3%BB%E5%88%97%E8%BF%9E%E8%BD%BD/"><span class="card-category-list-name">呓语系列连载</span><span class="card-category-list-count">8</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF/"><span class="card-category-list-name">大模型炼丹术</span><span class="card-category-list-count">4</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"><span class="card-category-list-name">推荐系统</span><span class="card-category-list-count">18</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/"><span class="card-category-list-name">数据竞赛</span><span class="card-category-list-count">8</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"><span class="card-category-list-name">机器学习算法</span><span class="card-category-list-count">7</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><span class="card-category-list-name">深度学习笔记</span><span class="card-category-list-count">95</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E7%A5%9E%E5%A5%87%E7%9A%84Python/"><span class="card-category-list-name">神奇的Python</span><span class="card-category-list-count">4</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E8%B7%A8%E8%80%83%E5%B0%8F%E7%99%BD%E5%AD%A6%E5%88%B7%E9%A2%98/"><span class="card-category-list-name">跨考小白学刷题</span><span class="card-category-list-count">2</span></a></li>
            </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>标签</span></div><div class="card-tag-cloud"><a href="/tags/DL/" style="font-size: 1.5em; color: #99a9bf">DL</a> <a href="/tags/GAN/" style="font-size: 1.43em; color: #99a6b9">GAN</a> <a href="/tags/LLM/" style="font-size: 1.23em; color: #999ea6">LLM</a> <a href="/tags/ML/" style="font-size: 1.3em; color: #99a1ac">ML</a> <a href="/tags/Python/" style="font-size: 1.43em; color: #99a6b9">Python</a> <a href="/tags/RL/" style="font-size: 1.1em; color: #999">RL</a> <a href="/tags/Spark/" style="font-size: 1.1em; color: #999">Spark</a> <a href="/tags/Transformer/" style="font-size: 1.1em; color: #999">Transformer</a> <a href="/tags/%E5%91%93%E8%AF%AD/" style="font-size: 1.3em; color: #99a1ac">呓语</a> <a href="/tags/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/" style="font-size: 1.1em; color: #999">性能优化</a> <a href="/tags/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/" style="font-size: 1.1em; color: #999">扩散模型</a> <a href="/tags/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/" style="font-size: 1.37em; color: #99a4b2">推荐算法</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/" style="font-size: 1.17em; color: #999c9f">数据竞赛</a> <a href="/tags/%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86/" style="font-size: 1.1em; color: #999">模型推理</a> <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/" style="font-size: 1.1em; color: #999">计算机基础</a></div></div><div class="card-widget card-archives"><div class="item-headline"><i class="fas fa-archive"></i><span>归档</span><a class="card-more-btn" href="/archives/" title="查看更多">
    <i class="fas fa-angle-right"></i></a></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/03/"><span class="card-archive-list-date">三月 2025</span><span class="card-archive-list-count">2</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/02/"><span class="card-archive-list-date">二月 2025</span><span class="card-archive-list-count">2</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/12/"><span class="card-archive-list-date">十二月 2024</span><span class="card-archive-list-count">3</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/10/"><span class="card-archive-list-date">十月 2024</span><span class="card-archive-list-count">2</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/09/"><span class="card-archive-list-date">九月 2024</span><span class="card-archive-list-count">1</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/06/"><span class="card-archive-list-date">六月 2024</span><span class="card-archive-list-count">1</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/05/"><span class="card-archive-list-date">五月 2024</span><span class="card-archive-list-count">1</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/01/"><span class="card-archive-list-date">一月 2024</span><span class="card-archive-list-count">1</span></a></li></ul></div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>网站资讯</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">文章数目 :</div><div class="item-count">169</div></div><div class="webinfo-item"><div class="item-name">本站总字数 :</div><div class="item-count">359k</div></div><div class="webinfo-item"><div class="item-name">本站访客数 :</div><div class="item-count" id="busuanzi_value_site_uv"></div></div><div class="webinfo-item"><div class="item-name">本站总访问量 :</div><div class="item-count" id="busuanzi_value_site_pv"></div></div><div class="webinfo-item"><div class="item-name">最后更新时间 :</div><div class="item-count" id="last-push-date" data-lastPushDate="2025-03-10T12:41:01.993Z"></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/img/tag1.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By 雨落诗山山亦奇</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><div class="js-pjax"><script>function subtitleType () {
  if (true) { 
    window.typed = new Typed("#subtitle", {
      strings: ["期望始终为零，方差交给时间"],
      startDelay: 300,
      typeSpeed: 150,
      loop: true,
      backSpeed: 50
    })
  } else {
    document.getElementById("subtitle").innerHTML = '期望始终为零，方差交给时间'
  }
}

if (true) {
  if (typeof Typed === 'function') {
    subtitleType()
  } else {
    getScript('https://cdn.jsdelivr.net/npm/typed.js/lib/typed.min.js').then(subtitleType)
  }
} else {
  subtitleType()
}</script></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/fireworks.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>