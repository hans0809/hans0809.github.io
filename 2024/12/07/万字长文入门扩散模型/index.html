<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>万字长文入门扩散模型 | 南极Python</title><meta name="keywords" content="扩散模型"><meta name="author" content="雨落诗山山亦奇"><meta name="copyright" content="雨落诗山山亦奇"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="扩散模型问世至今，因其训练过程的稳定性和生成样本的多样性，受到了广泛的关注和应用，相应的开源社区贡献的工具链也趋向于更易用，HuggingFace的diffusers库便是其中之一。 diffusers提供了非常简洁和直观的API接口，能够让研究人员和开发者快速实现扩散模型的训练和推理。即便是对扩散模型不太熟悉的用户，也能通过少量的代码实现高效的图像生成任务。 本文基于diffusers，包含如下">
<meta property="og:type" content="article">
<meta property="og:title" content="万字长文入门扩散模型">
<meta property="og:url" content="http://yoursite.com/2024/12/07/%E4%B8%87%E5%AD%97%E9%95%BF%E6%96%87%E5%85%A5%E9%97%A8%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/index.html">
<meta property="og:site_name" content="南极Python">
<meta property="og:description" content="扩散模型问世至今，因其训练过程的稳定性和生成样本的多样性，受到了广泛的关注和应用，相应的开源社区贡献的工具链也趋向于更易用，HuggingFace的diffusers库便是其中之一。 diffusers提供了非常简洁和直观的API接口，能够让研究人员和开发者快速实现扩散模型的训练和推理。即便是对扩散模型不太熟悉的用户，也能通过少量的代码实现高效的图像生成任务。 本文基于diffusers，包含如下">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://ice.frostsky.com/2024/12/07/bf4bfcbe17c50e1fcbc8d26acfac57c8.png">
<meta property="article:published_time" content="2024-12-07T06:51:51.000Z">
<meta property="article:modified_time" content="2024-12-07T07:14:41.551Z">
<meta property="article:author" content="雨落诗山山亦奇">
<meta property="article:tag" content="扩散模型">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ice.frostsky.com/2024/12/07/bf4bfcbe17c50e1fcbc8d26acfac57c8.png"><link rel="shortcut icon" href="https://www.cdnjson.com/images/2021/11/27/_20210211193948.png"><link rel="canonical" href="http://yoursite.com/2024/12/07/%E4%B8%87%E5%AD%97%E9%95%BF%E6%96%87%E5%85%A5%E9%97%A8%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '万字长文入门扩散模型',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-12-07 15:14:41'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/StaticFile_HEXO@latest/butterfly/css/macblack.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://www.cdnjson.com/images/2021/11/27/_20210211193948.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">170</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div></div><hr/></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://ice.frostsky.com/2024/12/07/bf4bfcbe17c50e1fcbc8d26acfac57c8.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">南极Python</a></span><div id="menus"><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">万字长文入门扩散模型</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-12-07T06:51:51.000Z" title="发表于 2024-12-07 14:51:51">2024-12-07</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-12-07T07:14:41.551Z" title="更新于 2024-12-07 15:14:41">2024-12-07</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">深度学习笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">9.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>42分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="万字长文入门扩散模型"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>扩散模型问世至今，因其训练过程的稳定性和生成样本的多样性，受到了广泛的关注和应用，相应的开源社区贡献的工具链也趋向于更易用，HuggingFace的diffusers库便是其中之一。</p>
<p>diffusers提供了非常简洁和直观的API接口，能够让研究人员和开发者快速实现扩散模型的训练和推理。即便是对扩散模型不太熟悉的用户，也能通过少量的代码实现高效的图像生成任务。</p>
<p>本文基于diffusers，包含如下内容：</p>
<p>1.通过一个简易demo来直观感受使用diffusers中的快速生图的方法</p>
<p>2.使用smithsonian_butterflies_subset数据集，通过diffusers来搭建一个完整的图像生成小项目，包括数据集准备、模型训练、模型推理等步骤</p>
<p>3.介绍如何基于已有的预训练权重，通过微调和引导技术，来控制生成图片整体的细节走向，如颜色偏好，内容偏好等</p>
<p>4.介绍火出圈的StableDiffusion</p>
<p>5.介绍DDIM反转，用于控制图像的局部区域生成细节走向，该技术极大地提高了扩散模型的可玩性</p>
<h1 id="零-准备工具函数"><a href="#零-准备工具函数" class="headerlink" title="零. 准备工具函数"></a>零. 准备工具函数</h1><p>首先，导入常用的库，并编写后续将被重复使用的工具函数，用于图像可视化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_images</span>(<span class="params">x</span>):</span></span><br><span class="line">  x = x * <span class="number">0.5</span> + <span class="number">0.5</span><span class="comment"># 将（-1，1）区间映射回（0，1）区间</span></span><br><span class="line">  grid = torchvision.utils.make_grid(x)</span><br><span class="line">  grid_im = grid.detach().cpu().permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).clip(<span class="number">0</span>, <span class="number">1</span>) *<span class="number">255</span></span><br><span class="line">  grid_im =Image.fromarray(np.array(grid_im).astype(np.uint8))</span><br><span class="line">  <span class="keyword">return</span> grid_im</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_grid</span>(<span class="params">images, size=<span class="number">64</span></span>):</span></span><br><span class="line">  output_im = Image.new(<span class="string">&quot;RGB&quot;</span>, (size * <span class="built_in">len</span>(images), size))</span><br><span class="line">  <span class="keyword">for</span> i,im <span class="keyword">in</span> <span class="built_in">enumerate</span>(images):</span><br><span class="line">    output_im.paste(im.resize((size, size)), (i * size, <span class="number">0</span>))</span><br><span class="line">  <span class="keyword">return</span> output_im</span><br></pre></td></tr></table></figure>

<h1 id="一-文生图demo的快速实现"><a href="#一-文生图demo的快速实现" class="headerlink" title="一. 文生图demo的快速实现"></a>一. 文生图demo的快速实现</h1><p>本节以DreamBooth为例，使用diffusers快速实现文生图。</p>
<p>其中涉及到的具体模型会在后续展开介绍，这里暂时无需深究，只需动手体验一下文生图即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> diffusers <span class="keyword">import</span> StableDiffusionPipeline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">model_id = <span class="string">&quot;sd-dreambooth-library/mr-potato-head&quot;</span></span><br><span class="line">pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(<span class="string">&#x27;cuda&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>运行上述代码，就搭建好了一个文生图的管线（注：在diffusers中，每个模型的推理函数都被封装成一个pipline，本文称之为管线），接下来调用该管线，就可以生成图像了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">prompt = <span class="string">&quot;an abstract sky of universe bypicasso&quot;</span></span><br><span class="line">image = pipe(prompt, num_inference_steps=<span class="number">50</span>,guidance_scale=<span class="number">7.5</span>).images[<span class="number">0</span>]</span><br><span class="line">image</span><br></pre></td></tr></table></figure>

<p><img src="1.png"></p>
<h1 id="二-使用diffusers动手做一个完整的图像生成项目"><a href="#二-使用diffusers动手做一个完整的图像生成项目" class="headerlink" title="二.使用diffusers动手做一个完整的图像生成项目"></a>二.使用diffusers动手做一个完整的图像生成项目</h1><p>本节要做的项目在diffusers中已有可直接使用的管线，这里先使用已有管线进行运行测试，然后来学习如何一步一步从头实现这个管线。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> diffusers <span class="keyword">import</span> DDPMPipeline</span><br><span class="line">butterfly_pipeline =DDPMPipeline.from_pretrained(<span class="string">&quot;johnowhitaker/ddpm-butterflies-32px&quot;</span>).to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">images = butterfly_pipeline(batch_size=<span class="number">8</span>).images</span><br><span class="line">make_grid(images)</span><br></pre></td></tr></table></figure>

<p><img src="2.png"></p>
<h2 id="2-1-数据集准备"><a href="#2-1-数据集准备" class="headerlink" title="2.1 数据集准备"></a>2.1 数据集准备</h2><p>首先下载训练数据集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line">!pip install -q datasets</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line">dataset = load_dataset(<span class="string">&quot;huggan/smithsonian_butterflies_subset&quot;</span>,split=<span class="string">&quot;train&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义数据集超参数</span></span><br><span class="line">image_size = <span class="number">32</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义数据增强过程</span></span><br><span class="line">preprocess = transforms.Compose(</span><br><span class="line">        [</span><br><span class="line">                    transforms.Resize((image_size, image_size)),  <span class="comment"># 调整大小</span></span><br><span class="line">                    transforms.RandomHorizontalFlip(),            <span class="comment"># 随机翻转</span></span><br><span class="line">                    transforms.ToTensor(),              <span class="comment"># 将张量映射到(0,1)区间</span></span><br><span class="line">                    transforms.Normalize([<span class="number">0.5</span>], [<span class="number">0.5</span>]), <span class="comment"># 映射到(-1, 1)区间</span></span><br><span class="line">                    ]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transform</span>(<span class="params">examples</span>):</span></span><br><span class="line">  images = [preprocess(image.convert(<span class="string">&quot;RGB&quot;</span>)) <span class="keyword">for</span> image <span class="keyword">in</span> examples[<span class="string">&quot;image&quot;</span>]]</span><br><span class="line">  <span class="keyword">return</span> &#123;<span class="string">&quot;images&quot;</span>: images&#125;</span><br><span class="line"></span><br><span class="line">dataset.set_transform(transform)</span><br><span class="line"></span><br><span class="line">train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>查看一下数据集中的部分图片：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">xb = <span class="built_in">next</span>(<span class="built_in">iter</span>(train_dataloader))[<span class="string">&quot;images&quot;</span>].to(<span class="string">&#x27;cuda&#x27;</span>)[:<span class="number">8</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;X shape:&quot;</span>, xb.shape)</span><br><span class="line">show_images(xb).resize((<span class="number">8</span> * <span class="number">64</span>, <span class="number">64</span>), resample=Image.NEAREST)</span><br></pre></td></tr></table></figure>

<p><img src="3.png"></p>
<h2 id="2-2-调度器"><a href="#2-2-调度器" class="headerlink" title="2.2 调度器"></a>2.2 调度器</h2><p>这里使用经典的DDPM调度器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> diffusers <span class="keyword">import</span> DDPMScheduler</span><br><span class="line"></span><br><span class="line">noise_scheduler = DDPMScheduler(num_train_timesteps=<span class="number">1000</span>, beta_start=<span class="number">0.001</span>, beta_end=<span class="number">0.004</span>)</span><br></pre></td></tr></table></figure>

<p>定义好调度器之后，来尝试使用该调度器对图片进行加噪：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">noise = torch.randn_like(xb)</span><br><span class="line">timesteps = torch.linspace(<span class="number">0</span>, <span class="number">999</span>, <span class="number">8</span>).long().to(<span class="string">&#x27;cuda&#x27;</span>)<span class="comment"># xb的batchsize是8，因此也需要8个对应的timesteps（shape：torch.Size([8])）</span></span><br><span class="line">noisy_xb = noise_scheduler.add_noise(xb, noise, timesteps)<span class="comment"># 加噪</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Noisy X shape&quot;</span>, noisy_xb.shape)</span><br><span class="line">show_images(noisy_xb).resize((<span class="number">8</span> * <span class="number">64</span>, <span class="number">64</span>),resample=Image.NEAREST)</span><br></pre></td></tr></table></figure>

<p><img src="4.png"><br>随着迭代次数的增加，清晰的图像被逐步加噪。</p>
<h2 id="2-3-定义扩散模型"><a href="#2-3-定义扩散模型" class="headerlink" title="2.3 定义扩散模型"></a>2.3 定义扩散模型</h2><p>用于预测噪声的模型是UNet，定义如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> diffusers <span class="keyword">import</span> UNet2DModel</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型</span></span><br><span class="line">model = UNet2DModel(</span><br><span class="line">        sample_size=image_size,   <span class="comment"># 目标图像分辨率</span></span><br><span class="line">        in_channels=<span class="number">3</span>,            <span class="comment"># 输入通道数，对于RGB图像来说，通道数为3</span></span><br><span class="line">        out_channels=<span class="number">3</span>,           <span class="comment"># 输出通道数</span></span><br><span class="line">        layers_per_block=<span class="number">2</span>,       <span class="comment"># 每个UNet块使用的ResNet层数</span></span><br><span class="line">        block_out_channels=(<span class="number">64</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="number">256</span>), <span class="comment"># 更多的通道→更多的参数</span></span><br><span class="line">        down_block_types=(</span><br><span class="line">            <span class="string">&quot;DownBlock2D&quot;</span>,        <span class="comment"># 一个常规的ResNet下采样模块</span></span><br><span class="line">            <span class="string">&quot;DownBlock2D&quot;</span>,</span><br><span class="line">            <span class="string">&quot;AttnDownBlock2D&quot;</span>,<span class="comment"># 一个带有空间自注意力的ResNet下采样模块</span></span><br><span class="line">            <span class="string">&quot;AttnDownBlock2D&quot;</span>,</span><br><span class="line">            ),</span><br><span class="line">        up_block_types=(</span><br><span class="line">            <span class="string">&quot;AttnUpBlock2D&quot;</span>,</span><br><span class="line">            <span class="string">&quot;AttnUpBlock2D&quot;</span>,      <span class="comment"># 一个带有空间自注意力的ResNet上采样模块</span></span><br><span class="line">            <span class="string">&quot;UpBlock2D&quot;</span>,</span><br><span class="line">            <span class="string">&quot;UpBlock2D&quot;</span>,          <span class="comment"># 一个常规的ResNet上采样模块</span></span><br><span class="line">            ),</span><br><span class="line">        )</span><br><span class="line">model.to(<span class="string">&#x27;cuda&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>测试一下模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">  model_prediction = model(noisy_xb, timesteps).sample<span class="comment"># noisy_xb：[8,3,32,32], timesteps: [8,1]</span></span><br><span class="line">model_prediction.shape<span class="comment"># torch.Size([8, 3, 32, 32])</span></span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([8, 3, 32, 32])</span><br></pre></td></tr></table></figure>

<h2 id="2-4-创建扩散模型训练循环"><a href="#2-4-创建扩散模型训练循环" class="headerlink" title="2.4 创建扩散模型训练循环"></a>2.4 创建扩散模型训练循环</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设定噪声调度器</span></span><br><span class="line">noise_scheduler = DDPMScheduler(</span><br><span class="line">    num_train_timesteps=<span class="number">1000</span>,</span><br><span class="line">    beta_schedule=<span class="string">&quot;squaredcos_cap_v2&quot;</span>)</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.AdamW(model.parameters(), lr=<span class="number">4e-4</span>)</span><br><span class="line"></span><br><span class="line">losses = []</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">30</span>):</span><br><span class="line">  <span class="keyword">for</span> step, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_dataloader):</span><br><span class="line">    clean_images = batch[<span class="string">&quot;images&quot;</span>].to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">    <span class="comment"># 为图片添加采样噪声</span></span><br><span class="line">    noise =torch.randn(clean_images.shape).to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">    bs = clean_images.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 为每张图片随机采样一个时间步</span></span><br><span class="line">    timesteps = torch.randint(<span class="number">0</span>, noise_scheduler.num_train_timesteps, (bs,),device=<span class="string">&#x27;cuda&#x27;</span>).long()</span><br><span class="line">    <span class="comment"># 根据每个时间步的噪声幅度，向清晰的图片中添加噪声</span></span><br><span class="line">    noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)</span><br><span class="line">    <span class="comment"># 获得模型的预测结果</span></span><br><span class="line">    noise_pred = model(noisy_images, timesteps, return_dict=<span class="literal">False</span>)[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 计算损失</span></span><br><span class="line">    loss = F.mse_loss(noise_pred, noise)</span><br><span class="line">    loss.backward(loss)</span><br><span class="line">    losses.append(loss.item())</span><br><span class="line">    <span class="comment"># 迭代模型参数</span></span><br><span class="line">    optimizer.step()</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">  <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">    loss_last_epoch = <span class="built_in">sum</span>(losses[-<span class="built_in">len</span>(train_dataloader) :])/<span class="built_in">len</span>(train_dataloader)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Epoch:<span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>, loss: <span class="subst">&#123;loss_last_epoch&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>训练日志：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Epoch:5, loss: 0.15131539199501276</span><br><span class="line">Epoch:10, loss: 0.11000193934887648</span><br><span class="line">Epoch:15, loss: 0.09896771190688014</span><br><span class="line">Epoch:20, loss: 0.0816401862539351</span><br><span class="line">Epoch:25, loss: 0.07495242427103221</span><br><span class="line">Epoch:30, loss: 0.07060358510352671</span><br></pre></td></tr></table></figure>

<p>检查一下训练损失：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">fig, axs = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize=(<span class="number">12</span>, <span class="number">4</span>))</span><br><span class="line">axs[<span class="number">0</span>].plot(losses)</span><br><span class="line">axs[<span class="number">1</span>].plot(np.log(losses))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="5.png"></p>
<p>这样，一个图像生成模型就训练完成了。</p>
<h2 id="2-5-图像的生成"><a href="#2-5-图像的生成" class="headerlink" title="2.5 图像的生成"></a>2.5 图像的生成</h2><p>在完成模型训练后，需要本小节来实现模型的推理，即图像生成。</p>
<h3 id="方法1：建立一个diffusers管线"><a href="#方法1：建立一个diffusers管线" class="headerlink" title="方法1：建立一个diffusers管线"></a>方法1：建立一个diffusers管线</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> diffusers <span class="keyword">import</span> DDPMPipeline</span><br><span class="line"></span><br><span class="line">image_pipe = DDPMPipeline(unet=model,scheduler=noise_scheduler)<span class="comment"># 将训练好的model传入管线</span></span><br><span class="line">pipeline_output = image_pipe()<span class="comment"># 开始生成</span></span><br><span class="line">pipeline_output.images[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<p><img src="6.png"></p>
<h3 id="方法2：自定义一个采样循环"><a href="#方法2：自定义一个采样循环" class="headerlink" title="方法2：自定义一个采样循环"></a>方法2：自定义一个采样循环</h3><p>首先来随机初始化8张图片：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sample = torch.randn(<span class="number">8</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>).to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">show_images(sample)</span><br></pre></td></tr></table></figure>

<p><img src="7.png"></p>
<p>可以看到，这些图片上全是噪声。</p>
<p>接下来，定义采样循环来迭代的消除噪声，就完成了图像的生成：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设 noise_scheduler.timesteps 是一个包含时间步的序列</span></span><br><span class="line"><span class="keyword">for</span> i, t <span class="keyword">in</span> tqdm(<span class="built_in">enumerate</span>(noise_scheduler.timesteps), total=<span class="built_in">len</span>(noise_scheduler.timesteps), desc=<span class="string">&quot;Diffusion Steps&quot;</span>):<span class="comment"># DDPM 迭代1000步</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        residual = model(sample, t).sample  <span class="comment"># 获取模型预测的噪声</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据预测结果更新图像</span></span><br><span class="line">    sample = noise_scheduler.step(residual, t, sample).prev_sample</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看生成的图片</span></span><br><span class="line">show_images(sample)</span><br></pre></td></tr></table></figure>

<p><img src="8.png"></p>
<p>至此，图像生成项目已全部完成。</p>
<h1 id="三-微调和引导"><a href="#三-微调和引导" class="headerlink" title="三. 微调和引导"></a>三. 微调和引导</h1><p>现在，你已经学会了如何借助diffusers库，在自定义数据集上训练图像生成模型并完成推理。</p>
<p>但是，生成的图像是和训练集近似分布的，如何引导模型的生成结果向定制化的需求靠近呢？本节将介绍相关的实现技术。</p>
<p>不过在开始之前，还需要优化一个问题。如果仔细观察就会发现，上面所使用的DDPM调度器，在推理时需要迭代1000步，这是一个相当慢的循环操作。</p>
<p>解决方法就是将调度器由DDPM换成DDIM。相较于DDPM，DDIM可以用单步迭代来模拟DDPM中的多步迭代，因此极大地降低了总的迭代次数。</p>
<h2 id="3-1-环境准备"><a href="#3-1-环境准备" class="headerlink" title="3.1 环境准备"></a>3.1 环境准备</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">!pip install -qq diffusers datasets accelerate wandb <span class="built_in">open</span>-clip-torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"><span class="keyword">from</span> diffusers <span class="keyword">import</span> DDIMScheduler, DDPMPipeline</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line">device=<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span></span><br></pre></td></tr></table></figure>

<h2 id="3-2-使用DDIM-更快的调度器"><a href="#3-2-使用DDIM-更快的调度器" class="headerlink" title="3.2 使用DDIM-更快的调度器"></a>3.2 使用DDIM-更快的调度器</h2><p>首先定义一个DDPM的管线：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">image_pipe = DDPMPipeline.from_pretrained(<span class="string">&quot;google/ddpm-celebahq-256&quot;</span>)</span><br><span class="line">image_pipe.to(device)</span><br><span class="line"></span><br><span class="line">images = image_pipe().images</span><br><span class="line">images[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<p><img src="9.png"></p>
<p>然后，创建一个更快的DDIM调度器并设置推理迭代次数为40：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scheduler = DDIMScheduler.from_pretrained(<span class="string">&quot;google/ddpm-celebahq-256&quot;</span>)</span><br><span class="line">scheduler.set_timesteps(num_inference_steps=<span class="number">40</span>)</span><br></pre></td></tr></table></figure>

<p>现在，将上面的DDPMPipeline的调度器由DDPM换成DDIM，使用和2.5小节中类似的自定义采样循环来实现图像生成：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从随机噪声开始</span></span><br><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">3</span>, <span class="number">256</span>, <span class="number">256</span>).to(device)</span><br><span class="line"><span class="keyword">for</span> i, t <span class="keyword">in</span> tqdm(<span class="built_in">enumerate</span>(scheduler.timesteps)):</span><br><span class="line">  <span class="comment"># 准备模型输入：给“带噪”图像加上时间步信息</span></span><br><span class="line">  model_input = scheduler.scale_model_input(x, t)<span class="comment"># DDIM 调度器会对噪声图像 x 进行适当缩放，使其符合扩散模型的输入规范。这种缩放通常基于时间步 t 的权重参数</span></span><br><span class="line">  <span class="comment"># 预测噪声</span></span><br><span class="line">  <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    noise_pred = image_pipe.unet(model_input, t)[<span class="string">&quot;sample&quot;</span>]</span><br><span class="line">  <span class="comment"># 使用调度器计算更新后的样本应该是什么样子</span></span><br><span class="line">  scheduler_output = scheduler.step(noise_pred, t, x)</span><br><span class="line">  <span class="comment"># 更新输入图像</span></span><br><span class="line">  x = scheduler_output.prev_sample</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 时不时看一下输入图像和预测的“去噪”图像</span></span><br><span class="line">  <span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">0</span> <span class="keyword">or</span> i == <span class="built_in">len</span>(scheduler.timesteps) - <span class="number">1</span>:</span><br><span class="line">    fig, axs = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize=(<span class="number">12</span>, <span class="number">5</span>))</span><br><span class="line">    grid = torchvision.utils.make_grid(x,nrow=<span class="number">4</span>).permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)</span><br><span class="line">    axs[<span class="number">0</span>].imshow(grid.cpu().clip(-<span class="number">1</span>, <span class="number">1</span>) * <span class="number">0.5</span> + <span class="number">0.5</span>)</span><br><span class="line">    axs[<span class="number">0</span>].set_title(<span class="string">f&quot;Current x (step <span class="subst">&#123;i&#125;</span>)&quot;</span>)</span><br><span class="line">    pred_x0 = (</span><br><span class="line">        scheduler_output.pred_original_sample</span><br><span class="line">        )</span><br><span class="line">    grid = torchvision.utils.make_grid(pred_x0, nrow=<span class="number">4</span>).permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)</span><br><span class="line">    axs[<span class="number">1</span>].imshow(grid.cpu().clip(-<span class="number">1</span>, <span class="number">1</span>) * <span class="number">0.5</span> + <span class="number">0.5</span>)</span><br><span class="line">    axs[<span class="number">1</span>].set_title(<span class="string">f&quot;Predicted denoised images (step<span class="subst">&#123;i&#125;</span>)&quot;</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="10.png"><br><img src="11.png"><br><img src="12.png"><br><img src="13.png"><br><img src="14.png"></p>
<p>现在，使用基于DDIM执行图像生成操作，可以看到，只需40次迭代就可以很快得到生成结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">image_pipe.scheduler = scheduler</span><br><span class="line">images = image_pipe(num_inference_steps=<span class="number">40</span>).images</span><br><span class="line">images[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<p><img src="15.png"></p>
<h2 id="3-3-微调扩散模型"><a href="#3-3-微调扩散模型" class="headerlink" title="3.3 微调扩散模型"></a>3.3 微调扩散模型</h2><p>首先准备数据集，仍使用之前的蝴蝶数据集。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">dataset_name =<span class="string">&quot;huggan/smithsonian_butterflies_subset&quot;</span></span><br><span class="line">dataset = load_dataset(dataset_name, split=<span class="string">&quot;train&quot;</span>)</span><br><span class="line">image_size =<span class="number">256</span></span><br><span class="line">batch_size =<span class="number">4</span></span><br><span class="line"></span><br><span class="line">preprocess = transforms.Compose(</span><br><span class="line">    [</span><br><span class="line">        transforms.Resize((image_size, image_size)),</span><br><span class="line">        transforms.RandomHorizontalFlip(),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize([<span class="number">0.5</span>], [<span class="number">0.5</span>]),</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transform</span>(<span class="params">examples</span>):</span></span><br><span class="line">  images = [preprocess(image.convert(<span class="string">&quot;RGB&quot;</span>)) <span class="keyword">for</span> image <span class="keyword">in</span> examples[<span class="string">&quot;image&quot;</span>]]</span><br><span class="line">  <span class="keyword">return</span> &#123;<span class="string">&quot;images&quot;</span>: images&#125;</span><br><span class="line"></span><br><span class="line">dataset.set_transform(transform)</span><br><span class="line">train_dataloader = torch.utils.data.DataLoader(</span><br><span class="line">    dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>查看数据集示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Previewing batch:&quot;</span>)</span><br><span class="line">batch =<span class="built_in">next</span>(<span class="built_in">iter</span>(train_dataloader))</span><br><span class="line">grid = torchvision.utils.make_grid(batch[<span class="string">&quot;images&quot;</span>], nrow=<span class="number">4</span>)</span><br><span class="line">plt.imshow(grid.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).cpu().clip(-<span class="number">1</span>, <span class="number">1</span>) *<span class="number">0.5</span>+<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure>

<p><img src="16.png"></p>
<p>现在，基于上面的celebahq人脸数据集上训练的权重，在蝴蝶数据集上进行微调：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = <span class="number">2</span></span><br><span class="line">lr = <span class="number">1e-5</span></span><br><span class="line">grad_accumulation_steps = <span class="number">2</span></span><br><span class="line">optimizer = torch.optim.AdamW(image_pipe.unet.parameters(),lr=lr)</span><br><span class="line">losses = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">  <span class="keyword">for</span> step, batch <span class="keyword">in</span> tqdm(<span class="built_in">enumerate</span>(train_dataloader),total=<span class="built_in">len</span>(train_dataloader)):</span><br><span class="line">    clean_images = batch[<span class="string">&quot;images&quot;</span>].to(device)</span><br><span class="line">    <span class="comment"># 随机生成一个噪声，稍后加到图像上</span></span><br><span class="line">    noise =torch.randn(clean_images.shape).to(clean_images.device)</span><br><span class="line">    bs = clean_images.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 随机选取一个时间步</span></span><br><span class="line">    timesteps = torch.randint(<span class="number">0</span>,image_pipe.scheduler.num_train_timesteps,(bs,),device=clean_images.device,).long()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据选中的时间步和确定的幅值，在干净图像上添加噪声</span></span><br><span class="line">    <span class="comment"># 此处为前向扩散过程</span></span><br><span class="line">    noisy_images =image_pipe.scheduler.add_noise(clean_images,noise,timesteps)</span><br><span class="line">    <span class="comment"># 使用“带噪”图像进行网络预测</span></span><br><span class="line">    noise_pred = image_pipe.unet(noisy_images, timesteps, return_dict=<span class="literal">False</span>)[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 对真正的噪声和预测的结果进行比较，注意这里是预测噪声</span></span><br><span class="line">    loss = F.mse_loss(noise_pred, noise)</span><br><span class="line">    <span class="comment"># 保存损失值</span></span><br><span class="line">    losses.append(loss.item())</span><br><span class="line">    <span class="comment"># 根据损失值更新梯度</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment"># 进行梯度累积，在累积到一定步数后更新模型参数</span></span><br><span class="line">    <span class="keyword">if</span> (step + <span class="number">1</span>) % grad_accumulation_steps == <span class="number">0</span>:</span><br><span class="line">      optimizer.step()</span><br><span class="line">      optimizer.zero_grad()</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch&#125;</span> average loss: <span class="subst">&#123;<span class="built_in">sum</span>(losses[-<span class="built_in">len</span>(train_dataloader):])/<span class="built_in">len</span>(train_dataloader)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>模型训练结束，执行推理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用在蝴蝶数据集上基于人脸生成模型的权重微调得到的模型进行推理</span></span><br><span class="line">x = torch.randn(<span class="number">8</span>, <span class="number">3</span>, <span class="number">256</span>, <span class="number">256</span>).to(device)</span><br><span class="line"><span class="keyword">for</span> i, t <span class="keyword">in</span> tqdm(<span class="built_in">enumerate</span>(image_pipe.scheduler.timesteps)):</span><br><span class="line">  model_input = scheduler.scale_model_input(x, t)</span><br><span class="line">  <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    noise_pred = image_pipe.unet(model_input, t)[<span class="string">&quot;sample&quot;</span>]</span><br><span class="line">    x = image_pipe.scheduler.step(noise_pred, t, x).prev_sample</span><br><span class="line">grid = torchvision.utils.make_grid(x, nrow=<span class="number">4</span>)</span><br><span class="line">plt.imshow(grid.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).cpu().clip(-<span class="number">1</span>, <span class="number">1</span>) * <span class="number">0.5</span> + <span class="number">0.5</span>)</span><br></pre></td></tr></table></figure>

<p><img src="17.png"><br>可以看到，生成结果已经由人脸趋向于蝴蝶的外观了。</p>
<h2 id="3-4-扩散模型之引导"><a href="#3-4-扩散模型之引导" class="headerlink" title="3.4 扩散模型之引导"></a>3.4 扩散模型之引导</h2><p>引导，指的是在模型训练完成后，在执行模型推理时，使用一些技术手段来使得生成结果的内容更偏向于自定义的风格。</p>
<p>本届介绍两种引导手段：</p>
<ul>
<li>颜色引导</li>
<li>CLIP引导</li>
</ul>
<p>在开始之前，首先来准备一个预训练好的管线，并将调度器更换成速度更快的DDIM：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pipeline_name =<span class="string">&quot;johnowhitaker/sd-class-wikiart-from-bedrooms&quot;</span></span><br><span class="line">image_pipe =DDPMPipeline.from_pretrained(pipeline_name).to(device)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用DDIM调度器，仅用40步生成一些图片</span></span><br><span class="line">scheduler = DDIMScheduler.from_pretrained(pipeline_name)</span><br><span class="line">scheduler.set_timesteps(num_inference_steps=<span class="number">40</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将随机噪声作为出发点</span></span><br><span class="line">x = torch.randn(<span class="number">8</span>, <span class="number">3</span>, <span class="number">256</span>, <span class="number">256</span>).to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用一个最简单的采样循环</span></span><br><span class="line"><span class="keyword">for</span> i, t <span class="keyword">in</span> tqdm(<span class="built_in">enumerate</span>(scheduler.timesteps)):</span><br><span class="line">  model_input = scheduler.scale_model_input(x, t)</span><br><span class="line">  <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    noise_pred = image_pipe.unet(model_input, t)[<span class="string">&quot;sample&quot;</span>]</span><br><span class="line">    x = scheduler.step(noise_pred, t, x).prev_sample</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看生成结果</span></span><br><span class="line">grid = torchvision.utils.make_grid(x, nrow=<span class="number">4</span>)</span><br><span class="line">plt.imshow(grid.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).cpu().clip(-<span class="number">1</span>, <span class="number">1</span>) *<span class="number">0.5</span>+<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure>

<p><img src="18.png"></p>
<h3 id="3-4-1-颜色引导"><a href="#3-4-1-颜色引导" class="headerlink" title="3.4.1 颜色引导"></a>3.4.1 颜色引导</h3><p>颜色引导（Color Guidance） 是一种在扩散模型中生成特定颜色风格或目标图像时，增加对颜色约束的方法。这种方法可以确保生成的图像在颜色空间上满足特定的要求，比如突出某种颜色、控制亮度对比，或者匹配参考图像的配色。</p>
<p>首先，定义颜色损失函数，并设置引导的强度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">color_loss</span>(<span class="params">images, target_color=(<span class="params"><span class="number">0.9</span>, <span class="number">0.9</span>, <span class="number">0.9</span></span>)</span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;给定一个RGB值，返回一个损失值，用于衡量图片的像素值与目标颜色相差多少&quot;&quot;&quot;</span></span><br><span class="line">  target = (</span><br><span class="line">      torch.tensor(target_color).to(images.device) * <span class="number">2</span> - <span class="number">1</span></span><br><span class="line">      )</span><br><span class="line">  <span class="comment"># 首先对target_color进行归一化，使它的取值区间为(-1, 1)</span></span><br><span class="line">  target = target[<span class="literal">None</span>, :, <span class="literal">None</span>, <span class="literal">None</span>]</span><br><span class="line">  <span class="comment"># 将所生成目标张量的形状改为(b, c, h, w)，以适配输入图像images的张量形状</span></span><br><span class="line">  error = torch.<span class="built_in">abs</span>(</span><br><span class="line">      images - target</span><br><span class="line">      ).mean()  <span class="comment"># 计算图片的像素值以及目标颜色的均方误差</span></span><br><span class="line">  <span class="keyword">return</span> error</span><br><span class="line"></span><br><span class="line"><span class="comment"># guidance_loss_scale用于决定引导的强度有多大</span></span><br><span class="line">guidance_loss_scale = <span class="number">40</span><span class="comment"># 可设定为5~100的任意数字</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">8</span>, <span class="number">3</span>, <span class="number">256</span>, <span class="number">256</span>).to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法1：</span></span><br><span class="line"><span class="keyword">for</span> i, t <span class="keyword">in</span> tqdm(<span class="built_in">enumerate</span>(scheduler.timesteps)):</span><br><span class="line">  <span class="comment"># 准备模型输入with DDIM scheduler</span></span><br><span class="line">  model_input = scheduler.scale_model_input(x, t)</span><br><span class="line">  <span class="comment"># 预测噪声</span></span><br><span class="line">  <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    noise_pred = image_pipe.unet(model_input, t)[<span class="string">&quot;sample&quot;</span>]</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 设置x.requires_grad为True</span></span><br><span class="line">  x = x.detach().requires_grad_()</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 从当前时间步得到“去噪”后的图像</span></span><br><span class="line">  x0 = scheduler.step(noise_pred, t, x).pred_original_sample</span><br><span class="line"></span><br><span class="line">  <span class="comment"># pred_original_sample：当你希望直接获取模型在当前步骤中预测的最终图像时使用</span></span><br><span class="line">  <span class="comment"># prev_sample：当你需要逐步生成图像，通过递归步骤从噪声到最终图像的过程时使用</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 计算损失值</span></span><br><span class="line">  loss = color_loss(x0) * guidance_loss_scale</span><br><span class="line">  <span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">    <span class="built_in">print</span>(i, <span class="string">&quot;loss:&quot;</span>, loss.item())</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 获取梯度</span></span><br><span class="line">  cond_grad = -torch.autograd.grad(loss, x)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 使用梯度更新x</span></span><br><span class="line">  x = x.detach() + cond_grad<span class="comment"># 为了降低颜色损失，这里执行梯度下降，使得预测的&quot;去噪&quot;后图片x的颜色更接近于目标颜色</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 使用调度器更新x</span></span><br><span class="line">  x = scheduler.step(noise_pred, t, x).prev_sample</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">0 loss: 29.88568115234375</span><br><span class="line">10 loss: 3.800239324569702</span><br><span class="line">20 loss: 3.4818835258483887</span><br><span class="line">30 loss: 3.5615406036376953</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看结果</span></span><br><span class="line">grid = torchvision.utils.make_grid(x, nrow=<span class="number">4</span>)</span><br><span class="line">im = grid.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).cpu().clip(-<span class="number">1</span>, <span class="number">1</span>) * <span class="number">0.5</span> + <span class="number">0.5</span></span><br><span class="line">Image.fromarray(np.array(im * <span class="number">255</span>).astype(np.uint8))</span><br></pre></td></tr></table></figure>

<p><img src="19.png"></p>
<p>我们预先设置的颜色是(0.9, 0.9, 0.9)，这是一种接近白色的颜色，可以看到，在推理过程中加入颜色损失来引导生成过程后，生成的图像颜色整体上已经偏向我们设置的目标颜色了。</p>
<h3 id="3-4-2-CLIP引导"><a href="#3-4-2-CLIP引导" class="headerlink" title="3.4.2 CLIP引导"></a>3.4.2 CLIP引导</h3><p>CLIP引导的核心思想是通过CLIP模型的文本和图像相似性来优化生成的图像，使其与文本描述的语义更加一致。通常情况下，CLIP模型会计算文本和图像之间的相似度，生成的图像通过梯度下降优化，使得图像与文本描述更加匹配。</p>
<p>这里使用open_clip封装好的CLIP模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> open_clip</span><br><span class="line">clip_model, _, preprocess =open_clip.create_model_and_transforms(<span class="string">&quot;ViT-B-32&quot;</span>, pretrained=<span class="string">&quot;openai&quot;</span>)</span><br><span class="line">clip_model.to(device)</span><br></pre></td></tr></table></figure>

<p>和颜色引导一样，这里需要定义clip引导所需的损失函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 图像变换：用于修改图像尺寸和增广数据，同时归一化数据，以使数据能够适配CLIP模型</span></span><br><span class="line">tfms = torchvision.transforms.Compose(</span><br><span class="line">      [</span><br><span class="line">        torchvision.transforms.RandomResizedCrop(<span class="number">224</span>),<span class="comment"># 随机裁剪</span></span><br><span class="line"></span><br><span class="line">        torchvision.transforms.RandomAffine(<span class="number">5</span>),       <span class="comment"># 随机扭曲图片</span></span><br><span class="line">        torchvision.transforms.RandomHorizontalFlip(),<span class="comment"># 随机左右镜像，</span></span><br><span class="line"></span><br><span class="line">        torchvision.transforms.Normalize(</span><br><span class="line">            mean=(<span class="number">0.48145466</span>, <span class="number">0.4578275</span>, <span class="number">0.40821073</span>),</span><br><span class="line">            std=(<span class="number">0.26862954</span>, <span class="number">0.26130258</span>, <span class="number">0.27577711</span>),</span><br><span class="line">            ),</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个损失函数，用于获取图片的特征，然后与提示文字的特征进行对比</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clip_loss</span>(<span class="params">image, text_features</span>):</span></span><br><span class="line">  image_features = clip_model.encode_image(</span><br><span class="line">      tfms(image)</span><br><span class="line">      )  <span class="comment"># 注意施加上面定义好的变换</span></span><br><span class="line">  input_normed = torch.nn.functional.normalize(image_features.unsqueeze(<span class="number">1</span>), dim=<span class="number">2</span>)</span><br><span class="line">  embed_normed = torch.nn.functional.normalize(text_features.unsqueeze(<span class="number">0</span>), dim=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">  dists = (</span><br><span class="line">      input_normed.sub(embed_normed).norm(dim=<span class="number">2</span>).div(<span class="number">2</span>).</span><br><span class="line">      arcsin().<span class="built_in">pow</span>(<span class="number">2</span>).mul(<span class="number">2</span>)</span><br><span class="line">      )  <span class="comment"># 使用Squared Great Circle Distance计算距离</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> dists.mean()</span><br></pre></td></tr></table></figure>

<p>定义超参数，并提取提示词的特征向量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">prompt =<span class="string">&quot;A vast, tranquil ocean under a golden sunset sky. The water is crystal clear, reflecting the warm hues of the sun with gentle ripples. The horizon stretches far, meeting the sky in a soft gradient of oranges, purples, and pinks. A few distant sailboats glide across the water, while seagulls soar above. The scene should evoke a sense of peacefulness and infinity, with delicate waves and a calm, serene atmosphere.&quot;</span></span><br><span class="line"></span><br><span class="line">guidance_scale = <span class="number">8</span></span><br><span class="line">n_cuts = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里使用稍微多一些的步数</span></span><br><span class="line">scheduler.set_timesteps(<span class="number">50</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用CLIP从提示文字中提取特征</span></span><br><span class="line">text = open_clip.tokenize([prompt]).to(device)</span><br><span class="line"><span class="keyword">with</span> torch.no_grad(), torch.cuda.amp.autocast():</span><br><span class="line">  text_features = clip_model.encode_text(text)</span><br></pre></td></tr></table></figure>

<p>开始生成：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">3</span>, <span class="number">256</span>, <span class="number">256</span>).to(device)<span class="comment"># 从随机噪声开始</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, t <span class="keyword">in</span> tqdm(<span class="built_in">enumerate</span>(scheduler.timesteps)):</span><br><span class="line">  model_input = scheduler.scale_model_input(x, t)</span><br><span class="line">  <span class="comment"># 预测噪声</span></span><br><span class="line">  <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    noise_pred = image_pipe.unet(model_input, t)[<span class="string">&quot;sample&quot;</span>]</span><br><span class="line"></span><br><span class="line">  cond_grad = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> cut <span class="keyword">in</span> <span class="built_in">range</span>(n_cuts):</span><br><span class="line">    <span class="comment"># 设置输入图像的requires_grad属性为True</span></span><br><span class="line">    x = x.detach().requires_grad_()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获得“去噪”后的图像</span></span><br><span class="line">    x0 = scheduler.step(noise_pred, t,x).pred_original_sample</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算损失值</span></span><br><span class="line">    loss = clip_loss(x0, text_features) * guidance_scale</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取梯度并使用n_cuts进行平均</span></span><br><span class="line">    cond_grad -= torch.autograd.grad(loss, x)[<span class="number">0</span>] / n_cuts</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> i % <span class="number">25</span> == <span class="number">0</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Step:&quot;</span>, i, <span class="string">&quot;, Guidance loss:&quot;</span>, loss.item())</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 根据这个梯度更新x</span></span><br><span class="line">  alpha_bar = scheduler.alphas_cumprod[i]</span><br><span class="line">  x = (</span><br><span class="line">      x.detach() + cond_grad * alpha_bar.sqrt()</span><br><span class="line">      )  <span class="comment"># 注意这里的缩放因子</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 使用调度器更新x</span></span><br><span class="line">  x = scheduler.step(noise_pred, t, x).prev_sample</span><br></pre></td></tr></table></figure>

<p>其中的n_cuts 是一个超参数，在每次计算损失后，梯度会被累加，并在每次计算中平均。这样可以减少梯度的波动，获得更稳定的优化过程。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">grid = torchvision.utils.make_grid(x.detach(), nrow=<span class="number">4</span>)</span><br><span class="line">im = grid.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).cpu().clip(-<span class="number">1</span>, <span class="number">1</span>) * <span class="number">0.5</span> + <span class="number">0.5</span></span><br><span class="line">Image.fromarray(np.array(im * <span class="number">255</span>).astype(np.uint8))</span><br></pre></td></tr></table></figure>

<p><img src="20.png"><br>可以用看到，生成图像的内容已经朝着提示词的方向靠近了。</p>
<h3 id="3-4-3-创建一个类别条件扩散模型"><a href="#3-4-3-创建一个类别条件扩散模型" class="headerlink" title="3.4.3 创建一个类别条件扩散模型"></a>3.4.3 创建一个类别条件扩散模型</h3><p>和先前的引导类似，类别条件扩散模型可以引导模型的生成结果更偏向于指定的“类别”。</p>
<h4 id="3-4-3-1-数据集准备"><a href="#3-4-3-1-数据集准备" class="headerlink" title="3.4.3.1 数据集准备"></a>3.4.3.1 数据集准备</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> diffusers <span class="keyword">import</span> DDPMScheduler, UNet2DModel</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br></pre></td></tr></table></figure>

<p>这里使用经典的手写数字数据集，这些图片都是单个通道的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">device = <span class="string">&#x27;mps&#x27;</span> <span class="keyword">if</span> torch.backends.mps.is_available() <span class="keyword">else</span><span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Using device: <span class="subst">&#123;device&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 载入MNIST数据集</span></span><br><span class="line">dataset = torchvision.datasets.MNIST(root=<span class="string">&quot;mnist/&quot;</span>, train=<span class="literal">True</span>,   download=<span class="literal">True</span>, transform=torchvision.transforms.ToTensor())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建数据加载器</span></span><br><span class="line">train_dataloader = DataLoader(dataset, batch_size=<span class="number">8</span>,shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">x, y = <span class="built_in">next</span>(<span class="built_in">iter</span>(train_dataloader))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Input shape:&#x27;</span>, x.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Labels:&#x27;</span>, y)</span><br><span class="line">plt.imshow(torchvision.utils.make_grid(x)[<span class="number">0</span>], cmap=<span class="string">&#x27;Greys&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p><img src="21.png"></p>
<h4 id="3-4-3-2-创建一个以类别为条件的UNet模型"><a href="#3-4-3-2-创建一个以类别为条件的UNet模型" class="headerlink" title="3.4.3.2 创建一个以类别为条件的UNet模型"></a>3.4.3.2 创建一个以类别为条件的UNet模型</h4><p>为了将类别信息注入模型，这里需要将类别信息映射到一个长度为class_emb_size的特征向量上，然后扩展成和输入图像高宽一样的张量，最后将其与输入图片在通道维度上进行拼接，这就是注入类别信息后模型的输入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ClassConditionedUnet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_classes=<span class="number">10</span>, class_emb_size=<span class="number">4</span></span>):</span></span><br><span class="line">    <span class="built_in">super</span>().__init__()</span><br><span class="line">    <span class="comment"># 这个网络层会把数字所属的类别映射到一个长度为class_emb_size的特征向量上</span></span><br><span class="line">    self.class_emb = nn.Embedding(num_classes, class_emb_size)</span><br><span class="line">    <span class="comment"># self.model是一个不带生成条件的UNet模型，在这里，我们给它添加了额外的输入通道，用于接收条件信息</span></span><br><span class="line">    self.model = UNet2DModel(</span><br><span class="line">        sample_size=<span class="number">28</span>,          <span class="comment"># 所生成图片的尺寸</span></span><br><span class="line">        in_channels=<span class="number">1</span>+ class_emb_size,  <span class="comment"># 加入额外的输入通道，#用于施加生成条件</span></span><br><span class="line">        out_channels=<span class="number">1</span>,          <span class="comment"># 输出结果的通道数</span></span><br><span class="line">        layers_per_block=<span class="number">2</span>,        <span class="comment"># 设置一个UNet模块有多少个残差连接层</span></span><br><span class="line">        block_out_channels=(<span class="number">32</span>, <span class="number">64</span>, <span class="number">64</span>),</span><br><span class="line">        down_block_types=(</span><br><span class="line">                    <span class="string">&quot;DownBlock2D&quot;</span>,    <span class="comment"># 常规的ResNet下采样模块</span></span><br><span class="line">                    <span class="string">&quot;AttnDownBlock2D&quot;</span>,  <span class="comment"># 含有spatial self-attention的</span></span><br><span class="line">                    <span class="string">&quot;AttnDownBlock2D&quot;</span>,  <span class="comment"># ResNet下采样模块</span></span><br><span class="line">                    ),</span><br><span class="line">        up_block_types=(</span><br><span class="line">                    <span class="string">&quot;AttnUpBlock2D&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;AttnUpBlock2D&quot;</span>,   <span class="comment"># 含有spatial self-attention的ResNet上采样模块</span></span><br><span class="line">                    <span class="string">&quot;UpBlock2D&quot;</span>,     <span class="comment"># 常规的ResNet下采样模块</span></span><br><span class="line">                    ),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 此时扩散模型的前向计算就会含有额外的类别标签作为输入了</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, t, class_labels</span>):</span></span><br><span class="line">    bs, ch, w, h = x.shape</span><br><span class="line">    <span class="comment"># 类别条件将会以额外通道的形式输入</span></span><br><span class="line">    class_cond = self.class_emb(class_labels)<span class="comment"># 将类别映射为向量形式，并扩展成类似于(bs, 4, 28, 28)的张量形状</span></span><br><span class="line">    class_cond = class_cond.view(bs, class_cond.shape[<span class="number">1</span>], <span class="number">1</span>, <span class="number">1</span>).expand(bs, class_cond.shape[<span class="number">1</span>], w, h)</span><br><span class="line">    <span class="comment"># 将原始输入和类别条件信息拼接到一起</span></span><br><span class="line">    net_input = torch.cat((x, class_cond), <span class="number">1</span>)  <span class="comment"># (bs, 5, 28, 28)</span></span><br><span class="line">    <span class="comment"># 使用模型进行预测</span></span><br><span class="line">    <span class="keyword">return</span> self.model(net_input, t).sample    <span class="comment"># (bs, 1, 28, 28)</span></span><br></pre></td></tr></table></figure>

<h4 id="3-4-3-3-训练和采样"><a href="#3-4-3-3-训练和采样" class="headerlink" title="3.4.3.3 训练和采样"></a>3.4.3.3 训练和采样</h4><p>准备好模型之后，现在来设置一些训练所需的基础方法，然后开启训练：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">model=ClassConditionedUnet()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个调度器</span></span><br><span class="line">noise_scheduler = DDPMScheduler(num_train_timesteps=<span class="number">1000</span>, beta_schedule=<span class="string">&#x27;squaredcos_cap_v2&#x27;</span>)</span><br><span class="line"><span class="comment"># 定义数据加载器</span></span><br><span class="line">train_dataloader = DataLoader(dataset, batch_size=<span class="number">128</span>,shuffle=<span class="literal">True</span>)</span><br><span class="line">n_epochs = <span class="number">10</span></span><br><span class="line">net = ClassConditionedUnet().to(device)</span><br><span class="line">loss_fn = nn.MSELoss()</span><br><span class="line">opt = torch.optim.Adam(net.parameters(), lr=<span class="number">1e-3</span>)</span><br><span class="line">losses = []  <span class="comment"># 记录损失值</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始训练</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(n_epochs):</span><br><span class="line">  <span class="keyword">for</span> x, y <span class="keyword">in</span> tqdm(train_dataloader):</span><br><span class="line">    <span class="comment"># 获取数据并添加噪声</span></span><br><span class="line">    x = x.to(device) * <span class="number">2</span> - <span class="number">1</span><span class="comment"># 数据被归一化到区间(-1, 1)</span></span><br><span class="line">    y = y.to(device)</span><br><span class="line">    <span class="comment"># print(x.shape,y.shape)# torch.Size([128, 1, 28, 28]) torch.Size([128])</span></span><br><span class="line">    noise = torch.randn_like(x)</span><br><span class="line">    timesteps = torch.randint(<span class="number">0</span>, <span class="number">999</span>,(x.shape[<span class="number">0</span>],)).long().to(device)<span class="comment"># 随机一个时间步</span></span><br><span class="line">    <span class="comment"># print(timesteps.shape)# torch.Size([128])</span></span><br><span class="line">    noisy_x = noise_scheduler.add_noise(x, noise,timesteps)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预测</span></span><br><span class="line">    pred = net(noisy_x, timesteps, y) <span class="comment"># 注意这里也输入了类别标签y</span></span><br><span class="line"></span><br><span class="line">    loss = loss_fn(pred, noise) <span class="comment"># 判断预测结果和实际的噪声有多接近</span></span><br><span class="line">    opt.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    opt.step()</span><br><span class="line"></span><br><span class="line">    losses.append(loss.item())</span><br><span class="line"></span><br><span class="line">  avg_loss = <span class="built_in">sum</span>(losses[-<span class="number">100</span>:])/<span class="number">100</span></span><br><span class="line">  <span class="built_in">print</span>(<span class="string">f&#x27;Finished epoch <span class="subst">&#123;epoch&#125;</span>. Average of the last 100loss values: <span class="subst">&#123;avg_loss:05f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>输出日志：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Finished epoch 0. Average of the last 100loss values: 0.053223</span><br><span class="line">Finished epoch 1. Average of the last 100loss values: 0.046675</span><br><span class="line">...</span><br><span class="line">Finished epoch 9. Average of the last 100loss values: 0.038562</span><br></pre></td></tr></table></figure>

<p>训练完成，现在来尝试让模型生成特定类别的手写数字图片，这里通过一个列表推导式来生成0到9的图片各8张：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 开始推理</span></span><br><span class="line">x = torch.randn(<span class="number">80</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>).to(device)</span><br><span class="line">y = torch.tensor([[i]*8<span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)]).flatten().to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 采样循环</span></span><br><span class="line"><span class="keyword">for</span> i, t <span class="keyword">in</span> tqdm(<span class="built_in">enumerate</span>(noise_scheduler.timesteps)):</span><br><span class="line">  <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    residual = net(x, t, y)</span><br><span class="line">  x = noise_scheduler.step(residual, t, x).prev_sample</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看生成的图像</span></span><br><span class="line">fig, ax = plt.subplots(<span class="number">1</span>, <span class="number">1</span>, figsize=(<span class="number">12</span>, <span class="number">12</span>))</span><br><span class="line">ax.imshow(torchvision.utils.make_grid(x.detach().cpu().clip(-<span class="number">1</span>,<span class="number">1</span>), nrow=<span class="number">8</span>)[<span class="number">0</span>], cmap=<span class="string">&#x27;Greys&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p><img src="22.png"></p>
<h1 id="四-Stable-Diffision"><a href="#四-Stable-Diffision" class="headerlink" title="四.Stable Diffision"></a>四.Stable Diffision</h1><p>Stable Diffusion 与传统扩散模型的最大区别在于其采用了潜在扩散（Latent Diffusion）方法，通过在低维潜在空间中进行扩散生成，而不是直接在高维图像空间进行去噪。这样做显著减少了计算复杂度和内存需求，提高了生成效率。此外，Stable Diffusion具备强大的文本到图像生成能力，能够根据自然语言描述生成图像，依托于与CLIP模型的结合，使得用户可以通过精确的文本提示控制生成的图像内容和风格。</p>
<h2 id="4-1-环境-amp-素材准备"><a href="#4-1-环境-amp-素材准备" class="headerlink" title="4.1 环境&amp;素材准备"></a>4.1 环境&amp;素材准备</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">!pip install -Uq diffusers ftfy accelerate</span><br><span class="line">!pip install -Uq git+https://github.com/huggingface/transformers</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> BytesIO</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> diffusers <span class="keyword">import</span> (</span><br><span class="line">    StableDiffusionPipeline,</span><br><span class="line">    StableDiffusionImg2ImgPipeline,</span><br><span class="line">    StableDiffusionInpaintPipeline,</span><br><span class="line">    StableDiffusionDepth2ImgPipeline</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 因为要用到的展示图片较多，所以我们写了一个旨在下载图片的函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download_image</span>(<span class="params">url</span>):</span></span><br><span class="line">  response = requests.get(url)</span><br><span class="line">  <span class="keyword">return</span> Image.<span class="built_in">open</span>(BytesIO(response.content)).convert(<span class="string">&quot;RGB&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Inpainting需要用到的图片</span></span><br><span class="line">img_url = <span class="string">&quot;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png&quot;</span></span><br><span class="line">mask_url = <span class="string">&quot;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png&quot;</span></span><br><span class="line"></span><br><span class="line">init_image = download_image(img_url).resize((<span class="number">512</span>, <span class="number">512</span>))</span><br><span class="line">mask_image = download_image(mask_url).resize((<span class="number">512</span>, <span class="number">512</span>))</span><br><span class="line"></span><br><span class="line">device = (<span class="string">&quot;mps&quot;</span><span class="keyword">if</span> torch.backends.mps.is_available()<span class="keyword">else</span><span class="string">&quot;cuda&quot;</span><span class="keyword">if</span> torch.cuda.is_available()<span class="keyword">else</span><span class="string">&quot;cpu&quot;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="4-2-体验Stable-Diffusion-：从文本生成图像"><a href="#4-2-体验Stable-Diffusion-：从文本生成图像" class="headerlink" title="4.2 体验Stable Diffusion ：从文本生成图像"></a>4.2 体验Stable Diffusion ：从文本生成图像</h2><p>通过一个封装好的管线，使用Stable Diffusion完成文生图：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 载入管线</span></span><br><span class="line">model_id = <span class="string">&quot;stabilityai/stable-diffusion-2-1-base&quot;</span></span><br><span class="line">pipe =StableDiffusionPipeline.from_pretrained(model_id).to(device)</span><br><span class="line"><span class="comment"># 也可以载入FP16精度版本</span></span><br><span class="line"><span class="comment"># pipe = StableDiffusionPipeline.from_pretrained(model_id, revision=&quot;fp16&quot;, torch_dtype=torch.float16).to(device)</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">pipe.enable_attention_slicing()<span class="comment"># 开启注意力切分功能，旨在通过降低速度来减少GPU显存的使用</span></span><br><span class="line"></span><br><span class="line">generator = torch.Generator(device=device).manual_seed(<span class="number">840</span>)</span><br><span class="line"></span><br><span class="line">pipe_output = pipe(</span><br><span class="line">    prompt=<span class="string">&quot;Palette knife painting of an autumn cityscape&quot;</span>,<span class="comment"># 提示文字：哪些要生成</span></span><br><span class="line">    negative_prompt=<span class="string">&quot;Oversaturated, blurry, low quality&quot;</span>,<span class="comment"># 提示文字：哪些不要生成</span></span><br><span class="line">    height=<span class="number">480</span>, width=<span class="number">640</span>,     <span class="comment"># 定义所生成图片的尺寸</span></span><br><span class="line">    guidance_scale=<span class="number">8</span>,          <span class="comment"># 提示文字的影响程度</span></span><br><span class="line">    num_inference_steps=<span class="number">35</span>,    <span class="comment"># 定义一次生成需要多少个推理步骤</span></span><br><span class="line">    generator=generator        <span class="comment"># 设定随机种子的生成器</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看生成图片</span></span><br><span class="line">pipe_output.images[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<p><img src="23.png"></p>
<h2 id="4-3-解析StableDiffusionPipeline的各个子模块"><a href="#4-3-解析StableDiffusionPipeline的各个子模块" class="headerlink" title="4.3 解析StableDiffusionPipeline的各个子模块"></a>4.3 解析StableDiffusionPipeline的各个子模块</h2><p>StableDiffusionPipeline 通过多个步骤将文本描述转化为图像，如下图所示。</p>
<p>首先，文本分词器 (tokenizer) 将用户提供的文本描述拆分为可以处理的“token”，以便后续处理。接着，文本编码器 (text_encoder) 将这些 token 转换为高维的语义向量，捕捉文本的含义。随后，U-Net 网络 (unet) 接收文本编码后的向量和初始的随机噪声图像，通过去噪的迭代过程生成潜在空间的表示。这个潜在表示会传递给变分自编码器 (VAE)，通过解码器将其转换回潜在空间的图像。最后，调度器 (scheduler) 控制扩散过程中的噪声去除，确保图像在多个迭代步骤中逐步清晰化，最终生成符合描述的高质量图像。</p>
<p>对了，在 StableDiffusionPipeline 中，隐变量并不需要作为显式输入提供给模型，它们是在生成过程中由模型内部自动生成和使用的，比如 (1, 4, 64, 64)就是一个低分辨率的隐变量。</p>
<p><img src="24.png"></p>
<h3 id="4-3-1-可变分自编码器"><a href="#4-3-1-可变分自编码器" class="headerlink" title="4.3.1 可变分自编码器"></a>4.3.1 可变分自编码器</h3><p>可变分自编码器（VAE）是一种生成模型，它通过学习数据的潜在分布来生成新样本。VAE 由编码器和解码器组成，编码器将输入数据映射到潜在空间的概率分布，解码器则从潜在空间中重构出与输入相似的数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建取值区间为(-1, 1)的伪数据</span></span><br><span class="line">images = torch.rand(<span class="number">1</span>, <span class="number">3</span>, <span class="number">512</span>, <span class="number">512</span>).to(device) * <span class="number">2</span> - <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Input images shape:&quot;</span>, images.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 编码到隐空间</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">  latents = <span class="number">0.18215</span> *pipe.vae.encode(images).latent_dist.mean</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Encoded latents shape:&quot;</span>, latents.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 再解码回来</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">  decoded_images = pipe.vae.decode(latents / <span class="number">0.18215</span>).sample</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Decoded images shape:&quot;</span>, decoded_images.shape)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># 创建取值区间为(-1, 1)的伪数据</span><br><span class="line">images = torch.rand(1, 3, 512, 512).to(device) * 2 - 1</span><br><span class="line">print(&quot;Input images shape:&quot;, images.shape)</span><br><span class="line"></span><br><span class="line"># 编码到隐空间</span><br><span class="line">with torch.no_grad():</span><br><span class="line">  latents = 0.18215 *pipe.vae.encode(images).latent_dist.mean</span><br><span class="line">print(&quot;Encoded latents shape:&quot;, latents.shape)</span><br><span class="line"></span><br><span class="line"># 再解码回来</span><br><span class="line">with torch.no_grad():</span><br><span class="line">  decoded_images = pipe.vae.decode(latents / 0.18215).sample</span><br><span class="line">print(&quot;Decoded images shape:&quot;, decoded_images.shape)</span><br></pre></td></tr></table></figure>

<h3 id="4-3-2-分词器和文本编码器"><a href="#4-3-2-分词器和文本编码器" class="headerlink" title="4.3.2 分词器和文本编码器"></a>4.3.2 分词器和文本编码器</h3><p>将用户输入的提示词进行分词</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 手动对提示文字进行分词和编码</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 分词</span></span><br><span class="line">input_ids = pipe.tokenizer([<span class="string">&quot;A painting of a flooble&quot;</span>])[<span class="string">&#x27;input_ids&#x27;</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Input ID -&gt; decoded token&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> input_id <span class="keyword">in</span> input_ids[<span class="number">0</span>]:</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;input_id&#125;</span> -&gt; <span class="subst">&#123;pipe.tokenizer.decode(input_id)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 手动对提示文字进行分词和编码</span><br><span class="line"></span><br><span class="line"># 分词</span><br><span class="line">input_ids = pipe.tokenizer([&quot;A painting of a flooble&quot;])[&#x27;input_ids&#x27;]</span><br><span class="line">print(&quot;Input ID -&gt; decoded token&quot;)</span><br><span class="line">for input_id in input_ids[0]:</span><br><span class="line">  print(f&quot;&#123;input_id&#125; -&gt; &#123;pipe.tokenizer.decode(input_id)&#125;&quot;)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将分词结果输入</span></span><br><span class="line">input_ids = torch.tensor(input_ids).to(device)</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">  text_embeddings = pipe.text_encoder(input_ids)[<span class="string">&#x27;last_hidden_state&#x27;</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Text embeddings shape:&quot;</span>, text_embeddings.shape)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 将分词结果输入</span><br><span class="line">input_ids = torch.tensor(input_ids).to(device)</span><br><span class="line">with torch.no_grad():</span><br><span class="line">  text_embeddings = pipe.text_encoder(input_ids)[&#x27;last_hidden_state&#x27;]</span><br><span class="line">print(&quot;Text embeddings shape:&quot;, text_embeddings.shape)</span><br></pre></td></tr></table></figure>

<h3 id="4-3-3-UNet"><a href="#4-3-3-UNet" class="headerlink" title="4.3.3 UNet"></a>4.3.3 UNet</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建伪输入</span></span><br><span class="line">timestep = pipe.scheduler.timesteps[<span class="number">0</span>]</span><br><span class="line">latents = torch.randn(<span class="number">1</span>, <span class="number">4</span>, <span class="number">64</span>, <span class="number">64</span>).to(device)</span><br><span class="line">text_embeddings = torch.randn(<span class="number">1</span>, <span class="number">77</span>, <span class="number">1024</span>).to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 让模型进行预测</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">  unet_output = pipe.unet(latents, timestep,text_embeddings).sample</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;UNet output shape:&#x27;</span>, unet_output.shape)<span class="comment"># [1, 4, 64, 64]</span></span><br></pre></td></tr></table></figure>

<h3 id="4-3-4-调度器"><a href="#4-3-4-调度器" class="headerlink" title="4.3.4 调度器"></a>4.3.4 调度器</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(pipe.scheduler.alphas_cumprod,label=<span class="string">r&#x27;$\bar&#123;\alpha&#125;$&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Timestep (high noise to low noise -&gt;)&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Noise schedule&#x27;</span>)</span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure>

<p><img src="25.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> diffusers <span class="keyword">import</span> LMSDiscreteScheduler</span><br><span class="line"></span><br><span class="line"><span class="comment"># 替换原来的调度器</span></span><br><span class="line">pipe.scheduler =LMSDiscreteScheduler.from_config(pipe.scheduler.config)<span class="comment"># 输出配置参数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Scheduler config:&#x27;</span>, pipe.scheduler)</span><br><span class="line"><span class="comment"># 使用新的调度器生成图片</span></span><br><span class="line">pipe(prompt=<span class="string">&quot;Palette knife painting of an winter cityscape&quot;</span>,   height=<span class="number">480</span>, width=<span class="number">480</span>,generator=torch.Generator(device=device).   manual_seed(<span class="number">42</span>)).images[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<p><img src="26.png"></p>
<h3 id="4-3-5-DIY采样循环"><a href="#4-3-5-DIY采样循环" class="headerlink" title="4.3.5 DIY采样循环"></a>4.3.5 DIY采样循环</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">guidance_scale = <span class="number">8</span></span><br><span class="line">num_inference_steps=<span class="number">30</span></span><br><span class="line">prompt = <span class="string">&quot;Beautiful picture of a wave breaking&quot;</span></span><br><span class="line">negative_prompt = <span class="string">&quot;zoomed in, blurry, oversaturated, warped&quot;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对提示文字进行编码</span></span><br><span class="line">text_embeddings = pipe._encode_prompt(prompt, device, <span class="number">1</span>, <span class="literal">True</span>,   negative_prompt)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建随机噪声作为起点</span></span><br><span class="line">latents = torch.randn((<span class="number">1</span>, <span class="number">4</span>, <span class="number">64</span>, <span class="number">64</span>), device=device,generator=generator)</span><br><span class="line">latents *= pipe.scheduler.init_noise_sigma</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备调度器</span></span><br><span class="line">pipe.scheduler.set_timesteps(num_inference_steps,device=device)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成过程开始</span></span><br><span class="line"><span class="keyword">for</span> i, t <span class="keyword">in</span> <span class="built_in">enumerate</span>(pipe.scheduler.timesteps):</span><br><span class="line">  latent_model_input = torch.cat([latents] * <span class="number">2</span>)</span><br><span class="line">  latent_model_input = pipe.scheduler.scale_model_input(latent_model_input, t)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    noise_pred = pipe.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample</span><br><span class="line">    noise_pred_uncond, noise_pred_text = noise_pred.chunk(<span class="number">2</span>)</span><br><span class="line">    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)<span class="comment"># classifer- redd guidence</span></span><br><span class="line">    latents = pipe.scheduler.step(noise_pred, t,latents).prev_sample</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将隐变量映射到图片</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">  image = pipe.decode_latents(latents.detach())</span><br><span class="line">pipe.numpy_to_pil(image)[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<p><img src="27.png"></p>
<h2 id="4-4-其它管线介绍"><a href="#4-4-其它管线介绍" class="headerlink" title="4.4 其它管线介绍"></a>4.4 其它管线介绍</h2><h3 id="4-4-1-Img2Img"><a href="#4-4-1-Img2Img" class="headerlink" title="4.4.1 Img2Img"></a>4.4.1 Img2Img</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 载入Img2Img管线</span></span><br><span class="line">model_id =<span class="string">&quot;stabilityai/stable-diffusion-2-1-base&quot;</span></span><br><span class="line">img2img_pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_id).to(device)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">result_image = img2img_pipe(</span><br><span class="line">    prompt=<span class="string">&quot;An oil painting of a man on a bench&quot;</span>,</span><br><span class="line">    image = init_image, <span class="comment"># 输入待编辑图片    s</span></span><br><span class="line">    trength = <span class="number">0.5</span>, <span class="comment"># 文本提示在设为0时完全不起作用，设为1时作用强度最大</span></span><br><span class="line">    ).images[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">fig, axs = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize=(<span class="number">12</span>, <span class="number">5</span>))</span><br><span class="line">axs[<span class="number">0</span>].imshow(init_image);axs[<span class="number">0</span>].set_title(<span class="string">&#x27;Input Image&#x27;</span>)</span><br><span class="line">axs[<span class="number">1</span>].imshow(result_image);axs[<span class="number">1</span>].set_title(<span class="string">&#x27;Result&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p><img src="28.png"></p>
<h3 id="4-4-2-Inpainting"><a href="#4-4-2-Inpainting" class="headerlink" title="4.4.2 Inpainting"></a>4.4.2 Inpainting</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pipe =StableDiffusionInpaintPipeline.from_pretrained(<span class="string">&quot;runwayml/stable-diffusion-inpainting&quot;</span>)</span><br><span class="line">pipe = pipe.to(device)<span class="comment"># 添加提示文字，用于让模型知道补全图像时使用什么内容</span></span><br><span class="line">prompt =<span class="string">&quot;A small robot, high resolution, sitting on a parkbench&quot;</span></span><br><span class="line">image = pipe(prompt=prompt, image=init_image,    mask_image=mask_image).images[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看结果</span></span><br><span class="line">fig, axs = plt.subplots(<span class="number">1</span>, <span class="number">3</span>, figsize=(<span class="number">16</span>, <span class="number">5</span>))</span><br><span class="line">axs[<span class="number">0</span>].imshow(init_image);axs[<span class="number">0</span>].set_title(<span class="string">&#x27;Input Image&#x27;</span>)</span><br><span class="line">axs[<span class="number">1</span>].imshow(mask_image);axs[<span class="number">1</span>].set_title(<span class="string">&#x27;Mask&#x27;</span>)</span><br><span class="line">axs[<span class="number">2</span>].imshow(image);axs[<span class="number">2</span>].set_title(<span class="string">&#x27;Result&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p><img src="29.png"></p>
<h3 id="4-4-3-Depth2Image"><a href="#4-4-3-Depth2Image" class="headerlink" title="4.4.3 Depth2Image"></a>4.4.3 Depth2Image</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 载入Depth2Img管线</span></span><br><span class="line">pipe = StableDiffusionDepth2ImgPipeline.from_pretrained(<span class="string">&quot;stabilityai/stable-diffusion-2-depth&quot;</span>)</span><br><span class="line">pipe = pipe.to(device)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用提示文字进行图像补全</span></span><br><span class="line">prompt =<span class="string">&quot;An oil painting of a man on a bench&quot;</span></span><br><span class="line">image = pipe(prompt=prompt, image=init_image).images[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看结果</span></span><br><span class="line">fig, axs = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize=(<span class="number">16</span>, <span class="number">5</span>))</span><br><span class="line">axs[<span class="number">0</span>].imshow(init_image);axs[<span class="number">0</span>].set_title(<span class="string">&#x27;Input Image&#x27;</span>)</span><br><span class="line">axs[<span class="number">1</span>].imshow(image);axs[<span class="number">1</span>].set_title(<span class="string">&#x27;Result&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p><img src="30.png"></p>
<h1 id="五-DDIM-反转"><a href="#五-DDIM-反转" class="headerlink" title="五.DDIM 反转"></a>五.DDIM 反转</h1><h2 id="5-1-数学原理"><a href="#5-1-数学原理" class="headerlink" title="5.1 数学原理"></a>5.1 数学原理</h2><p>DDIM反转的概念有些难以理解，因此，在介绍它之前，首先介绍DDPM的正向过程和反向过程，然后再介绍DDIM。</p>
<h3 id="5-1-1-DDPM"><a href="#5-1-1-DDPM" class="headerlink" title="5.1.1 DDPM"></a>5.1.1 DDPM</h3><p>在上面的章节中，你已经使用过DDPM：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> diffusers <span class="keyword">import</span> DDPMScheduler</span><br><span class="line">noise_scheduler = DDPMScheduler(num_train_timesteps=<span class="number">1000</span>, beta_start=<span class="number">0.001</span>, beta_end=<span class="number">0.004</span>)</span><br></pre></td></tr></table></figure>

<p>这里来剖析一下其内部的数学原理。</p>
<p>在 DDPM (Denoising Diffusion Probabilistic Models) 中，正向过程（扩散过程）和反向过程（去噪过程）是核心组成部分。正向过程通过逐步添加噪声将数据转化为噪声，反向过程则是通过逐步去噪来恢复原始数据。</p>
<p><strong>1)  正向扩散过程</strong> (Forward Diffusion Process)</p>
<p>假设我们有一个真实的样本$x_0$（通常是图像）。正向扩散过程是通过多步逐渐将噪声加入到原始数据中，直到最终得到一个纯噪声$x_T$。在每个时间步$t$，数据的噪声是通过以下的分布加入的：</p>
<p><img src="31.png"></p>
<p>其中：</p>
<ul>
<li>$\alpha_t$是一个随时间变化的超参数，控制每一步添加噪声的量。</li>
<li>$x_t$是在时间步$t$生成的噪声图像，$x_0$是原始图像。</li>
</ul>
<p>将上述分布转换成数学等式，即“均值+标准差*高斯噪声”，如下：<br><img src="32.png"></p>
<p>根据此公式，可以进一步推导出$x_t$和$x_0$之间的关系：<br><img src="33.png"></p>
<p>改成数学等式的形式，为：<br><img src="34.png"><br><img src="35.png"></p>
<p>由此可见，$x_t$可以由$x_0$一步得到！</p>
<p>这对于前向加噪操作是非常方便的。回想一下，在之前的章节训练时，会将随机生成的噪声施加到输入图片$x_0$上，然后随机采样一个时间步$t$，那里之所以可以随机采样时间步，就是因为$x_t$可以由$x_0$一步得到。</p>
<ol start="2">
<li><strong>反向去噪过程</strong>(Reverse Diffusion Process)</li>
</ol>
<p>这里引用知乎上一篇文章的内容(<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/666552214)%EF%BC%8C%E9%87%8C%E9%9D%A2%E5%AF%B9%E4%BA%8EDDPM%E7%9A%84%E6%95%B4%E4%B8%AA%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%88%B0%E8%AE%B2%E8%A7%A3%E7%9A%84%E5%BE%88%E6%B8%85%E6%A5%9A%E3%80%82">https://zhuanlan.zhihu.com/p/666552214)，里面对于DDPM的整个公式推到讲解的很清楚。</a></p>
<p><img src="36.png"><br><img src="37.png"><br><img src="38.png"></p>
<p>可以看到，方差是一个常数。</p>
<p>综上所述，反向去噪过程的公式可以表示为：<br><img src="39.png"></p>
<p>改写成数学等式的形式，就得到了DDPM反向去噪过程的数学公式了：<br><img src="40.png"><br>在许多实现中，去噪模型仅关注去除噪声部分，即主要输出的是去噪后的均值，而方差作为常数项并没有显著影响反向过程的表示，因此上述DDPM的反向去噪过程可以省略方差。</p>
<h3 id="5-1-2-DDIM"><a href="#5-1-2-DDIM" class="headerlink" title="5.1.2 DDIM"></a>5.1.2 DDIM</h3><p>DDIM（Denoising Diffusion Implicit Models）是一种对传统 Denoising Diffusion Probabilistic Models（DDPM）进行改进的方法。它通过修改扩散模型的反向过程，使得可以在更少的时间步内生成样本，同时保持较高的样本质量。DDIM 是一个隐式模型，它不依赖于明确的概率分布来生成样本，而是通过优化生成的过程来加速生成。<br><img src="41.png"><br><img src="42.png"><br><img src="43.png"></p>
<p>现将DDPM和DDIM的对比总结如下：<br><img src="44.png"></p>
<h2 id="5-2-实战：反转"><a href="#5-2-实战：反转" class="headerlink" title="5.2 实战：反转"></a>5.2 实战：反转</h2><h3 id="5-2-1-配置"><a href="#5-2-1-配置" class="headerlink" title="5.2.1 配置"></a>5.2.1 配置</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">!pip install -q transformers diffusers accelerate</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> BytesIO</span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms <span class="keyword">as</span> tfms</span><br><span class="line"><span class="keyword">from</span> diffusers <span class="keyword">import</span> StableDiffusionPipeline, DDIMScheduler</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义接下来将要用到的函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_image</span>(<span class="params">url, size=<span class="literal">None</span></span>):</span></span><br><span class="line">  response = requests.get(url,timeout=<span class="number">0.2</span>)</span><br><span class="line">  img = Image.<span class="built_in">open</span>(BytesIO(response.content)).convert(<span class="string">&#x27;RGB&#x27;</span>)</span><br><span class="line">  <span class="keyword">if</span> size <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    img = img.resize(size)</span><br><span class="line">  <span class="keyword">return</span> img</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span><span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span><span class="string">&quot;cpu&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="5-2-2-载入一个预训练过的StableDiffusion管线"><a href="#5-2-2-载入一个预训练过的StableDiffusion管线" class="headerlink" title="5.2.2 载入一个预训练过的StableDiffusion管线"></a>5.2.2 载入一个预训练过的StableDiffusion管线</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 载入一个管线</span></span><br><span class="line">pipe =StableDiffusionPipeline.from_pretrained(<span class="string">&quot;runwayml/stable-diffusion-v1-5&quot;</span>).to(device)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 配置DDIM调度器</span></span><br><span class="line">pipe.scheduler =DDIMScheduler.from_config(pipe.scheduler.config)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从中采样一次，以保证代码运行正常</span></span><br><span class="line">prompt = <span class="string">&#x27;Beautiful DSLR Photograph of a cute cat on the beach, golden hour&#x27;</span></span><br><span class="line">negative_prompt = <span class="string">&#x27;blurry, ugly, stock photo&#x27;</span></span><br><span class="line">im = pipe(prompt, negative_prompt=negative_prompt).images[<span class="number">0</span>]</span><br><span class="line">im.resize((<span class="number">256</span>, <span class="number">256</span>)) <span class="comment"># 调整至有利于查看的尺寸</span></span><br></pre></td></tr></table></figure>

<p><img src="45.png"></p>
<h3 id="5-2-3-DDIM采样"><a href="#5-2-3-DDIM采样" class="headerlink" title="5.2.3 DDIM采样"></a>5.2.3 DDIM采样</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 绘制&#x27;alpha&#x27;，&#x27;alpha&#x27;（即α）在DDPM论文中被称为&#x27;alpha bar&#x27;（即α）。</span></span><br><span class="line"><span class="comment"># 为了能够清晰地表现出来，我们选择使用Diffusers中的alphas_cumprod函数来得到alphas</span></span><br><span class="line">timesteps = pipe.scheduler.timesteps.cpu()</span><br><span class="line">alphas = pipe.scheduler.alphas_cumprod[timesteps]</span><br><span class="line">plt.plot(timesteps, alphas, label=<span class="string">&#x27;alpha_t&#x27;</span>)</span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure>

<p><img src="46.png"></p>
<p>在采样过程中，我们选择从时间步1000的纯噪声开始，慢慢向着时间步0前进，遵循公式：<br><img src="47.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 采样函数（标准的DDIM采样）</span></span><br><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span>(<span class="params">prompt, start_step=<span class="number">0</span>, start_latents=<span class="literal">None</span>, guidance_scale=<span class="number">3.5</span>, num_inference_steps=<span class="number">30</span>, num_images_per_prompt=<span class="number">1</span>, do_classifier_free_guidance=<span class="literal">True</span>, negative_prompt=<span class="string">&#x27;&#x27;</span>, device=device</span>):</span></span><br><span class="line">  <span class="comment"># 对文本提示语进行编码</span></span><br><span class="line">  text_embeddings = pipe._encode_prompt(</span><br><span class="line">      prompt, device, num_images_per_prompt,</span><br><span class="line">      do_classifier_free_guidance, negative_prompt</span><br><span class="line">      )</span><br><span class="line">  <span class="comment"># 配置推理的步数</span></span><br><span class="line">  pipe.scheduler.set_timesteps(num_inference_steps,device=device)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 如果没有起点，就创建一个随机的起点</span></span><br><span class="line">  <span class="keyword">if</span> start_latents <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    start_latents = torch.randn(<span class="number">1</span>, <span class="number">4</span>, <span class="number">64</span>, <span class="number">64</span>, device=device)</span><br><span class="line">    start_latents *= pipe.scheduler.init_noise_sigma</span><br><span class="line"></span><br><span class="line">  latents = start_latents.clone()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(start_step, num_inference_steps)):</span><br><span class="line">    t = pipe.scheduler.timesteps[i]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果正在进行CFG，则对隐层进行扩展</span></span><br><span class="line">    latent_model_input = torch.cat([latents] * <span class="number">2</span>) <span class="keyword">if</span> do_classifier_free_guidance <span class="keyword">else</span> latents</span><br><span class="line">    latent_model_input =pipe.scheduler.scale_model_input(latent_model_input, t)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预测残留的噪声</span></span><br><span class="line">    noise_pred = pipe.unet(latent_model_input, t,encoder_hidden_states=text_embeddings).sample</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 进行引导</span></span><br><span class="line">    <span class="keyword">if</span> do_classifier_free_guidance:</span><br><span class="line">      noise_pred_uncond, noise_pred_text =noise_pred.chunk(<span class="number">2</span>)</span><br><span class="line">      noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用调度器更新步骤</span></span><br><span class="line">    <span class="comment"># latents = pipe.scheduler.step(noise_pred, t,latents).prev_sample</span></span><br><span class="line">    <span class="comment"># 现在不用调度器，而是自行实现,这里假设方差为0</span></span><br><span class="line">    prev_t = <span class="built_in">max</span>(<span class="number">1</span>, t.item() - (<span class="number">1000</span>//num_inference_steps))<span class="comment"># 从当前时间步t推算要预测的时间步，比如t-1</span></span><br><span class="line">    alpha_t = pipe.scheduler.alphas_cumprod[t.item()]</span><br><span class="line">    alpha_t_prev = pipe.scheduler.alphas_cumprod[prev_t]</span><br><span class="line">    <span class="comment"># DDIM论文中公式的实现：sample x_&#123;t-1&#125;/x_&#123;prev&#125; from x_t</span></span><br><span class="line">    predicted_x0 = (latents - (<span class="number">1</span>-alpha_t).sqrt()*noise_pred ) / alpha_t.sqrt()</span><br><span class="line">    direction_pointing_to_xt = (<span class="number">1</span>-alpha_t_prev).sqrt()*noise_pred</span><br><span class="line">    latents = alpha_t_prev.sqrt()*predicted_x0 + direction_pointing_to_xt</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 后处理</span></span><br><span class="line">  images = pipe.decode_latents(latents)</span><br><span class="line">  images = pipe.numpy_to_pil(images)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> images</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sample(<span class="string">&#x27;Watercolor painting of a beach sunset&#x27;</span>, num_inference_steps=<span class="number">50</span>)[<span class="number">0</span>].resize((<span class="number">256</span>,<span class="number">256</span>))</span><br></pre></td></tr></table></figure>

<p><img src="48.png"></p>
<h3 id="5-2-4-反转"><a href="#5-2-4-反转" class="headerlink" title="5.2.4 反转"></a>5.2.4 反转</h3><p>首先准备一张图片，将其用VAE编码成隐向量，稍后会对其进行反转操作</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">input_image = load_image(<span class="string">&#x27;https://images.pexels.com/photos/8306128/pexels-photo-8306128.jpeg&#x27;</span>, size=(<span class="number">512</span>, <span class="number">512</span>))</span><br><span class="line">input_image_prompt =<span class="string">&quot;Photograph of a puppy on the grass&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用VAE进行编码</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">  latent = pipe.vae.encode(tfms.functional.to_tensor(input_image).unsqueeze(<span class="number">0</span>).to(device)*<span class="number">2</span>-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">cur_latent = <span class="number">0.18215</span> * latent.latent_dist.sample()</span><br></pre></td></tr></table></figure>

<p><img src="49.png"><br><img src="50.png"></p>
<p>$\sigma_t$设置为0，公式改写成由t-1到t的形式，使得生成图像朝着越来越高的噪声方向移动：<br><img src="51.png"></p>
<p>上述公式就是DDIM反转所用到的公式，现在来代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 反转</span></span><br><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">invert</span>(<span class="params">start_latents, prompt, guidance_scale=<span class="number">3.5</span>, num_inference_steps=<span class="number">80</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">           num_images_per_prompt=<span class="number">1</span>,do_classifier_free_guidance=<span class="literal">True</span>,negative_prompt=<span class="string">&#x27;&#x27;</span>,device=device</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 对提示文本进行编码</span></span><br><span class="line">  text_embeddings = pipe._encode_prompt(prompt, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 已经指定好起点</span></span><br><span class="line">  latents = start_latents.clone()</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 用一个列表保存反转的隐层</span></span><br><span class="line">  intermediate_latents = []</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 配置推理的步数</span></span><br><span class="line">  pipe.scheduler.set_timesteps(num_inference_steps,device=device)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 反转的时间步</span></span><br><span class="line">  timesteps = <span class="built_in">reversed</span>(pipe.scheduler.timesteps)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(<span class="number">1</span>, num_inference_steps), total=num_inference_steps-<span class="number">1</span>):</span><br><span class="line">    <span class="comment"># 跳过最后一次迭代</span></span><br><span class="line">    <span class="keyword">if</span> i &gt;= num_inference_steps - <span class="number">1</span>:</span><br><span class="line">      <span class="keyword">continue</span></span><br><span class="line">    t = timesteps[i]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果正在进行CFG，则对隐层进行扩展(</span></span><br><span class="line">    latent_model_input = torch.cat([latents] * <span class="number">2</span>) <span class="keyword">if</span> do_classifier_free_guidance <span class="keyword">else</span> latents</span><br><span class="line">    latent_model_input = pipe.scheduler.scale_model_input(latent_model_input, t)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预测残留的噪声</span></span><br><span class="line">    noise_pred = pipe.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 进行引导</span></span><br><span class="line">    <span class="keyword">if</span> do_classifier_free_guidance:</span><br><span class="line">      noise_pred_uncond, noise_pred_text = noise_pred.chunk(<span class="number">2</span>)</span><br><span class="line">      noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)</span><br><span class="line"></span><br><span class="line">    current_t = <span class="built_in">max</span>(<span class="number">0</span>, t.item() - (<span class="number">1000</span>//num_inference_steps))<span class="comment">#t</span></span><br><span class="line">    next_t = t <span class="comment"># min(999, t.item() + (1000//num_inference_steps)) #t+1</span></span><br><span class="line">    alpha_t = pipe.scheduler.alphas_cumprod[current_t]</span><br><span class="line">    alpha_t_next = pipe.scheduler.alphas_cumprod[next_t]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 反转的更新步（重新排列更新步，利用xt-1（当前隐层）得到xt（新的隐层））</span></span><br><span class="line">    latents = (latents - (<span class="number">1</span>-alpha_t).sqrt()*noise_pred)*(alpha_t_next.sqrt()/alpha_t.sqrt()) + (<span class="number">1</span>-alpha_t_next).sqrt()*noise_pred</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存</span></span><br><span class="line">    intermediate_latents.append(latents)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> torch.cat(intermediate_latents)</span><br></pre></td></tr></table></figure>

<p>现在，对正常图片的隐变量进行反转，得到噪声图的隐变量：```python<br>inverted_latents = invert(cur_latent,input_image_prompt,num_inference_steps=50)<br>inverted_latents.shape# [48, 4, 64, 64]</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">对隐变量进行解码，恢复到相应的图片：</span><br><span class="line">```python</span><br><span class="line"># 解码反转的最后一个隐层</span><br><span class="line">with torch.no_grad():</span><br><span class="line">  im = pipe.decode_latents(inverted_latents[-1].unsqueeze(0))</span><br><span class="line">pipe.numpy_to_pil(im)[0]</span><br></pre></td></tr></table></figure>

<p><img src="52.png"></p>
<p>对每个隐变量都进行解码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设 inverted_latents 是一个包含多个 latent 的列表</span></span><br><span class="line">images = []</span><br><span class="line"><span class="keyword">for</span> latent <span class="keyword">in</span> inverted_latents:</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        im = pipe.decode_latents(latent.unsqueeze(<span class="number">0</span>))[<span class="number">0</span>]</span><br><span class="line">        images.append(im)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个绘图</span></span><br><span class="line">fig, axs = plt.subplots(<span class="number">1</span>, <span class="built_in">len</span>(images), figsize=(<span class="number">15</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ax, img <span class="keyword">in</span> <span class="built_in">zip</span>(axs, images):</span><br><span class="line">    ax.imshow(img)</span><br><span class="line">    ax.axis(<span class="string">&#x27;off&#x27;</span>)  <span class="comment"># 不显示坐标轴</span></span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="53.png"></p>
<p>使用最后一层的隐变量，执行反向去噪：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将上面的得到的最终的噪声进行去噪</span></span><br><span class="line">pipe(input_image_prompt, latents=inverted_latents[-<span class="number">1</span>][<span class="literal">None</span>],num_inference_steps=<span class="number">50</span>, guidance_scale=<span class="number">3.5</span>).images[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<p><img src="54.png"></p>
<p>我们遇到一个问题：这不是最初使用的那张图片。这是因为DDIM反转需要一个重要的假设——在时刻t预测的噪声与在时刻t +1预测的噪声相同，但这个假设在反转50步或100步时是不成立的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置起点的原因</span></span><br><span class="line">start_step=<span class="number">10</span></span><br><span class="line">sample(input_image_prompt, start_latents=inverted_latents[-(start_step+<span class="number">1</span>)][<span class="literal">None</span>], start_step=start_step, num_inference_steps=<span class="number">50</span>)[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<p><img src="55.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用新的文本提示语进行采样</span></span><br><span class="line">start_step=<span class="number">10</span></span><br><span class="line">new_prompt = input_image_prompt.replace(<span class="string">&#x27;puppy&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>)</span><br><span class="line">sample(new_prompt, start_latents=inverted_latents[-(start_step+<span class="number">1</span>)][<span class="literal">None</span>], start_step=start_step, num_inference_steps=<span class="number">50</span>)[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<p><img src="56.png"></p>
<h2 id="5-3-组合封装"><a href="#5-3-组合封装" class="headerlink" title="5.3 组合封装"></a>5.3 组合封装</h2><p>将采样和反转函数组合起来，就得到了图像编辑函数 <code>edit</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">edit</span>(<span class="params">input_image, input_image_prompt, edit_prompt,num_steps=<span class="number">100</span>, start_step=<span class="number">30</span>,guidance_scale=<span class="number">3.5</span></span>):</span></span><br><span class="line">  <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    latent =pipe.vae.encode(tfms.functional.to_tensor(input_image).unsqueeze(<span class="number">0</span>).to(device)*<span class="number">2</span>-<span class="number">1</span>)</span><br><span class="line">    l = <span class="number">0.18215</span> * latent.latent_dist.sample()</span><br><span class="line">    inverted_latents = invert(l,input_image_prompt,num_inference_steps=num_steps)</span><br><span class="line">    final_im = sample(edit_prompt,start_latents=inverted_latents[-(start_step+<span class="number">1</span>)][<span class="literal">None</span>],start_step=start_step,num_inference_steps=num_steps,guidance_scale=guidance_scale)[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> final_im</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实际操作</span></span><br><span class="line">edit(input_image, <span class="string">&#x27;A puppy on the grass&#x27;</span>, <span class="string">&#x27;an old grey dog on the grass&#x27;</span>, num_steps=<span class="number">50</span>,start_step=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>

<p><img src="57.png"></p>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/">扩散模型</a></div><div class="post_share"><div class="social-share" data-image="https://ice.frostsky.com/2024/12/07/bf4bfcbe17c50e1fcbc8d26acfac57c8.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/12/07/PyTorch%E8%BD%ACTensorRT-engine%E4%BF%9D%E5%A7%86%E7%BA%A7%E6%95%99%E7%A8%8B/Markdown%20_%20%E8%AE%A9%E6%8E%92%E7%89%88%E5%8F%98%20Nice/"><img class="prev-cover" src="/img/tag1.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info"></div></div></a></div><div class="next-post pull-right"><a href="/2024/10/27/%E4%BD%BF%E7%94%A8%E5%8E%9F%E7%94%9FTensortRT-API%E5%8A%A0%E9%80%9F%E6%8E%A8%E7%90%86/"><img class="next-cover" src="https://ice.frostsky.com/2024/10/27/fddae99226a3fef4f567cd1922aef787.jpeg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">使用原生TensortRT-API加速推理</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://www.cdnjson.com/images/2021/11/27/_20210211193948.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">雨落诗山山亦奇</div><div class="author-info__description">本站为读研版&工作版博客，大学版移步 --> fuhanshi.github.io</div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">170</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">本站内容的最终版本将发布在微信公众号[南极Python]</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%9B%B6-%E5%87%86%E5%A4%87%E5%B7%A5%E5%85%B7%E5%87%BD%E6%95%B0"><span class="toc-number">1.</span> <span class="toc-text">零. 准备工具函数</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80-%E6%96%87%E7%94%9F%E5%9B%BEdemo%E7%9A%84%E5%BF%AB%E9%80%9F%E5%AE%9E%E7%8E%B0"><span class="toc-number">2.</span> <span class="toc-text">一. 文生图demo的快速实现</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C-%E4%BD%BF%E7%94%A8diffusers%E5%8A%A8%E6%89%8B%E5%81%9A%E4%B8%80%E4%B8%AA%E5%AE%8C%E6%95%B4%E7%9A%84%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E9%A1%B9%E7%9B%AE"><span class="toc-number">3.</span> <span class="toc-text">二.使用diffusers动手做一个完整的图像生成项目</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-%E6%95%B0%E6%8D%AE%E9%9B%86%E5%87%86%E5%A4%87"><span class="toc-number">3.1.</span> <span class="toc-text">2.1 数据集准备</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-%E8%B0%83%E5%BA%A6%E5%99%A8"><span class="toc-number">3.2.</span> <span class="toc-text">2.2 调度器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-%E5%AE%9A%E4%B9%89%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.3.</span> <span class="toc-text">2.3 定义扩散模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-4-%E5%88%9B%E5%BB%BA%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E5%BE%AA%E7%8E%AF"><span class="toc-number">3.4.</span> <span class="toc-text">2.4 创建扩散模型训练循环</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-5-%E5%9B%BE%E5%83%8F%E7%9A%84%E7%94%9F%E6%88%90"><span class="toc-number">3.5.</span> <span class="toc-text">2.5 图像的生成</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%B9%E6%B3%951%EF%BC%9A%E5%BB%BA%E7%AB%8B%E4%B8%80%E4%B8%AAdiffusers%E7%AE%A1%E7%BA%BF"><span class="toc-number">3.5.1.</span> <span class="toc-text">方法1：建立一个diffusers管线</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%B9%E6%B3%952%EF%BC%9A%E8%87%AA%E5%AE%9A%E4%B9%89%E4%B8%80%E4%B8%AA%E9%87%87%E6%A0%B7%E5%BE%AA%E7%8E%AF"><span class="toc-number">3.5.2.</span> <span class="toc-text">方法2：自定义一个采样循环</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%89-%E5%BE%AE%E8%B0%83%E5%92%8C%E5%BC%95%E5%AF%BC"><span class="toc-number">4.</span> <span class="toc-text">三. 微调和引导</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87"><span class="toc-number">4.1.</span> <span class="toc-text">3.1 环境准备</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-%E4%BD%BF%E7%94%A8DDIM-%E6%9B%B4%E5%BF%AB%E7%9A%84%E8%B0%83%E5%BA%A6%E5%99%A8"><span class="toc-number">4.2.</span> <span class="toc-text">3.2 使用DDIM-更快的调度器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-%E5%BE%AE%E8%B0%83%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B"><span class="toc-number">4.3.</span> <span class="toc-text">3.3 微调扩散模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-4-%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E4%B9%8B%E5%BC%95%E5%AF%BC"><span class="toc-number">4.4.</span> <span class="toc-text">3.4 扩散模型之引导</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-1-%E9%A2%9C%E8%89%B2%E5%BC%95%E5%AF%BC"><span class="toc-number">4.4.1.</span> <span class="toc-text">3.4.1 颜色引导</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-2-CLIP%E5%BC%95%E5%AF%BC"><span class="toc-number">4.4.2.</span> <span class="toc-text">3.4.2 CLIP引导</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-3-%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E7%B1%BB%E5%88%AB%E6%9D%A1%E4%BB%B6%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B"><span class="toc-number">4.4.3.</span> <span class="toc-text">3.4.3 创建一个类别条件扩散模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-3-1-%E6%95%B0%E6%8D%AE%E9%9B%86%E5%87%86%E5%A4%87"><span class="toc-number">4.4.3.1.</span> <span class="toc-text">3.4.3.1 数据集准备</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-3-2-%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E4%BB%A5%E7%B1%BB%E5%88%AB%E4%B8%BA%E6%9D%A1%E4%BB%B6%E7%9A%84UNet%E6%A8%A1%E5%9E%8B"><span class="toc-number">4.4.3.2.</span> <span class="toc-text">3.4.3.2 创建一个以类别为条件的UNet模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-3-3-%E8%AE%AD%E7%BB%83%E5%92%8C%E9%87%87%E6%A0%B7"><span class="toc-number">4.4.3.3.</span> <span class="toc-text">3.4.3.3 训练和采样</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%9B-Stable-Diffision"><span class="toc-number">5.</span> <span class="toc-text">四.Stable Diffision</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-%E7%8E%AF%E5%A2%83-amp-%E7%B4%A0%E6%9D%90%E5%87%86%E5%A4%87"><span class="toc-number">5.1.</span> <span class="toc-text">4.1 环境&amp;素材准备</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-%E4%BD%93%E9%AA%8CStable-Diffusion-%EF%BC%9A%E4%BB%8E%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90%E5%9B%BE%E5%83%8F"><span class="toc-number">5.2.</span> <span class="toc-text">4.2 体验Stable Diffusion ：从文本生成图像</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-3-%E8%A7%A3%E6%9E%90StableDiffusionPipeline%E7%9A%84%E5%90%84%E4%B8%AA%E5%AD%90%E6%A8%A1%E5%9D%97"><span class="toc-number">5.3.</span> <span class="toc-text">4.3 解析StableDiffusionPipeline的各个子模块</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-1-%E5%8F%AF%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8"><span class="toc-number">5.3.1.</span> <span class="toc-text">4.3.1 可变分自编码器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-2-%E5%88%86%E8%AF%8D%E5%99%A8%E5%92%8C%E6%96%87%E6%9C%AC%E7%BC%96%E7%A0%81%E5%99%A8"><span class="toc-number">5.3.2.</span> <span class="toc-text">4.3.2 分词器和文本编码器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-3-UNet"><span class="toc-number">5.3.3.</span> <span class="toc-text">4.3.3 UNet</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-4-%E8%B0%83%E5%BA%A6%E5%99%A8"><span class="toc-number">5.3.4.</span> <span class="toc-text">4.3.4 调度器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-5-DIY%E9%87%87%E6%A0%B7%E5%BE%AA%E7%8E%AF"><span class="toc-number">5.3.5.</span> <span class="toc-text">4.3.5 DIY采样循环</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-4-%E5%85%B6%E5%AE%83%E7%AE%A1%E7%BA%BF%E4%BB%8B%E7%BB%8D"><span class="toc-number">5.4.</span> <span class="toc-text">4.4 其它管线介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-1-Img2Img"><span class="toc-number">5.4.1.</span> <span class="toc-text">4.4.1 Img2Img</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-2-Inpainting"><span class="toc-number">5.4.2.</span> <span class="toc-text">4.4.2 Inpainting</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-3-Depth2Image"><span class="toc-number">5.4.3.</span> <span class="toc-text">4.4.3 Depth2Image</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%94-DDIM-%E5%8F%8D%E8%BD%AC"><span class="toc-number">6.</span> <span class="toc-text">五.DDIM 反转</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#5-1-%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86"><span class="toc-number">6.1.</span> <span class="toc-text">5.1 数学原理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-1-DDPM"><span class="toc-number">6.1.1.</span> <span class="toc-text">5.1.1 DDPM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-2-DDIM"><span class="toc-number">6.1.2.</span> <span class="toc-text">5.1.2 DDIM</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-2-%E5%AE%9E%E6%88%98%EF%BC%9A%E5%8F%8D%E8%BD%AC"><span class="toc-number">6.2.</span> <span class="toc-text">5.2 实战：反转</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-1-%E9%85%8D%E7%BD%AE"><span class="toc-number">6.2.1.</span> <span class="toc-text">5.2.1 配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-2-%E8%BD%BD%E5%85%A5%E4%B8%80%E4%B8%AA%E9%A2%84%E8%AE%AD%E7%BB%83%E8%BF%87%E7%9A%84StableDiffusion%E7%AE%A1%E7%BA%BF"><span class="toc-number">6.2.2.</span> <span class="toc-text">5.2.2 载入一个预训练过的StableDiffusion管线</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-3-DDIM%E9%87%87%E6%A0%B7"><span class="toc-number">6.2.3.</span> <span class="toc-text">5.2.3 DDIM采样</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-4-%E5%8F%8D%E8%BD%AC"><span class="toc-number">6.2.4.</span> <span class="toc-text">5.2.4 反转</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-3-%E7%BB%84%E5%90%88%E5%B0%81%E8%A3%85"><span class="toc-number">6.3.</span> <span class="toc-text">5.3 组合封装</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/03/11/LLM%E8%87%AA%E5%9B%9E%E5%BD%92%E9%A2%84%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E8%AF%A6%E8%A7%A3-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%BA%94)/" title="LLM自回归预训练过程详解-大模型炼丹术(五)"><img src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="LLM自回归预训练过程详解-大模型炼丹术(五)"/></a><div class="content"><a class="title" href="/2025/03/11/LLM%E8%87%AA%E5%9B%9E%E5%BD%92%E9%A2%84%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E8%AF%A6%E8%A7%A3-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%BA%94)/" title="LLM自回归预训练过程详解-大模型炼丹术(五)">LLM自回归预训练过程详解-大模型炼丹术(五)</a><time datetime="2025-03-11T13:31:10.000Z" title="发表于 2025-03-11 21:31:10">2025-03-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/03/07/%E5%8A%A8%E6%89%8B%E6%90%AD%E5%BB%BAGPT2%E6%9E%B6%E6%9E%84-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E5%9B%9B)/" title="动手搭建GPT2架构-大模型炼丹术(四)"><img src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="动手搭建GPT2架构-大模型炼丹术(四)"/></a><div class="content"><a class="title" href="/2025/03/07/%E5%8A%A8%E6%89%8B%E6%90%AD%E5%BB%BAGPT2%E6%9E%B6%E6%9E%84-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E5%9B%9B)/" title="动手搭建GPT2架构-大模型炼丹术(四)">动手搭建GPT2架构-大模型炼丹术(四)</a><time datetime="2025-03-07T15:35:12.000Z" title="发表于 2025-03-07 23:35:12">2025-03-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/03/04/%E4%BB%8E%E5%8D%95%E5%A4%B4%E5%88%B0%E5%A4%9A%E5%A4%B4%EF%BC%8C%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%B8%89)/" title="从单头到多头，深度解析大模型的注意力机制-大模型炼丹术(三)"><img src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="从单头到多头，深度解析大模型的注意力机制-大模型炼丹术(三)"/></a><div class="content"><a class="title" href="/2025/03/04/%E4%BB%8E%E5%8D%95%E5%A4%B4%E5%88%B0%E5%A4%9A%E5%A4%B4%EF%BC%8C%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%B8%89)/" title="从单头到多头，深度解析大模型的注意力机制-大模型炼丹术(三)">从单头到多头，深度解析大模型的注意力机制-大模型炼丹术(三)</a><time datetime="2025-03-04T14:03:57.000Z" title="发表于 2025-03-04 22:03:57">2025-03-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/02/24/%E4%BB%8E%E7%A6%BB%E6%95%A3%E7%9A%84tokenID%E5%88%B0%E5%85%B7%E6%9C%89%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%E7%9A%84embedding-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%BA%8C)/" title="从离散的token IDs到具有语义信息的embedding-大模型炼丹术(二)"><img src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="从离散的token IDs到具有语义信息的embedding-大模型炼丹术(二)"/></a><div class="content"><a class="title" href="/2025/02/24/%E4%BB%8E%E7%A6%BB%E6%95%A3%E7%9A%84tokenID%E5%88%B0%E5%85%B7%E6%9C%89%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%E7%9A%84embedding-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%BA%8C)/" title="从离散的token IDs到具有语义信息的embedding-大模型炼丹术(二)">从离散的token IDs到具有语义信息的embedding-大模型炼丹术(二)</a><time datetime="2025-02-24T14:28:29.000Z" title="发表于 2025-02-24 22:28:29">2025-02-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/02/20/%E4%BB%8Etokenizer%E8%AF%B4%E8%B5%B7%EF%BC%8C%E4%B8%BALLM%E8%87%AA%E5%9B%9E%E5%BD%92%E9%A2%84%E8%AE%AD%E7%BB%83%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE%E9%9B%86-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%B8%80)/" title="从tokenizer说起，为LLM自回归预训练准备数据集-大模型炼丹术(一)"><img src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="从tokenizer说起，为LLM自回归预训练准备数据集-大模型炼丹术(一)"/></a><div class="content"><a class="title" href="/2025/02/20/%E4%BB%8Etokenizer%E8%AF%B4%E8%B5%B7%EF%BC%8C%E4%B8%BALLM%E8%87%AA%E5%9B%9E%E5%BD%92%E9%A2%84%E8%AE%AD%E7%BB%83%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE%E9%9B%86-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%B8%80)/" title="从tokenizer说起，为LLM自回归预训练准备数据集-大模型炼丹术(一)">从tokenizer说起，为LLM自回归预训练准备数据集-大模型炼丹术(一)</a><time datetime="2025-02-20T14:28:29.000Z" title="发表于 2025-02-20 22:28:29">2025-02-20</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://ice.frostsky.com/2024/12/07/bf4bfcbe17c50e1fcbc8d26acfac57c8.png')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By 雨落诗山山亦奇</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/fireworks.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>