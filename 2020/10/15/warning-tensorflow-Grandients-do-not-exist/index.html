<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>【警告原因不明，怀疑是梯度消失】warning:tensorflow:Grandients_do_not_exist | 南极Python</title><meta name="keywords" content="DL"><meta name="author" content="雨落诗山山亦奇"><meta name="copyright" content="雨落诗山山亦奇"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="更新！！！ 好像并不是Dropout的原因，在训练SGAN时也出现了同样的警告，即使已经设置了Dropout层的traing&#x3D;True，怀疑是梯度消失（有文章说是这个 https:&#x2F;&#x2F;www.jiqizhixin.com&#x2F;articles&#x2F;2018-11-27-24 ）或爆炸  在本文中，可能设置training&#x3D;Trur恰好避免了梯度消失或爆炸，只是凑巧而已（猜测） 具体原因后续再来分析，先这样">
<meta property="og:type" content="article">
<meta property="og:title" content="【警告原因不明，怀疑是梯度消失】warning:tensorflow:Grandients_do_not_exist">
<meta property="og:url" content="http://yoursite.com/2020/10/15/warning-tensorflow-Grandients-do-not-exist/index.html">
<meta property="og:site_name" content="南极Python">
<meta property="og:description" content="更新！！！ 好像并不是Dropout的原因，在训练SGAN时也出现了同样的警告，即使已经设置了Dropout层的traing&#x3D;True，怀疑是梯度消失（有文章说是这个 https:&#x2F;&#x2F;www.jiqizhixin.com&#x2F;articles&#x2F;2018-11-27-24 ）或爆炸  在本文中，可能设置training&#x3D;Trur恰好避免了梯度消失或爆炸，只是凑巧而已（猜测） 具体原因后续再来分析，先这样">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s1.ax1x.com/2020/07/19/URRLqI.jpg">
<meta property="article:published_time" content="2020-10-15T07:29:51.000Z">
<meta property="article:modified_time" content="2024-04-19T16:39:46.000Z">
<meta property="article:author" content="雨落诗山山亦奇">
<meta property="article:tag" content="DL">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s1.ax1x.com/2020/07/19/URRLqI.jpg"><link rel="shortcut icon" href="https://www.cdnjson.com/images/2021/11/27/_20210211193948.png"><link rel="canonical" href="http://yoursite.com/2020/10/15/warning-tensorflow-Grandients-do-not-exist/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '【警告原因不明，怀疑是梯度消失】warning:tensorflow:Grandients_do_not_exist',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-04-20 00:39:46'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/StaticFile_HEXO@latest/butterfly/css/macblack.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://www.cdnjson.com/images/2021/11/27/_20210211193948.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">169</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div></div><hr/></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://s1.ax1x.com/2020/07/19/URRLqI.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">南极Python</a></span><div id="menus"><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">【警告原因不明，怀疑是梯度消失】warning:tensorflow:Grandients_do_not_exist</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-10-15T07:29:51.000Z" title="发表于 2020-10-15 15:29:51">2020-10-15</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-04-19T16:39:46.000Z" title="更新于 2024-04-20 00:39:46">2024-04-20</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">深度学习笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">838</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>4分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="【警告原因不明，怀疑是梯度消失】warning:tensorflow:Grandients_do_not_exist"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>更新！！！</p>
<p>好像并不是<code>Dropout</code>的原因，在训练<code>SGAN</code>时也出现了同样的警告，即使已经设置了<code>Dropout</code>层的<code>traing=True</code>，怀疑是梯度消失（有文章说是这个 <a target="_blank" rel="noopener" href="https://www.jiqizhixin.com/articles/2018-11-27-24">https://www.jiqizhixin.com/articles/2018-11-27-24</a> ）或爆炸</p>
<p><img src="1.jpg"></p>
<p>在本文中，可能设置<code>training=Trur</code>恰好避免了梯度消失或爆炸，只是凑巧而已（猜测）</p>
<p>具体原因后续再来分析，先这样了。</p>
<h3 id="问题提出"><a href="#问题提出" class="headerlink" title="问题提出"></a>问题提出</h3><p>基于DCGAN（<a target="_blank" rel="noopener" href="https://fx0809.gitee.io/2020/10/07/DCGAN/%EF%BC%89%E7%9A%84%E4%BB%A3%E7%A0%81%EF%BC%8C%E6%83%B3%E8%A6%81%E5%B0%86%E7%94%9F%E6%88%90%E5%99%A8%E5%92%8C%E5%88%A4%E5%88%AB%E5%99%A8%E7%9A%84%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F%E6%94%B9%E4%B8%BA%E7%BB%A7%E6%89%BF%E8%87%AA%60tf.keras.Model%60%E7%B1%BB%E7%9A%84%E6%96%B9%E5%BC%8F%EF%BC%8C%E4%BF%AE%E6%94%B9%E9%83%A8%E5%88%86%E7%9A%84%E4%BB%A3%E7%A0%81%E5%A6%82%E4%B8%8B%EF%BC%9A">https://fx0809.gitee.io/2020/10/07/DCGAN/）的代码，想要将生成器和判别器的实现方式改为继承自`tf.keras.Model`类的方式，修改部分的代码如下：</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator_model</span>(<span class="params">tf.keras.Model</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        </span><br><span class="line">        self.dense=tf.keras.layers.Dense(<span class="number">7</span>*<span class="number">7</span>*<span class="number">256</span>,use_bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1=tf.keras.layers.BatchNormalization()</span><br><span class="line">        self.leakyrelu1=tf.keras.layers.LeakyReLU()</span><br><span class="line">        </span><br><span class="line">        self.reshape=tf.keras.layers.Reshape((<span class="number">7</span>,<span class="number">7</span>,<span class="number">256</span>))</span><br><span class="line">        </span><br><span class="line">        self.convT1=tf.keras.layers.Conv2DTranspose(<span class="number">128</span>,(<span class="number">5</span>,<span class="number">5</span>),strides=(<span class="number">1</span>,<span class="number">1</span>),padding=<span class="string">&#x27;same&#x27;</span>,use_bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn2=tf.keras.layers.BatchNormalization()</span><br><span class="line">        self.leakyrelu2=tf.keras.layers.LeakyReLU()</span><br><span class="line">        </span><br><span class="line">        self.convT2=tf.keras.layers.Conv2DTranspose(<span class="number">64</span>,(<span class="number">5</span>,<span class="number">5</span>),strides=(<span class="number">2</span>,<span class="number">2</span>),padding=<span class="string">&#x27;same&#x27;</span>,use_bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn3=tf.keras.layers.BatchNormalization()</span><br><span class="line">        self.leakyrelu3=tf.keras.layers.LeakyReLU()</span><br><span class="line">        </span><br><span class="line">        self.convT3=tf.keras.layers.Conv2DTranspose(<span class="number">1</span>,(<span class="number">5</span>,<span class="number">5</span>),strides=(<span class="number">2</span>,<span class="number">2</span>),padding=<span class="string">&#x27;same&#x27;</span>,use_bias=<span class="literal">False</span>,activation=<span class="string">&#x27;tanh&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self,inputs,training=<span class="literal">True</span></span>):</span></span><br><span class="line">        x=self.dense(inputs)</span><br><span class="line">        x=self.bn1(x,training)</span><br><span class="line">        x=self.leakyrelu1(x)</span><br><span class="line">        </span><br><span class="line">        x=self.reshape(x)</span><br><span class="line">        </span><br><span class="line">        x=self.convT1(x)</span><br><span class="line">        x=self.bn2(x,training)</span><br><span class="line">        x=self.leakyrelu2(x)</span><br><span class="line">        </span><br><span class="line">        x=self.convT2(x)</span><br><span class="line">        x=self.bn3(x,training)</span><br><span class="line">        x=self.leakyrelu3(x)</span><br><span class="line">        </span><br><span class="line">        x=self.convT3(x)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Discriminator_model</span>(<span class="params">tf.keras.Model</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        </span><br><span class="line">        self.conv1=tf.keras.layers.Conv2D(<span class="number">64</span>,(<span class="number">5</span>,<span class="number">5</span>),strides=(<span class="number">2</span>,<span class="number">2</span>),padding=<span class="string">&#x27;same&#x27;</span>)</span><br><span class="line">        self.leakyrelu1=tf.keras.layers.LeakyReLU()</span><br><span class="line">        self.dropout1=tf.keras.layers.Dropout(<span class="number">0.3</span>)</span><br><span class="line">        </span><br><span class="line">        self.conv2=tf.keras.layers.Conv2D(<span class="number">128</span>,(<span class="number">5</span>,<span class="number">5</span>),strides=(<span class="number">2</span>,<span class="number">2</span>),padding=<span class="string">&#x27;same&#x27;</span>)</span><br><span class="line">        self.leakyrelu2=tf.keras.layers.LeakyReLU()</span><br><span class="line">        self.dropout2=tf.keras.layers.Dropout(<span class="number">0.3</span>)</span><br><span class="line">        </span><br><span class="line">        self.flatten=tf.keras.layers.Flatten()</span><br><span class="line">        </span><br><span class="line">        self.dense=tf.keras.layers.Dense(<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self,inputs,training=<span class="literal">True</span></span>):</span></span><br><span class="line">        x=self.conv1(inputs)</span><br><span class="line">        x=self.leakyrelu1(x)</span><br><span class="line">        x=self.dropout1(x)</span><br><span class="line">        </span><br><span class="line">        x=self.conv2(inputs)</span><br><span class="line">        x=self.leakyrelu2(x)</span><br><span class="line">        x=self.dropout2(x)  </span><br><span class="line">        </span><br><span class="line">        x=self.flatten(x)</span><br><span class="line">        </span><br><span class="line">        x=self.dense(x)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>



<p>运行全部完整后，发现会不断地弹出<code>warning</code>信息：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">WARNING:tensorflow:Gradients do not exist for variables [&#x27;discriminator_model/conv2d/kernel:0&#x27;, &#x27;discriminator_model/conv2d/bias:0&#x27;] when minimizing the loss.</span><br></pre></td></tr></table></figure>

<h3 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h3><p>把判别器的<code>Dropout</code>层中的<code>traing</code>参数设置为<code>Ture</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Discriminator_model</span>(<span class="params">tf.keras.Model</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        </span><br><span class="line">        self.conv1=tf.keras.layers.Conv2D(<span class="number">64</span>,(<span class="number">5</span>,<span class="number">5</span>),strides=(<span class="number">2</span>,<span class="number">2</span>),padding=<span class="string">&#x27;same&#x27;</span>)</span><br><span class="line">        self.leakyrelu1=tf.keras.layers.LeakyReLU()</span><br><span class="line">        self.dropout1=tf.keras.layers.Dropout(<span class="number">0.3</span>)</span><br><span class="line">        </span><br><span class="line">        self.conv2=tf.keras.layers.Conv2D(<span class="number">128</span>,(<span class="number">5</span>,<span class="number">5</span>),strides=(<span class="number">2</span>,<span class="number">2</span>),padding=<span class="string">&#x27;same&#x27;</span>)</span><br><span class="line">        self.leakyrelu2=tf.keras.layers.LeakyReLU()</span><br><span class="line">        self.dropout2=tf.keras.layers.Dropout(<span class="number">0.3</span>)</span><br><span class="line">        </span><br><span class="line">        self.flatten=tf.keras.layers.Flatten()</span><br><span class="line">        </span><br><span class="line">        self.dense=tf.keras.layers.Dense(<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self,inputs,training=<span class="literal">True</span></span>):</span></span><br><span class="line">        x=self.conv1(inputs)</span><br><span class="line">        x=self.leakyrelu1(x)</span><br><span class="line">        x=self.dropout1(x,training)</span><br><span class="line">        </span><br><span class="line">        x=self.conv2(inputs)</span><br><span class="line">        x=self.leakyrelu2(x)</span><br><span class="line">        x=self.dropout2(x,training)  </span><br><span class="line">        </span><br><span class="line">        x=self.flatten(x)</span><br><span class="line">        </span><br><span class="line">        x=self.dense(x)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><p>官网关于<code>Dropout</code>层的描述如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting. Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over all inputs is unchanged.</span><br><span class="line"></span><br><span class="line">Note that the Dropout layer only applies when training is set to True such that no values are dropped during inference. When using model.fit, training will be appropriately set to True automatically, and in other contexts, you can set the kwarg explicitly to True when calling the layer.</span><br><span class="line"></span><br><span class="line">(This is in contrast to setting trainable=False for a Dropout layer. trainable does not affect the layer&#x27;s behavior, as Dropout does not have any variables/weights that can be frozen during training.)</span><br></pre></td></tr></table></figure>

<p>或许正如<code>in other contexts, you can set the kwarg explicitly to True when calling the layer</code>所说，我们在使用其他方式搭建模型(比如这里的call方法)时，要手动设置<code>training=True</code>。</p>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/DL/">DL</a></div><div class="post_share"><div class="social-share" data-image="https://s1.ax1x.com/2020/07/19/URRLqI.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/10/15/DCGAN-V2-0/"><img class="prev-cover" src="https://s1.ax1x.com/2020/10/07/0dSxjx.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">DCGAN_V2.0</div></div></a></div><div class="next-post pull-right"><a href="/2020/10/15/SGAN/"><img class="next-cover" src="https://s1.ax1x.com/2020/10/15/07Q3Eq.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">【训练结果很糟糕】SGAN</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2021/04/05/1%E5%B0%8F%E6%97%B6%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8PyTorch/" title="1小时快速入门PyTorch"><img class="cover" src="https://z3.ax1x.com/2021/04/10/cajxDs.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-04-05</div><div class="title">1小时快速入门PyTorch</div></div></a></div><div><a href="/2021/07/28/6D%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" title="FFB6D"><img class="cover" src="https://www.cdnjson.com/images/2021/08/02/288c7ab422d3d3d15.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-07-28</div><div class="title">FFB6D</div></div></a></div><div><a href="/2021/04/09/FaceFromX/" title="FaceFromX"><img class="cover" src="https://z3.ax1x.com/2021/04/09/cUQzfU.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-04-09</div><div class="title">FaceFromX</div></div></a></div><div><a href="/2021/07/30/G2L-Net/" title="G2L-Net"><img class="cover" src="https://www.cdnjson.com/images/2021/08/02/14eac1ef7f05c4615.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-07-30</div><div class="title">G2L-Net</div></div></a></div><div><a href="/2020/10/12/Keras%E4%B8%AD%E5%85%B3%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84trainable%E7%8A%B6%E6%80%81%E7%9A%84%E9%97%AE%E9%A2%98/" title="Keras中关于模型的trainable状态的问题"><img class="cover" src="https://s1.ax1x.com/2020/07/19/URRLqI.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-10-12</div><div class="title">Keras中关于模型的trainable状态的问题</div></div></a></div><div><a href="/2020/08/16/LSTM/" title="LSTM"><img class="cover" src="https://s1.ax1x.com/2020/08/16/dVcI61.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-08-16</div><div class="title">LSTM</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://www.cdnjson.com/images/2021/11/27/_20210211193948.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">雨落诗山山亦奇</div><div class="author-info__description">本站为读研版&工作版博客，大学版移步 --> fuhanshi.github.io</div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">169</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">本站内容的最终版本将发布在微信公众号[南极Python]</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%E6%8F%90%E5%87%BA"><span class="toc-number">1.</span> <span class="toc-text">问题提出</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3"><span class="toc-number">2.</span> <span class="toc-text">问题解决</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E6%9E%90"><span class="toc-number">3.</span> <span class="toc-text">分析</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/03/07/%E5%8A%A8%E6%89%8B%E6%90%AD%E5%BB%BAGPT2%E6%9E%B6%E6%9E%84-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E5%9B%9B)/" title="动手搭建GPT2架构-大模型炼丹术(三)"><img src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="动手搭建GPT2架构-大模型炼丹术(三)"/></a><div class="content"><a class="title" href="/2025/03/07/%E5%8A%A8%E6%89%8B%E6%90%AD%E5%BB%BAGPT2%E6%9E%B6%E6%9E%84-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E5%9B%9B)/" title="动手搭建GPT2架构-大模型炼丹术(三)">动手搭建GPT2架构-大模型炼丹术(三)</a><time datetime="2025-03-07T15:35:12.000Z" title="发表于 2025-03-07 23:35:12">2025-03-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/03/04/%E4%BB%8E%E5%8D%95%E5%A4%B4%E5%88%B0%E5%A4%9A%E5%A4%B4%EF%BC%8C%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%B8%89)/" title="从单头到多头，深度解析大模型的注意力机制-大模型炼丹术(三)"><img src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="从单头到多头，深度解析大模型的注意力机制-大模型炼丹术(三)"/></a><div class="content"><a class="title" href="/2025/03/04/%E4%BB%8E%E5%8D%95%E5%A4%B4%E5%88%B0%E5%A4%9A%E5%A4%B4%EF%BC%8C%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%B8%89)/" title="从单头到多头，深度解析大模型的注意力机制-大模型炼丹术(三)">从单头到多头，深度解析大模型的注意力机制-大模型炼丹术(三)</a><time datetime="2025-03-04T14:03:57.000Z" title="发表于 2025-03-04 22:03:57">2025-03-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/02/24/%E4%BB%8E%E7%A6%BB%E6%95%A3%E7%9A%84tokenID%E5%88%B0%E5%85%B7%E6%9C%89%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%E7%9A%84embedding-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%BA%8C)/" title="从离散的token IDs到具有语义信息的embedding-大模型炼丹术(二)"><img src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="从离散的token IDs到具有语义信息的embedding-大模型炼丹术(二)"/></a><div class="content"><a class="title" href="/2025/02/24/%E4%BB%8E%E7%A6%BB%E6%95%A3%E7%9A%84tokenID%E5%88%B0%E5%85%B7%E6%9C%89%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%E7%9A%84embedding-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%BA%8C)/" title="从离散的token IDs到具有语义信息的embedding-大模型炼丹术(二)">从离散的token IDs到具有语义信息的embedding-大模型炼丹术(二)</a><time datetime="2025-02-24T14:28:29.000Z" title="发表于 2025-02-24 22:28:29">2025-02-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/02/20/%E4%BB%8Etokenizer%E8%AF%B4%E8%B5%B7%EF%BC%8C%E4%B8%BALLM%E8%87%AA%E5%9B%9E%E5%BD%92%E9%A2%84%E8%AE%AD%E7%BB%83%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE%E9%9B%86-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%B8%80)/" title="从tokenizer说起，为LLM自回归预训练准备数据集-大模型炼丹术(一)"><img src="https://i.miji.bid/2025/02/24/d3f99c0abebc6eb1a20faf08505cfc1f.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="从tokenizer说起，为LLM自回归预训练准备数据集-大模型炼丹术(一)"/></a><div class="content"><a class="title" href="/2025/02/20/%E4%BB%8Etokenizer%E8%AF%B4%E8%B5%B7%EF%BC%8C%E4%B8%BALLM%E8%87%AA%E5%9B%9E%E5%BD%92%E9%A2%84%E8%AE%AD%E7%BB%83%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE%E9%9B%86-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%82%BC%E4%B8%B9%E6%9C%AF(%E4%B8%80)/" title="从tokenizer说起，为LLM自回归预训练准备数据集-大模型炼丹术(一)">从tokenizer说起，为LLM自回归预训练准备数据集-大模型炼丹术(一)</a><time datetime="2025-02-20T14:28:29.000Z" title="发表于 2025-02-20 22:28:29">2025-02-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/30/%E6%8A%8A%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E6%90%AC%E5%88%B0GPU-%E8%8B%B1%E4%BC%9F%E8%BE%BEDALI%E5%8A%A0%E9%80%9F%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/" title="把数据预处理搬到GPU-英伟达DALI加速数据预处理"><img src="https://ice.frostsky.com/2024/12/30/88a897ed803ad9cb7462f4320a31ac67.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="把数据预处理搬到GPU-英伟达DALI加速数据预处理"/></a><div class="content"><a class="title" href="/2024/12/30/%E6%8A%8A%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E6%90%AC%E5%88%B0GPU-%E8%8B%B1%E4%BC%9F%E8%BE%BEDALI%E5%8A%A0%E9%80%9F%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/" title="把数据预处理搬到GPU-英伟达DALI加速数据预处理">把数据预处理搬到GPU-英伟达DALI加速数据预处理</a><time datetime="2024-12-30T13:32:21.000Z" title="发表于 2024-12-30 21:32:21">2024-12-30</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://s1.ax1x.com/2020/07/19/URRLqI.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By 雨落诗山山亦奇</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><div class="js-pjax"></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/fireworks.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>